{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db785940",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO for Football Player Tracking and Heatmap Generation**\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "\n",
    "## Objective\n",
    "This notebook provides a complete workflow for building a football analytics tool using computer vision. We will fine-tune a **YOLO (You Only Look Once)** model to detect and track players on a football pitch. Using this model, we will generate advanced analytics like **player trajectories** and **positional heatmaps**.\n",
    "\n",
    "## Key Features\n",
    "* **Data Preparation**: Convert annotations from COCO format to the YOLO format required for training.\n",
    "* **Model Training**: Fine-tune a pre-trained YOLO model on a custom football dataset.\n",
    "* **Player Tracking**: Apply the trained model to track individual players across video frames.\n",
    "* **Trajectory Visualization**: Draw the paths of players to analyze movement patterns.\n",
    "* **Heatmap Generation**: Create heatmaps to visualize team positioning and field control.\n",
    "\n",
    "## Libraries & Prerequisites\n",
    "* **Core Libraries**: `ultralytics`, `opencv-python`, `matplotlib`, `numpy`.\n",
    "* **Environment**: A Python environment with GPU support (like Google Colab) is highly recommended for efficient model training.\n",
    "* **Dataset**: You'll need a custom dataset of football images with annotations in COCO format (`.json` file) and the corresponding video files for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bd405",
   "metadata": {},
   "source": [
    "## Annotate your Custom dataset using Labellerr\n",
    "\n",
    " ***1. Visit the [Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) website and click **‚ÄúSign Up‚Äù**.*** \n",
    "\n",
    " ***2. After signing in, create your workspace by entering a unique name.***\n",
    "\n",
    " ***3. Navigate to your workspace‚Äôs API keys page (e.g., `https://<your-workspace>.labellerr.com/workspace/api-keys`) to generate your **API Key** and **API Secret**.***\n",
    "\n",
    " ***4. Store the credentials securely, and then use them to initialise the SDK or API client with `api_key`, `api_secret`.*** \n",
    "\n",
    "\n",
    "### Use Labellerr SDK for uploading and perform annotation of your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to install required packages in a Jupyter notebook environment\n",
    "\n",
    "# !pip install git+https://github.com/Labellerr/SDKPython.git\n",
    "# !pip install ipyfilechooser\n",
    "# !git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a68661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports required for this notebook\n",
    "from labellerr.client import LabellerrClient\n",
    "from labellerr.core.datasets import create_dataset_from_local\n",
    "from labellerr.core.annotation_templates import create_template\n",
    "from labellerr.core.projects import create_project\n",
    "from labellerr.core.schemas import DatasetConfig, AnnotationQuestion, QuestionType, CreateTemplateParams, DatasetDataType, CreateProjectParams, RotationConfig\n",
    "from labellerr.core.projects import LabellerrProject\n",
    "from labellerr.core.exceptions import LabellerrError\n",
    "\n",
    "import uuid\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd701233",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"YOUR_API_KEY\")        # go to labellerr workspace to get your API key\n",
    "api_secret = input(\"YOUR_API_SECRET\")  # go to labellerr workspace to get your API secret\n",
    "client_id = input(\"YOUR_CLIENT_ID\")   # Contact labellerr support to get your client ID i.e. support@tensormatics.com\n",
    "\n",
    "client = LabellerrClient(api_key, api_secret, client_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648caf6",
   "metadata": {},
   "source": [
    "### ***STEP-1: Create a dataset on labellerr from your local folder***\n",
    "\n",
    "The SDK supports in creating dataset by uploading local files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf918c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bab602150d0462d8a724c7f48ac1984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='D:\\', filename='', title='Select a folder containing your dataset', show_hidden=False, selec‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a folder chooser starting from a directory (for example, your home directory)\n",
    "chooser = FileChooser('/')\n",
    "\n",
    "# Set the chooser to folder selection mode only\n",
    "chooser.title = 'Select a folder containing your dataset'\n",
    "chooser.show_only_dirs = True\n",
    "\n",
    "# Display the widget\n",
    "display(chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea17dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\frames_output\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = chooser.selected_path\n",
    "print(\"You selected:\", path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34469be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset type: video\n"
     ]
    }
   ],
   "source": [
    "my_dataset_type = input(\"Enter your dataset type (video or image): \").lower()\n",
    "print(\"Selected dataset type:\", my_dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset_from_local(\n",
    "    client=client,\n",
    "    dataset_config=DatasetConfig(dataset_name=\"My Dataset\", data_type=\"image\"),\n",
    "    folder_to_upload=path_to_dataset\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with ID: {dataset.dataset_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57a000",
   "metadata": {},
   "source": [
    "### ***STEP-2: Create annotation project on labellerr of your created dataset***\n",
    "\n",
    "Create a annotation project of your uploaded dataset to start performing annotation on labellerr UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation guideline template for video annotation project (like classes to be annotated)\n",
    "\n",
    "template = create_template(\n",
    "    client=client,\n",
    "    params=CreateTemplateParams(\n",
    "        template_name=\"My Template\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        questions=[\n",
    "            AnnotationQuestion(\n",
    "                question_number=1,\n",
    "                question=\"Object\",\n",
    "                question_id=str(uuid.uuid4()),\n",
    "                question_type=QuestionType.polygon,\n",
    "                required=True,\n",
    "                color=\"#FF0000\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(f\"Annotation template created with ID: {template.annotation_template_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.status()        # wait until dataset is processed before creating project\n",
    "\n",
    "project = create_project(\n",
    "    client=client,\n",
    "    params=CreateProjectParams(\n",
    "        project_name=\"My Project\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        rotations=RotationConfig(\n",
    "            annotation_rotation_count=1,\n",
    "            review_rotation_count=1,\n",
    "            client_review_rotation_count=1\n",
    "        )\n",
    "    ),\n",
    "    datasets=[dataset],\n",
    "    annotation_template=template\n",
    ")\n",
    "\n",
    "print(f\"‚úì Project created: {project.project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51029b",
   "metadata": {},
   "source": [
    "Your project has been created now go to labellerr platform to perform annotation \n",
    "\n",
    "***click to go to labellerr.com***\n",
    "\n",
    "[![Labellerr](https://cdn.labellerr.com/1%20%20Documentation/1c9dc7ce-9a54-4111-8fd5-0363ba3e00e1.webp)](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks)\n",
    "Open the project you created (Projects ‚Üí select your project).\n",
    "\n",
    "Click Start Labeling to open the annotation interface. Use the configured labeling tools (bounding boxes, polygon, dot, classification, etc.) to annotate files.\n",
    "### ***STEP-3: Export your annotation in required format***\n",
    "\n",
    "Generate a temporary download URL to retrieve your exported JSON file:\n",
    "\n",
    "### Export Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `export_name` | string | Display name for the export |\n",
    "| `export_description` | string | Description of what this export contains |\n",
    "| `export_format` | string | Output format (e.g., `json`, `xml`, `coco`) |\n",
    "| `statuses` | list | Annotation statuses to include in export |\n",
    "\n",
    "### Common Annotation Statuses\n",
    "\n",
    "- **`review`**: Annotations pending review\n",
    "- **`r_assigned`**: Review assigned to a reviewer\n",
    "- **`client_review`**: Under client review\n",
    "- **`cr_assigned`**: Client review assigned\n",
    "- **`accepted`**: Annotations accepted and finalized\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_config = {\n",
    "    \"export_name\": \"Weekly Export\",\n",
    "    \"export_description\": \"Export of all accepted annotations\",\n",
    "    \"export_format\": \"coco_json\",\n",
    "    \"statuses\": ['review', 'r_assigned','client_review', 'cr_assigned','accepted']\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Get project instance\n",
    "    project = LabellerrProject(client=client, project_id=project.project_id)\n",
    "    \n",
    "    # Create export\n",
    "    result = project.create_local_export(export_config)\n",
    "    export_id = result[\"response\"]['report_id']\n",
    "    print(f\"Local export created successfully. Export ID: {export_id}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Local export creation failed: {str(e)}\")\n",
    "    \n",
    "    \n",
    "try:\n",
    "    download_url = client.fetch_download_url(\n",
    "        project_id=project.project_id,\n",
    "        uuid=str(uuid.uuid4()),\n",
    "        export_id=export_id\n",
    "    )\n",
    "    print(f\"Download URL: {download_url}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Failed to fetch download URL: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac91632",
   "metadata": {},
   "source": [
    "Now you can download your annotations locally using given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841bcf4",
   "metadata": {},
   "source": [
    "## **Dataset Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7eb33",
   "metadata": {},
   "source": [
    "This step prepares our dataset for training. The key function we'll use is `coco_to_yolo_converter`, which transforms annotations from the common COCO format into the specific YOLO `.txt` format required by the `ultralytics` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.bbox_converter import coco_to_yolo_converter\n",
    "# Convert COCO annotations to YOLO format\n",
    "# Ensure the paths are correct and the dataset_annotation.json is in COCO format\n",
    "# The images_dir should contain the images dataset\n",
    "# The json_path should point to the COCO annotations file\n",
    "result = coco_to_yolo_converter(\n",
    "            json_path='./dataset_annotation.json',\n",
    "            images_dir='./dataset',\n",
    "            output_dir='yolo_format',\n",
    "            use_split=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3853399",
   "metadata": {},
   "source": [
    "## **Training the YOLO Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846997f",
   "metadata": {},
   "source": [
    "With our dataset properly formatted, we can now fine-tune the YOLO model. We'll start by checking the `ultralytics` installation. Then, we define the path to our dataset's configuration file (`dataset.yaml`) and launch the training process using a pre-trained model. This leverages **transfer learning** to adapt the model to our specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de35ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32823e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = !pwd\n",
    "dataset_path = f\"{location[0]}/yolo_format\"\n",
    "print(f\"Dataset path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train data={dataset_path}/dataset.yaml model=\"yolo11x.pt\" epochs=200 imgsz=640 batch=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4623298e",
   "metadata": {},
   "source": [
    "## **Tracking Player on the Field**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6254ff7",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can use it for tracking players in videos. We'll load our custom-trained weights and apply them to new video files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df111513",
   "metadata": {},
   "source": [
    "To quickly verify that our model's tracking is working, let's test it on a single video frame. We will load our new weights, run the tracker, and use `matplotlib` to visualize the bounding boxes and track IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('./runs/detect/train2/weights/last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(source=\"./Video/3.mp4\", persist=True, stream=True)\n",
    "\n",
    "# Find frame #30\n",
    "for frame_idx, res in enumerate(results):\n",
    "    if frame_idx < 30:\n",
    "        continue\n",
    "\n",
    "    # Grab img, boxes, track-IDs and class-IDs\n",
    "    frame_rgb = cv2.cvtColor(res.orig_img, cv2.COLOR_BGR2RGB)\n",
    "    boxes     = res.boxes.xyxy.cpu().numpy()    # (N,4)\n",
    "    track_ids = res.boxes.id.cpu().numpy()      # (N,)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)  # (N,)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(frame_rgb)\n",
    "    ax.axis('off')\n",
    "\n",
    "    team_colors = {0: 'red', 4: 'blue'}  # map your relevant class IDs ‚Üí colors\n",
    "\n",
    "    # Now zip over the three arrays, using a different name than `cls`\n",
    "    for (x1, y1, x2, y2), tid, cid in zip(boxes, track_ids, class_ids):\n",
    "        # print out for debug\n",
    "        print(f\"Player ID: {tid}, Class: {cid}\")\n",
    "\n",
    "        # only draw if the class is in your team mapping\n",
    "        if cid in team_colors:\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            color = team_colors[cid]\n",
    "\n",
    "            # draw box\n",
    "            rect = plt.Rectangle(\n",
    "                (x1, y1), w, h,\n",
    "                linewidth=2, edgecolor=color, facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # label with track ID\n",
    "            ax.text(\n",
    "                x1, y1 - 6, f\"Player {int(tid)}\",\n",
    "                color='white', fontsize=7,\n",
    "                bbox=dict(facecolor=color, alpha=0.5, pad=1, linewidth=0)\n",
    "            )\n",
    "\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ae2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output paths\n",
    "video_path = \"./Video/2.mp4\"\n",
    "output_path = \"./Video/2_tracked-2.mp4\"\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define VideoWriter\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Team color mapping\n",
    "team_colors = {0: (0, 0, 255), 4: (255, 0, 0)}  # Red for class 0, Blue for class 4 (BGR)\n",
    "\n",
    "# Run tracking on the video as a stream\n",
    "results = model.track(source=video_path, persist=True, stream=True, )\n",
    "\n",
    "# Process frame by frame\n",
    "for res in results:\n",
    "    frame = res.orig_img.copy()  # BGR format for saving with OpenCV\n",
    "\n",
    "    if res.boxes.id is None:\n",
    "        out.write(frame)\n",
    "        continue\n",
    "\n",
    "    boxes     = res.boxes.xyxy.cpu().numpy()\n",
    "    track_ids = res.boxes.id.cpu().numpy().astype(int)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    for (x1, y1, x2, y2), tid, cid in zip(boxes, track_ids, class_ids):\n",
    "        if cid in team_colors:\n",
    "            color = team_colors[cid]\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            # Draw label\n",
    "            label = f\"Player {tid}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"‚úÖ Tracking video saved at:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311f4f1",
   "metadata": {},
   "source": [
    "## **Tracking the Trajectory of the Player**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63faeb73",
   "metadata": {},
   "source": [
    "Beyond simple tracking, we can visualize player movement by plotting their trajectories. We'll store the center point of each player's bounding box over time and draw lines to create a visual trail showing each player's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Input and output video paths\n",
    "video_path = \"./Video/4.mp4\"\n",
    "output_path = \"./Video/4_output_with_trajectory.mp4\"\n",
    "\n",
    "# Open video and get properties\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Output writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Team color mapping (BGR)\n",
    "team_colors = {0: (0, 0, 255), 4: (255, 0, 0)}  # Red for class 0, Blue for class 4\n",
    "\n",
    "# Dictionary to store trajectory points\n",
    "trajectories = defaultdict(list)\n",
    "\n",
    "# Perform tracking\n",
    "results = model.track(source=video_path, persist=True, stream=True)\n",
    "\n",
    "for res in results:\n",
    "    frame = res.orig_img.copy()\n",
    "\n",
    "    if res.boxes.id is None:\n",
    "        out.write(frame)\n",
    "        continue\n",
    "\n",
    "    boxes     = res.boxes.xyxy.cpu().numpy()\n",
    "    track_ids = res.boxes.id.cpu().numpy().astype(int)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    for (x1, y1, x2, y2), tid, cid in zip(boxes, track_ids, class_ids):\n",
    "        if cid in team_colors:\n",
    "            color = team_colors[cid]\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)\n",
    "\n",
    "            # Update trajectory for this track_id\n",
    "            trajectories[tid].append((cx, cy))\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, f\"Player {tid}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            # Draw trajectory line (at least 2 points needed)\n",
    "            if len(trajectories[tid]) >= 2:\n",
    "                for j in range(1, len(trajectories[tid])):\n",
    "                    pt1 = trajectories[tid][j - 1]\n",
    "                    pt2 = trajectories[tid][j]\n",
    "                    cv2.line(frame, pt1, pt2, color, 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"‚úÖ Video with trajectories saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d6b8b",
   "metadata": {},
   "source": [
    "## **Heatmap of the Players on the ground**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7abf3c",
   "metadata": {},
   "source": [
    "To get a high-level view of player positioning, we can generate a heatmap. This visualization aggregates player locations over the entire video. We create an accumulator for each team and increment the values where players are detected. Finally, we apply a colormap and overlay it on a video frame to see which areas were most occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "video_path = \"./Video/4.mp4\"\n",
    "\n",
    "classes_of_interest = [0, 4]  # your team classes\n",
    "\n",
    "# Open video to get shape & frame count\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cap.release()\n",
    "\n",
    "# 2. Initialize accumulators: one 2D float array per class\n",
    "heatmaps = {cls: np.zeros((height, width), dtype=np.float32) \n",
    "            for cls in classes_of_interest}\n",
    "\n",
    "# 3. Run tracking (or detection) over entire video\n",
    "results = model.track(source=video_path, persist=True, stream=True)\n",
    "\n",
    "for res in results:\n",
    "    if res.boxes.id is None:\n",
    "        continue\n",
    "\n",
    "    boxes     = res.boxes.xyxy.cpu().numpy().astype(int)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)\n",
    "\n",
    "    for (x1, y1, x2, y2), cid in zip(boxes, class_ids):\n",
    "        if cid not in classes_of_interest:\n",
    "            continue\n",
    "\n",
    "        # Option A: accumulate box area\n",
    "        heatmaps[cid][y1:y2, x1:x2] += 1\n",
    "\n",
    "        # ‚ÄîOR‚Äî Option B: accumulate only the center point\n",
    "        # cx, cy = (x1 + x2)//2, (y1 + y2)//2\n",
    "        # heatmaps[cid][cy, cx] += 1\n",
    "\n",
    "# 4. Normalize & colorize each heatmap\n",
    "colored_maps = {}\n",
    "for cid, hm in heatmaps.items():\n",
    "    # normalize to 0‚Äì255\n",
    "    hm_norm = cv2.normalize(hm, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    # apply JET colormap\n",
    "    colored = cv2.applyColorMap(hm_norm, cv2.COLORMAP_JET)\n",
    "    colored_maps[cid] = colored  # BGR image\n",
    "\n",
    "# 5. Overlay on a sample frame (e.g., the first frame)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 100)\n",
    "\n",
    "ret, base = cap.read()\n",
    "cap.release()\n",
    "if not ret:\n",
    "    raise RuntimeError(\"Failed to read sample frame.\")\n",
    "\n",
    "overlay = base.copy()\n",
    "alpha = 0.5  # transparency\n",
    "\n",
    "for cid, cmap in colored_maps.items():\n",
    "    # blend heatmap with the base frame\n",
    "    cv2.addWeighted(cmap, alpha, overlay, 1 - alpha, 0, overlay)\n",
    "\n",
    "# 6. Save or display\n",
    "cv2.imwrite(\"class_heatmaps_overlay.png\", overlay)\n",
    "print(\"Saved overlay image with heatmaps: per_class_heatmaps_overlay.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade8d32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
