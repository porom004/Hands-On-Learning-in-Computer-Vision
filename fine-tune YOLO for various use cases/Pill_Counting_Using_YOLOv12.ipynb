{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5f6175",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Pill Counting Using YOLOv12**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924e9d9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This documentation covers the implementation of a real-time pill counting system using YOLOv12 segmentation model. The solution processes video footage of pills, counts them in real-time using instance segmentation, and provides interactive ROI (Region of Interest) selection for focused analysis. This implementation is valuable for pharmaceutical quality control, inventory management, and automated medication dispensing systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Dependencies](#1-setup-and-dependencies)\n",
    "2. [Data Preparation](#2-data-preparation)\n",
    "3. [Model Training](#3-model-training)\n",
    "4. [Inference and Tracking](#4-inference-and-tracking)\n",
    "5. [Real-Time Pill Counting System](#5-real-time-pill-counting-system)\n",
    "6. [Usage Examples](#6-usage-examples)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8c007",
   "metadata": {},
   "source": [
    "## 1. Annotate your Custom dataset using Labellerr\n",
    "\n",
    " ***1. Visit the [Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) website and click **‚ÄúSign Up‚Äù**.*** \n",
    "\n",
    " ***2. After signing in, create your workspace by entering a unique name.***\n",
    "\n",
    " ***3. Navigate to your workspace‚Äôs API keys page (e.g., `https://<your-workspace>.labellerr.com/workspace/api-keys`) to generate your **API Key** and **API Secret**.***\n",
    "\n",
    " ***4. Store the credentials securely, and then use them to initialise the SDK or API client with `api_key`, `api_secret`.*** \n",
    "\n",
    "\n",
    "### Use Labellerr SDK for uploading and perform annotation of your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e8e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to install required packages in a Jupyter notebook environment\n",
    "\n",
    "# !pip install git+https://github.com/Labellerr/SDKPython.git\n",
    "# !pip install ipyfilechooser\n",
    "# !git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports required for this notebook\n",
    "from labellerr.client import LabellerrClient\n",
    "from labellerr.core.datasets import create_dataset_from_local\n",
    "from labellerr.core.annotation_templates import create_template\n",
    "from labellerr.core.projects import create_project\n",
    "from labellerr.core.schemas import DatasetConfig, AnnotationQuestion, QuestionType, CreateTemplateParams, DatasetDataType, CreateProjectParams, RotationConfig\n",
    "from labellerr.core.projects import LabellerrProject\n",
    "from labellerr.core.exceptions import LabellerrError\n",
    "\n",
    "import uuid\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"YOUR_API_KEY\")        # go to labellerr workspace to get your API key\n",
    "api_secret = input(\"YOUR_API_SECRET\")  # go to labellerr workspace to get your API secret\n",
    "client_id = input(\"YOUR_CLIENT_ID\")   # Contact labellerr support to get your client ID i.e. support@tensormatics.com\n",
    "\n",
    "client = LabellerrClient(api_key, api_secret, client_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57723fa",
   "metadata": {},
   "source": [
    "### ***STEP-1: Create a dataset on labellerr from your local folder***\n",
    "\n",
    "The SDK supports in creating dataset by uploading local files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55fe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bab602150d0462d8a724c7f48ac1984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='D:\\', filename='', title='Select a folder containing your dataset', show_hidden=False, selec‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a folder chooser starting from a directory (for example, your home directory)\n",
    "chooser = FileChooser('/')\n",
    "\n",
    "# Set the chooser to folder selection mode only\n",
    "chooser.title = 'Select a folder containing your dataset'\n",
    "chooser.show_only_dirs = True\n",
    "\n",
    "# Display the widget\n",
    "display(chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa64996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\frames_output\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = chooser.selected_path\n",
    "print(\"You selected:\", path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da44e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset type: video\n"
     ]
    }
   ],
   "source": [
    "my_dataset_type = input(\"Enter your dataset type (video or image): \").lower()\n",
    "print(\"Selected dataset type:\", my_dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5aeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset_from_local(\n",
    "    client=client,\n",
    "    dataset_config=DatasetConfig(dataset_name=\"My Dataset\", data_type=\"image\"),\n",
    "    folder_to_upload=path_to_dataset\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with ID: {dataset.dataset_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0881cb",
   "metadata": {},
   "source": [
    "### ***STEP-2: Create annotation project on labellerr of your created dataset***\n",
    "\n",
    "Create a annotation project of your uploaded dataset to start performing annotation on labellerr UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaeb73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation guideline template for video annotation project (like classes to be annotated)\n",
    "\n",
    "template = create_template(\n",
    "    client=client,\n",
    "    params=CreateTemplateParams(\n",
    "        template_name=\"My Template\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        questions=[\n",
    "            AnnotationQuestion(\n",
    "                question_number=1,\n",
    "                question=\"Object\",\n",
    "                question_id=str(uuid.uuid4()),\n",
    "                question_type=QuestionType.polygon,\n",
    "                required=True,\n",
    "                color=\"#FF0000\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(f\"Annotation template created with ID: {template.annotation_template_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b21466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.status()        # wait until dataset is processed before creating project\n",
    "\n",
    "project = create_project(\n",
    "    client=client,\n",
    "    params=CreateProjectParams(\n",
    "        project_name=\"My Project\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        rotations=RotationConfig(\n",
    "            annotation_rotation_count=1,\n",
    "            review_rotation_count=1,\n",
    "            client_review_rotation_count=1\n",
    "        )\n",
    "    ),\n",
    "    datasets=[dataset],\n",
    "    annotation_template=template\n",
    ")\n",
    "\n",
    "print(f\"‚úì Project created: {project.project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c3846",
   "metadata": {},
   "source": [
    "Your project has been created now go to labellerr platform to perform annotation \n",
    "\n",
    "***click to go to labellerr.com***\n",
    "\n",
    "[![Labellerr](https://cdn.labellerr.com/1%20%20Documentation/1c9dc7ce-9a54-4111-8fd5-0363ba3e00e1.webp)](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks)\n",
    "Open the project you created (Projects ‚Üí select your project).\n",
    "\n",
    "Click Start Labeling to open the annotation interface. Use the configured labeling tools (bounding boxes, polygon, dot, classification, etc.) to annotate files.\n",
    "### ***STEP-3: Export your annotation in required format***\n",
    "\n",
    "Generate a temporary download URL to retrieve your exported JSON file:\n",
    "\n",
    "### Export Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `export_name` | string | Display name for the export |\n",
    "| `export_description` | string | Description of what this export contains |\n",
    "| `export_format` | string | Output format (e.g., `json`, `xml`, `coco`) |\n",
    "| `statuses` | list | Annotation statuses to include in export |\n",
    "\n",
    "### Common Annotation Statuses\n",
    "\n",
    "- **`review`**: Annotations pending review\n",
    "- **`r_assigned`**: Review assigned to a reviewer\n",
    "- **`client_review`**: Under client review\n",
    "- **`cr_assigned`**: Client review assigned\n",
    "- **`accepted`**: Annotations accepted and finalized\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4cc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_config = {\n",
    "    \"export_name\": \"Weekly Export\",\n",
    "    \"export_description\": \"Export of all accepted annotations\",\n",
    "    \"export_format\": \"coco_json\",\n",
    "    \"statuses\": ['review', 'r_assigned','client_review', 'cr_assigned','accepted']\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Get project instance\n",
    "    project = LabellerrProject(client=client, project_id=project.project_id)\n",
    "    \n",
    "    # Create export\n",
    "    result = project.create_local_export(export_config)\n",
    "    export_id = result[\"response\"]['report_id']\n",
    "    print(f\"Local export created successfully. Export ID: {export_id}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Local export creation failed: {str(e)}\")\n",
    "    \n",
    "    \n",
    "try:\n",
    "    download_url = client.fetch_download_url(\n",
    "        project_id=project.project_id,\n",
    "        uuid=str(uuid.uuid4()),\n",
    "        export_id=export_id\n",
    "    )\n",
    "    print(f\"Download URL: {download_url}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Failed to fetch download URL: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9179287",
   "metadata": {},
   "source": [
    "Now you can download your annotations locally using given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9362f0",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Import YOLO Segmentation Conversion Module\n",
    "\n",
    "Import the specific function needed to convert video annotations into YOLO segmentation format, which is required for training instance segmentation models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.video_annotation.yolo_converter import convert_to_yolo_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11015e",
   "metadata": {},
   "source": [
    "**What it does**: Imports the conversion function that transforms annotated video data into YOLO-compatible segmentation format.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Convert Annotations to YOLO Segmentation Format\n",
    "\n",
    "This cell performs the actual conversion of video annotations to YOLO format and splits the dataset into train, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATION_FILE = \"annotations.json\"\n",
    "VIDEOS_DIRECTORY = \"new_video\"\n",
    "OUTPUT_DATASET_DIR = \"yolo_dataset_2\"\n",
    "\n",
    "\n",
    "convert_to_yolo_segmentation(\n",
    "    annotation_path=ANNOTATION_FILE,\n",
    "    videos_dir=VIDEOS_DIRECTORY,\n",
    "    use_split=True,\n",
    "    split_ratio=(0.7, 0.2, 0.1),\n",
    "    output_dir=OUTPUT_DATASET_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0856eda",
   "metadata": {},
   "source": [
    "**Parameters**:\n",
    "- `annotation_path`: Path to the JSON file containing video annotations\n",
    "- `videos_dir`: Directory containing the source video files\n",
    "- `use_split`: Boolean flag to enable dataset splitting\n",
    "- `split_ratio`: Tuple defining train/validation/test split (70%/20%/10%)\n",
    "- `output_dir`: Destination directory for the converted YOLO dataset\n",
    "\n",
    "**Output**: Creates a structured YOLO dataset with images and labels organized into train, val, and test folders, along with a `data.yaml` configuration file.\n",
    "\n",
    "> **Note**: The conversion process extracts frames from videos and generates corresponding YOLO segmentation labels in normalized polygon format.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Training\n",
    "\n",
    "### 3.1 Clear GPU Memory Cache\n",
    "\n",
    "Before training, it's important to clear GPU memory to avoid out-of-memory errors and ensure optimal resource utilization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992778",
   "metadata": {},
   "source": [
    "**Purpose**: Releases cached GPU memory from previous operations.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Check GPU Status\n",
    "\n",
    "Verify GPU availability and memory status before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a92c84",
   "metadata": {},
   "source": [
    "**Output**: Displays GPU information including memory usage, temperature, and active processes.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Monitor GPU Memory Usage\n",
    "\n",
    "Check detailed GPU memory statistics programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory status\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
    "print(f\"Free: {torch.cuda.mem_get_info(0)[0]/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f2450",
   "metadata": {},
   "source": [
    "**Purpose**: Provides granular insight into GPU memory allocation for debugging and optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Import YOLO Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051e86e",
   "metadata": {},
   "source": [
    "**What it does**: Imports the Ultralytics YOLO library for model training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Train YOLOv12 Segmentation Model\n",
    "\n",
    "This is the main training cell that fine-tunes the YOLOv12 segmentation model on the pill dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolo12n-seg.yaml\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=r\"yolo_dataset\\data.yaml\",  # Path to your dataset YAML file\n",
    "    epochs=300,                         # Number of training epochs\n",
    "    imgsz=640,                         # Image size\n",
    "    batch=-1,                          # Batch size\n",
    "    device=0,                          # GPU device (0 for first GPU, 'cpu' for CPU)\n",
    "    workers=4,                         # Number of dataloader workers\n",
    "    project=\"yolo12_segmentation-2\",     # Project folder name\n",
    "    name=\"train_run\",                  # Experiment name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b2e9d",
   "metadata": {},
   "source": [
    "**Training Parameters**:\n",
    "- `data`: Path to the data.yaml configuration file\n",
    "- `epochs`: Number of training iterations (200 for thorough training)\n",
    "- `imgsz`: Input image size (640x640 pixels)\n",
    "- `batch`: Batch size (-1 enables auto-batch sizing based on GPU memory)\n",
    "- `device`: GPU device ID (0 for first GPU, 'cpu' for CPU training)\n",
    "- `workers`: Number of parallel data loading threads\n",
    "- `project`: Output directory for training artifacts\n",
    "- `name`: Specific run name for organization\n",
    "\n",
    "**Output**: The model will be trained and checkpoints saved to `yolo12_segmentation-2/train_run/weights/`. Key files include:\n",
    "- `best.pt`: Best performing model weights\n",
    "- `last.pt`: Weights from the final epoch\n",
    "- Training curves and validation metrics\n",
    "\n",
    "> **Training Tip**: The auto-batch feature (`batch=-1`) automatically determines the optimal batch size based on available GPU memory. Monitor the training output for metrics like box_loss, seg_loss, cls_loss, and mAP scores.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Inference and Tracking\n",
    "\n",
    "### 4.1 Run Inference with Tracking on Video\n",
    "\n",
    "This cell demonstrates basic inference and tracking on a video file using the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(r\"yolo12_segmentation\\train_run\\weights\\best.pt\")\n",
    "\n",
    "# Run inference on video\n",
    "results = model.track(\n",
    "    source=r\"new_video\\pill sample 2_trim.mp4\",\n",
    "    save=True,\n",
    "    conf=0.25,\n",
    "    iou=0.7,\n",
    "    show=True,\n",
    "    show_labels=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d22d5e",
   "metadata": {},
   "source": [
    "**Parameters**:\n",
    "- `source`: Path to input video file\n",
    "- `save`: Save output video with annotations\n",
    "- `conf`: Confidence threshold for detections (0.25 = 25% minimum confidence)\n",
    "- `iou`: IOU threshold for Non-Maximum Suppression (0.7)\n",
    "- `show`: Display results in real-time\n",
    "- `show_labels`: Toggle class label display\n",
    "\n",
    "**Output**: Generates annotated video with tracked pill instances and displays results in a window.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Real-Time Pill Counting System\n",
    "\n",
    "### 5.1 Real-Time Pill Counter Class Architecture\n",
    "\n",
    "The `RealTimePillCounter` class provides a comprehensive solution for counting pills in videos with the following capabilities:\n",
    "\n",
    "- Interactive polygon ROI selection\n",
    "- Point-in-polygon testing for spatial filtering\n",
    "- Unique color generation for tracked objects\n",
    "- Segmentation mask visualization\n",
    "- Frame-by-frame statistics tracking\n",
    "\n",
    "### 5.2 Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e352edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class RealTimePillCounter:\n",
    "    def __init__(self, model_path='best.pt', conf_threshold=0.6, iou_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the real-time pill counter with YOLO model\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the trained YOLO model\n",
    "            conf_threshold: Confidence threshold for detections (default: 0.6)\n",
    "            iou_threshold: IOU threshold for NMS (default: 0.5)\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.roi = None\n",
    "        self.track_history = defaultdict(lambda: deque(maxlen=30))\n",
    "        \n",
    "    def select_roi(self, frame):\n",
    "        \"\"\"\n",
    "        Interactive ROI selection on the first frame using fullscreen and point input\n",
    "        Creates a polygon from selected vertices\n",
    "        \n",
    "        Args:\n",
    "            frame: First frame of the video\n",
    "            \n",
    "        Returns:\n",
    "            roi: Selected region vertices as list of points [(x1,y1), (x2,y2), ...]\n",
    "        \"\"\"\n",
    "        clone = frame.copy()\n",
    "        points = []\n",
    "        \n",
    "        def mouse_callback(event, x, y, flags, param):\n",
    "            if event == cv2.EVENT_LBUTTONDOWN:\n",
    "                points.append((x, y))\n",
    "                \n",
    "        # Create fullscreen window\n",
    "        window_name = 'Select ROI - Fullscreen'\n",
    "        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "        cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "        cv2.setMouseCallback(window_name, mouse_callback)\n",
    "        \n",
    "        print(\"\\n=== ROI Selection Instructions ===\")\n",
    "        print(\"1. Click on the screen to add vertices of the region\")\n",
    "        print(\"2. Add at least 3 points to form a polygon\")\n",
    "        print(\"3. Press SPACE to confirm selection\")\n",
    "        print(\"4. Press 'r' to reset points\")\n",
    "        print(\"5. Press ESC to exit\")\n",
    "        print(\"===================================\\n\")\n",
    "        \n",
    "        while True:\n",
    "            display = clone.copy()\n",
    "            \n",
    "            # Draw all clicked points\n",
    "            for i, point in enumerate(points):\n",
    "                cv2.circle(display, point, 8, (0, 255, 255), -1)\n",
    "                cv2.putText(display, f'V{i+1}', (point[0]+15, point[1]-15),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "            \n",
    "            # Draw polygon if we have at least 3 points\n",
    "            if len(points) >= 3:\n",
    "                # Draw semi-transparent polygon\n",
    "                overlay = display.copy()\n",
    "                pts = np.array(points, np.int32)\n",
    "                pts = pts.reshape((-1, 1, 2))\n",
    "                cv2.fillPoly(overlay, [pts], (0, 255, 0))\n",
    "                cv2.addWeighted(overlay, 0.2, display, 0.8, 0, display)\n",
    "                \n",
    "                # Draw polygon outline\n",
    "                cv2.polylines(display, [pts], True, (0, 255, 0), 3)\n",
    "                \n",
    "            # Draw lines between consecutive points\n",
    "            if len(points) >= 2:\n",
    "                for i in range(len(points) - 1):\n",
    "                    cv2.line(display, points[i], points[i+1], (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw instructions\n",
    "            instructions = [\n",
    "                f'Vertices: {len(points)} | Click to add vertices',\n",
    "                'SPACE: Confirm | R: Reset | ESC: Exit'\n",
    "            ]\n",
    "            \n",
    "            y_offset = 40\n",
    "            for i, text in enumerate(instructions):\n",
    "                # Background for text\n",
    "                (text_w, text_h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "                cv2.rectangle(display, (10, y_offset - 30 + i*50), \n",
    "                             (text_w + 30, y_offset + 10 + i*50), (0, 0, 0), -1)\n",
    "                cv2.putText(display, text, (20, y_offset + i*50),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.imshow(window_name, display)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            \n",
    "            if key == 32:  # SPACE key\n",
    "                if len(points) >= 3:\n",
    "                    self.roi = points.copy()\n",
    "                    print(f\"ROI polygon selected with {len(points)} vertices: {self.roi}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please select at least 3 vertices to form a polygon!\")\n",
    "                    \n",
    "            elif key == ord('r') or key == ord('R'):  # Reset\n",
    "                points.clear()\n",
    "                print(\"Vertices reset\")\n",
    "                \n",
    "            elif key == 27:  # ESC key\n",
    "                if len(points) >= 3:\n",
    "                    self.roi = points.copy()\n",
    "                    print(f\"ROI polygon selected with {len(points)} vertices: {self.roi}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Exiting without ROI selection\")\n",
    "                    break\n",
    "                \n",
    "        cv2.destroyWindow(window_name)\n",
    "        return self.roi\n",
    "    \n",
    "    def point_in_roi(self, point):\n",
    "        \"\"\"\n",
    "        Check if a point is inside the ROI polygon\n",
    "        \n",
    "        Args:\n",
    "            point: (x, y) coordinates\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if point is inside ROI polygon\n",
    "        \"\"\"\n",
    "        if self.roi is None:\n",
    "            return True\n",
    "        \n",
    "        # Convert ROI to numpy array for polygon check\n",
    "        pts = np.array(self.roi, np.int32)\n",
    "        result = cv2.pointPolygonTest(pts, point, False)\n",
    "        return result >= 0  # Returns >= 0 if inside or on the polygon\n",
    "    \n",
    "    def generate_color(self, track_id):\n",
    "        \"\"\"\n",
    "        Generate a unique color for each track ID\n",
    "        \n",
    "        Args:\n",
    "            track_id: Tracking ID\n",
    "            \n",
    "        Returns:\n",
    "            color: BGR color tuple\n",
    "        \"\"\"\n",
    "        np.random.seed(int(track_id))\n",
    "        color = tuple(map(int, np.random.randint(0, 255, 3)))\n",
    "        return color\n",
    "    \n",
    "    def process_video(self, video_path, output_path='output_pill_count.mp4', \n",
    "                     show_labels=True, show_segmentation=True, use_tracking=True):\n",
    "        \"\"\"\n",
    "        Process video for CURRENT pill counting (not cumulative)\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video\n",
    "            output_path: Path to save output video\n",
    "            show_labels: If True, display track IDs on pills (default: True)\n",
    "            show_segmentation: If True, display segmentation masks and contours (default: True)\n",
    "            use_tracking: If True, use ByteTrack for stable counting (default: True)\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Read first frame for ROI selection\n",
    "        ret, first_frame = cap.read()\n",
    "        if not ret:\n",
    "            raise ValueError(\"Cannot read first frame\")\n",
    "        \n",
    "        # Select ROI\n",
    "        print(\"Select the region of interest...\")\n",
    "        self.select_roi(first_frame)\n",
    "        print(f\"ROI selected with {len(self.roi)} vertices\")\n",
    "        \n",
    "        # Reset video to beginning\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        \n",
    "        # Video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        # For statistics\n",
    "        max_pills_seen = 0\n",
    "        frame_counts = []\n",
    "        \n",
    "        print(\"Processing video...\")\n",
    "        print(f\"Total frames: {total_frames}\")\n",
    "        print(f\"Confidence threshold: {self.conf_threshold}\")\n",
    "        print(f\"IOU threshold: {self.iou_threshold}\")\n",
    "        print(f\"Tracking enabled: {use_tracking}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # CRITICAL: Run YOLO with tracking or detection based on mode\n",
    "            if use_tracking:\n",
    "                # Use tracking for stable IDs\n",
    "                results = self.model.track(\n",
    "                    frame, \n",
    "                    persist=True, \n",
    "                    conf=self.conf_threshold,\n",
    "                    iou=self.iou_threshold,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                # Use detection only (no tracking)\n",
    "                results = self.model(\n",
    "                    frame,\n",
    "                    conf=self.conf_threshold,\n",
    "                    iou=self.iou_threshold,\n",
    "                    verbose=False\n",
    "                )\n",
    "            \n",
    "            # CURRENT FRAME: Count pills in ROI for THIS frame only\n",
    "            current_pills_in_roi = set()  # Track IDs currently in ROI\n",
    "            current_count = 0  # Current detection count\n",
    "            \n",
    "            if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "                boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "                \n",
    "                # Get track IDs if tracking is enabled\n",
    "                if use_tracking and results[0].boxes.id is not None:\n",
    "                    track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "                else:\n",
    "                    # Generate dummy IDs for detection-only mode\n",
    "                    track_ids = list(range(len(boxes)))\n",
    "                \n",
    "                # Get masks if available\n",
    "                masks = results[0].masks\n",
    "                \n",
    "                # Process each detection\n",
    "                for i, (box, track_id) in enumerate(zip(boxes, track_ids)):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    center_x = int((x1 + x2) / 2)\n",
    "                    center_y = int((y1 + y2) / 2)\n",
    "                    \n",
    "                    # Check if pill center is in ROI\n",
    "                    if self.point_in_roi((center_x, center_y)):\n",
    "                        current_pills_in_roi.add(track_id)\n",
    "                        current_count += 1\n",
    "                        \n",
    "                        # Get color for this track ID\n",
    "                        color = self.generate_color(track_id)\n",
    "                        \n",
    "                        # Draw segmentation if enabled\n",
    "                        if show_segmentation and masks is not None:\n",
    "                            mask = masks.data[i].cpu().numpy()\n",
    "                            mask = cv2.resize(mask, (width, height))\n",
    "                            mask = (mask > 0.5).astype(np.uint8)\n",
    "                            \n",
    "                            # Create colored mask\n",
    "                            colored_mask = np.zeros_like(frame)\n",
    "                            colored_mask[mask == 1] = color\n",
    "                            \n",
    "                            # Blend with original frame\n",
    "                            frame = cv2.addWeighted(frame, 1, colored_mask, 0.5, 0)\n",
    "                            \n",
    "                            # Draw contour\n",
    "                            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, \n",
    "                                                          cv2.CHAIN_APPROX_SIMPLE)\n",
    "                            cv2.drawContours(frame, contours, -1, color, 2)\n",
    "                            \n",
    "                            # Calculate centroid for label placement\n",
    "                            if show_labels and len(contours) > 0:\n",
    "                                M = cv2.moments(contours[0])\n",
    "                                if M[\"m00\"] != 0:\n",
    "                                    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                                    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                                else:\n",
    "                                    cX, cY = center_x, center_y\n",
    "                                \n",
    "                                # Draw track ID\n",
    "                                label = f'{track_id}'\n",
    "                                (label_w, label_h), _ = cv2.getTextSize(\n",
    "                                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2\n",
    "                                )\n",
    "                                \n",
    "                                # Background rectangle\n",
    "                                cv2.rectangle(frame,\n",
    "                                            (cX - label_w//2 - 5, cY - label_h//2 - 5),\n",
    "                                            (cX + label_w//2 + 5, cY + label_h//2 + 5),\n",
    "                                            (0, 0, 0), -1)\n",
    "                                \n",
    "                                # Text\n",
    "                                cv2.putText(frame, label, \n",
    "                                          (cX - label_w//2, cY + label_h//2),\n",
    "                                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                        \n",
    "                        elif show_segmentation:\n",
    "                            # Fallback: draw circle\n",
    "                            cv2.circle(frame, (center_x, center_y), 15, color, -1)\n",
    "                            \n",
    "                            if show_labels:\n",
    "                                label = f'{track_id}'\n",
    "                                (label_w, label_h), _ = cv2.getTextSize(\n",
    "                                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2\n",
    "                                )\n",
    "                                cv2.putText(frame, label,\n",
    "                                          (center_x - label_w//2, center_y + label_h//2),\n",
    "                                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                        \n",
    "                        elif show_labels:\n",
    "                            # Only labels, no segmentation\n",
    "                            label = f'{track_id}'\n",
    "                            (label_w, label_h), _ = cv2.getTextSize(\n",
    "                                label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2\n",
    "                            )\n",
    "                            \n",
    "                            # Background rectangle\n",
    "                            cv2.rectangle(frame,\n",
    "                                        (center_x - label_w//2 - 5, center_y - label_h//2 - 5),\n",
    "                                        (center_x + label_w//2 + 5, center_y + label_h//2 + 5),\n",
    "                                        (0, 0, 0), -1)\n",
    "                            \n",
    "                            # Text\n",
    "                            cv2.putText(frame, label, \n",
    "                                      (center_x - label_w//2, center_y + label_h//2),\n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Draw ROI polygon with light transparent overlay\n",
    "            if self.roi:\n",
    "                pts = np.array(self.roi, np.int32)\n",
    "                pts = pts.reshape((-1, 1, 2))\n",
    "                \n",
    "                # Create overlay with light color\n",
    "                overlay = frame.copy()\n",
    "                cv2.fillPoly(overlay, [pts], (144, 238, 144))  # Light green\n",
    "                \n",
    "                # Blend overlay with frame (light transparency)\n",
    "                cv2.addWeighted(overlay, 0.15, frame, 0.85, 0, frame)\n",
    "                \n",
    "                # Draw polygon border\n",
    "                cv2.polylines(frame, [pts], True, (0, 255, 0), 3)\n",
    "            \n",
    "            # DISPLAY CURRENT COUNT (not cumulative)\n",
    "            count_text = f'Current Pills: {current_count}'\n",
    "            \n",
    "            # Get text size for background\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                count_text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 4\n",
    "            )\n",
    "            \n",
    "            # Draw background rectangle in upper right\n",
    "            cv2.rectangle(frame, \n",
    "                         (width - text_width - 30, 10),\n",
    "                         (width - 10, text_height + baseline + 30),\n",
    "                         (0, 0, 0), -1)\n",
    "            \n",
    "            # Draw text\n",
    "            cv2.putText(frame, count_text,\n",
    "                       (width - text_width - 20, text_height + 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 4)\n",
    "            \n",
    "            # Optional: Display frame number\n",
    "            frame_info = f'Frame: {frame_count}/{total_frames}'\n",
    "            cv2.putText(frame, frame_info, (20, 40),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "            \n",
    "            # Update statistics\n",
    "            if current_count > max_pills_seen:\n",
    "                max_pills_seen = current_count\n",
    "            frame_counts.append(current_count)\n",
    "            \n",
    "            # Write frame\n",
    "            out.write(frame)\n",
    "            \n",
    "            # Display progress\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Frame {frame_count}/{total_frames} | Current pills: {current_count} | Max seen: {max_pills_seen}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total frames processed: {frame_count}\")\n",
    "        print(f\"Maximum pills seen simultaneously: {max_pills_seen}\")\n",
    "        print(f\"Average pills per frame: {np.mean(frame_counts):.2f}\")\n",
    "        print(f\"Minimum pills seen: {np.min(frame_counts)}\")\n",
    "        print(f\"Output saved to: {output_path}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return max_pills_seen\n",
    "\n",
    "\n",
    "def count_pills_in_video(video_path, model_path='best.pt', output_path='output_pill_count.mp4', \n",
    "                         show_labels=True, show_segmentation=True, use_tracking=True,\n",
    "                         conf_threshold=0.6, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Main function to count CURRENT pills in a video (not cumulative)\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        model_path: Path to YOLO model (default: 'best.pt')\n",
    "        output_path: Path to save output video (default: 'output_pill_count.mp4')\n",
    "        show_labels: If True, display track IDs on pills (default: True)\n",
    "        show_segmentation: If True, display segmentation masks and contours (default: True)\n",
    "        use_tracking: If True, use ByteTrack for stable counting (default: True)\n",
    "        conf_threshold: Confidence threshold for detections (default: 0.6)\n",
    "        iou_threshold: IOU threshold for NMS (default: 0.5)\n",
    "        \n",
    "    Returns:\n",
    "        max_pills: Maximum number of pills seen simultaneously in the video\n",
    "    \"\"\"\n",
    "    counter = RealTimePillCounter(model_path, conf_threshold, iou_threshold)\n",
    "    max_pills = counter.process_video(video_path, output_path, show_labels, \n",
    "                                      show_segmentation, use_tracking)\n",
    "    return max_pills\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7cb89",
   "metadata": {},
   "source": [
    "**Purpose**: Provides a simple interface to the pill counting system with sensible defaults.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Usage Examples\n",
    "\n",
    "### 6.1 Basic Usage - Full Visualization with Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c95edd",
   "metadata": {},
   "source": [
    "**Recommended Settings**:\n",
    "- `show_labels=True`: Display track IDs for verification\n",
    "- `show_segmentation=False`: Cleaner visualization\n",
    "- `use_tracking=True`: Stable counting across frames\n",
    "- `conf_threshold=0.6`: Balanced precision/recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Process a video with current count (not cumulative)\n",
    "    video_path = r\"video_dataset\\pill sample 2.mp4\"  \n",
    "    model_path = r\"yolo12_segmentation\\train_run\\weights\\best.pt\"\n",
    "    output_path = \"counted_pills_full_2.mp4\"\n",
    "    \n",
    "    # Full visualization with tracking (recommended)\n",
    "    max_pills = count_pills_in_video(\n",
    "        video_path, \n",
    "        model_path, \n",
    "        output_path, \n",
    "        show_labels=True, \n",
    "        show_segmentation=False,\n",
    "        use_tracking=True,\n",
    "        conf_threshold=0.6,  # Adjust based on your model\n",
    "        iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"\\nMaximum pills seen simultaneously: {max_pills}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7650f145",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
