{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5f6175",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Pill Counting Using YOLOv12**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924e9d9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This documentation covers the implementation of a real-time pill counting system using YOLOv12 segmentation model. The solution processes video footage of pills, counts them in real-time using instance segmentation, and provides interactive ROI (Region of Interest) selection for focused analysis. This implementation is valuable for pharmaceutical quality control, inventory management, and automated medication dispensing systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Dependencies](#1-setup-and-dependencies)\n",
    "2. [Data Preparation](#2-data-preparation)\n",
    "3. [Model Training](#3-model-training)\n",
    "4. [Inference and Tracking](#4-inference-and-tracking)\n",
    "5. [Real-Time Pill Counting System](#5-real-time-pill-counting-system)\n",
    "6. [Usage Examples](#6-usage-examples)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "### 1.1 Clone YOLO Fine-tuning Utilities Repository\n",
    "\n",
    "This step clones the YOLO fine-tuning utilities repository from Labellerr, which provides essential tools for converting annotations and preparing datasets for YOLO model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0904b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9362f0",
   "metadata": {},
   "source": [
    "**Purpose**: Downloads utilities for annotation conversion and dataset preparation specifically designed for YOLO models.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Import YOLO Segmentation Conversion Module\n",
    "\n",
    "Import the specific function needed to convert video annotations into YOLO segmentation format, which is required for training instance segmentation models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.video_annotation.yolo_converter import convert_to_yolo_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11015e",
   "metadata": {},
   "source": [
    "**What it does**: Imports the conversion function that transforms annotated video data into YOLO-compatible segmentation format.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Convert Annotations to YOLO Segmentation Format\n",
    "\n",
    "This cell performs the actual conversion of video annotations to YOLO format and splits the dataset into train, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATION_FILE = \"annotations.json\"\n",
    "VIDEOS_DIRECTORY = \"new_video\"\n",
    "OUTPUT_DATASET_DIR = \"yolo_dataset_2\"\n",
    "\n",
    "\n",
    "convert_to_yolo_segmentation(\n",
    "    annotation_path=ANNOTATION_FILE,\n",
    "    videos_dir=VIDEOS_DIRECTORY,\n",
    "    use_split=True,\n",
    "    split_ratio=(0.7, 0.2, 0.1),\n",
    "    output_dir=OUTPUT_DATASET_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0856eda",
   "metadata": {},
   "source": [
    "**Parameters**:\n",
    "- `annotation_path`: Path to the JSON file containing video annotations\n",
    "- `videos_dir`: Directory containing the source video files\n",
    "- `use_split`: Boolean flag to enable dataset splitting\n",
    "- `split_ratio`: Tuple defining train/validation/test split (70%/20%/10%)\n",
    "- `output_dir`: Destination directory for the converted YOLO dataset\n",
    "\n",
    "**Output**: Creates a structured YOLO dataset with images and labels organized into train, val, and test folders, along with a `data.yaml` configuration file.\n",
    "\n",
    "> **Note**: The conversion process extracts frames from videos and generates corresponding YOLO segmentation labels in normalized polygon format.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Training\n",
    "\n",
    "### 3.1 Clear GPU Memory Cache\n",
    "\n",
    "Before training, it's important to clear GPU memory to avoid out-of-memory errors and ensure optimal resource utilization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc992778",
   "metadata": {},
   "source": [
    "**Purpose**: Releases cached GPU memory from previous operations.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Check GPU Status\n",
    "\n",
    "Verify GPU availability and memory status before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a92c84",
   "metadata": {},
   "source": [
    "**Output**: Displays GPU information including memory usage, temperature, and active processes.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Monitor GPU Memory Usage\n",
    "\n",
    "Check detailed GPU memory statistics programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory status\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
    "print(f\"Free: {torch.cuda.mem_get_info(0)[0]/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f2450",
   "metadata": {},
   "source": [
    "**Purpose**: Provides granular insight into GPU memory allocation for debugging and optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Import YOLO Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051e86e",
   "metadata": {},
   "source": [
    "**What it does**: Imports the Ultralytics YOLO library for model training and inference.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Train YOLOv12 Segmentation Model\n",
    "\n",
    "This is the main training cell that fine-tunes the YOLOv12 segmentation model on the pill dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolo12n-seg.yaml\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=r\"yolo_dataset\\data.yaml\",  # Path to your dataset YAML file\n",
    "    epochs=300,                         # Number of training epochs\n",
    "    imgsz=640,                         # Image size\n",
    "    batch=-1,                          # Batch size\n",
    "    device=0,                          # GPU device (0 for first GPU, 'cpu' for CPU)\n",
    "    workers=4,                         # Number of dataloader workers\n",
    "    project=\"yolo12_segmentation-2\",     # Project folder name\n",
    "    name=\"train_run\",                  # Experiment name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b2e9d",
   "metadata": {},
   "source": [
    "**Training Parameters**:\n",
    "- `data`: Path to the data.yaml configuration file\n",
    "- `epochs`: Number of training iterations (200 for thorough training)\n",
    "- `imgsz`: Input image size (640x640 pixels)\n",
    "- `batch`: Batch size (-1 enables auto-batch sizing based on GPU memory)\n",
    "- `device`: GPU device ID (0 for first GPU, 'cpu' for CPU training)\n",
    "- `workers`: Number of parallel data loading threads\n",
    "- `project`: Output directory for training artifacts\n",
    "- `name`: Specific run name for organization\n",
    "\n",
    "**Output**: The model will be trained and checkpoints saved to `yolo12_segmentation-2/train_run/weights/`. Key files include:\n",
    "- `best.pt`: Best performing model weights\n",
    "- `last.pt`: Weights from the final epoch\n",
    "- Training curves and validation metrics\n",
    "\n",
    "> **Training Tip**: The auto-batch feature (`batch=-1`) automatically determines the optimal batch size based on available GPU memory. Monitor the training output for metrics like box_loss, seg_loss, cls_loss, and mAP scores.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Inference and Tracking\n",
    "\n",
    "### 4.1 Run Inference with Tracking on Video\n",
    "\n",
    "This cell demonstrates basic inference and tracking on a video file using the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(r\"yolo12_segmentation\\train_run\\weights\\best.pt\")\n",
    "\n",
    "# Run inference on video\n",
    "results = model.track(\n",
    "    source=r\"new_video\\pill sample 2_trim.mp4\",\n",
    "    save=True,\n",
    "    conf=0.25,\n",
    "    iou=0.7,\n",
    "    show=True,\n",
    "    show_labels=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d22d5e",
   "metadata": {},
   "source": [
    "**Parameters**:\n",
    "- `source`: Path to input video file\n",
    "- `save`: Save output video with annotations\n",
    "- `conf`: Confidence threshold for detections (0.25 = 25% minimum confidence)\n",
    "- `iou`: IOU threshold for Non-Maximum Suppression (0.7)\n",
    "- `show`: Display results in real-time\n",
    "- `show_labels`: Toggle class label display\n",
    "\n",
    "**Output**: Generates annotated video with tracked pill instances and displays results in a window.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Real-Time Pill Counting System\n",
    "\n",
    "### 5.1 Real-Time Pill Counter Class Architecture\n",
    "\n",
    "The `RealTimePillCounter` class provides a comprehensive solution for counting pills in videos with the following capabilities:\n",
    "\n",
    "- Interactive polygon ROI selection\n",
    "- Point-in-polygon testing for spatial filtering\n",
    "- Unique color generation for tracked objects\n",
    "- Segmentation mask visualization\n",
    "- Frame-by-frame statistics tracking\n",
    "\n",
    "### 5.2 Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e352edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "class RealTimePillCounter:\n",
    "    def __init__(self, model_path='best.pt', conf_threshold=0.6, iou_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the real-time pill counter with YOLO model\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the trained YOLO model\n",
    "            conf_threshold: Confidence threshold for detections (default: 0.6)\n",
    "            iou_threshold: IOU threshold for NMS (default: 0.5)\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.roi = None\n",
    "        self.track_history = defaultdict(lambda: deque(maxlen=30))\n",
    "        \n",
    "    def select_roi(self, frame):\n",
    "        \"\"\"\n",
    "        Interactive ROI selection on the first frame using fullscreen and point input\n",
    "        Creates a polygon from selected vertices\n",
    "        \n",
    "        Args:\n",
    "            frame: First frame of the video\n",
    "            \n",
    "        Returns:\n",
    "            roi: Selected region vertices as list of points [(x1,y1), (x2,y2), ...]\n",
    "        \"\"\"\n",
    "        clone = frame.copy()\n",
    "        points = []\n",
    "        \n",
    "        def mouse_callback(event, x, y, flags, param):\n",
    "            if event == cv2.EVENT_LBUTTONDOWN:\n",
    "                points.append((x, y))\n",
    "                \n",
    "        # Create fullscreen window\n",
    "        window_name = 'Select ROI - Fullscreen'\n",
    "        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "        cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "        cv2.setMouseCallback(window_name, mouse_callback)\n",
    "        \n",
    "        print(\"\\n=== ROI Selection Instructions ===\")\n",
    "        print(\"1. Click on the screen to add vertices of the region\")\n",
    "        print(\"2. Add at least 3 points to form a polygon\")\n",
    "        print(\"3. Press SPACE to confirm selection\")\n",
    "        print(\"4. Press 'r' to reset points\")\n",
    "        print(\"5. Press ESC to exit\")\n",
    "        print(\"===================================\\n\")\n",
    "        \n",
    "        while True:\n",
    "            display = clone.copy()\n",
    "            \n",
    "            # Draw all clicked points\n",
    "            for i, point in enumerate(points):\n",
    "                cv2.circle(display, point, 8, (0, 255, 255), -1)\n",
    "                cv2.putText(display, f'V{i+1}', (point[0]+15, point[1]-15),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "            \n",
    "            # Draw polygon if we have at least 3 points\n",
    "            if len(points) >= 3:\n",
    "                # Draw semi-transparent polygon\n",
    "                overlay = display.copy()\n",
    "                pts = np.array(points, np.int32)\n",
    "                pts = pts.reshape((-1, 1, 2))\n",
    "                cv2.fillPoly(overlay, [pts], (0, 255, 0))\n",
    "                cv2.addWeighted(overlay, 0.2, display, 0.8, 0, display)\n",
    "                \n",
    "                # Draw polygon outline\n",
    "                cv2.polylines(display, [pts], True, (0, 255, 0), 3)\n",
    "                \n",
    "            # Draw lines between consecutive points\n",
    "            if len(points) >= 2:\n",
    "                for i in range(len(points) - 1):\n",
    "                    cv2.line(display, points[i], points[i+1], (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw instructions\n",
    "            instructions = [\n",
    "                f'Vertices: {len(points)} | Click to add vertices',\n",
    "                'SPACE: Confirm | R: Reset | ESC: Exit'\n",
    "            ]\n",
    "            \n",
    "            y_offset = 40\n",
    "            for i, text in enumerate(instructions):\n",
    "                # Background for text\n",
    "                (text_w, text_h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "                cv2.rectangle(display, (10, y_offset - 30 + i*50), \n",
    "                             (text_w + 30, y_offset + 10 + i*50), (0, 0, 0), -1)\n",
    "                cv2.putText(display, text, (20, y_offset + i*50),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.imshow(window_name, display)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            \n",
    "            if key == 32:  # SPACE key\n",
    "                if len(points) >= 3:\n",
    "                    self.roi = points.copy()\n",
    "                    print(f\"ROI polygon selected with {len(points)} vertices: {self.roi}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please select at least 3 vertices to form a polygon!\")\n",
    "                    \n",
    "            elif key == ord('r') or key == ord('R'):  # Reset\n",
    "                points.clear()\n",
    "                print(\"Vertices reset\")\n",
    "                \n",
    "            elif key == 27:  # ESC key\n",
    "                if len(points) >= 3:\n",
    "                    self.roi = points.copy()\n",
    "                    print(f\"ROI polygon selected with {len(points)} vertices: {self.roi}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Exiting without ROI selection\")\n",
    "                    break\n",
    "                \n",
    "        cv2.destroyWindow(window_name)\n",
    "        return self.roi\n",
    "    \n",
    "    def point_in_roi(self, point):\n",
    "        \"\"\"\n",
    "        Check if a point is inside the ROI polygon\n",
    "        \n",
    "        Args:\n",
    "            point: (x, y) coordinates\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if point is inside ROI polygon\n",
    "        \"\"\"\n",
    "        if self.roi is None:\n",
    "            return True\n",
    "        \n",
    "        # Convert ROI to numpy array for polygon check\n",
    "        pts = np.array(self.roi, np.int32)\n",
    "        result = cv2.pointPolygonTest(pts, point, False)\n",
    "        return result >= 0  # Returns >= 0 if inside or on the polygon\n",
    "    \n",
    "    def generate_color(self, track_id):\n",
    "        \"\"\"\n",
    "        Generate a unique color for each track ID\n",
    "        \n",
    "        Args:\n",
    "            track_id: Tracking ID\n",
    "            \n",
    "        Returns:\n",
    "            color: BGR color tuple\n",
    "        \"\"\"\n",
    "        np.random.seed(int(track_id))\n",
    "        color = tuple(map(int, np.random.randint(0, 255, 3)))\n",
    "        return color\n",
    "    \n",
    "    def process_video(self, video_path, output_path='output_pill_count.mp4', \n",
    "                     show_labels=True, show_segmentation=True, use_tracking=True):\n",
    "        \"\"\"\n",
    "        Process video for CURRENT pill counting (not cumulative)\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video\n",
    "            output_path: Path to save output video\n",
    "            show_labels: If True, display track IDs on pills (default: True)\n",
    "            show_segmentation: If True, display segmentation masks and contours (default: True)\n",
    "            use_tracking: If True, use ByteTrack for stable counting (default: True)\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Cannot open video: {video_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Read first frame for ROI selection\n",
    "        ret, first_frame = cap.read()\n",
    "        if not ret:\n",
    "            raise ValueError(\"Cannot read first frame\")\n",
    "        \n",
    "        # Select ROI\n",
    "        print(\"Select the region of interest...\")\n",
    "        self.select_roi(first_frame)\n",
    "        print(f\"ROI selected with {len(self.roi)} vertices\")\n",
    "        \n",
    "        # Reset video to beginning\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        \n",
    "        # Video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        # For statistics\n",
    "        max_pills_seen = 0\n",
    "        frame_counts = []\n",
    "        \n",
    "        print(\"Processing video...\")\n",
    "        print(f\"Total frames: {total_frames}\")\n",
    "        print(f\"Confidence threshold: {self.conf_threshold}\")\n",
    "        print(f\"IOU threshold: {self.iou_threshold}\")\n",
    "        print(f\"Tracking enabled: {use_tracking}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # CRITICAL: Run YOLO with tracking or detection based on mode\n",
    "            if use_tracking:\n",
    "                # Use tracking for stable IDs\n",
    "                results = self.model.track(\n",
    "                    frame, \n",
    "                    persist=True, \n",
    "                    conf=self.conf_threshold,\n",
    "                    iou=self.iou_threshold,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                # Use detection only (no tracking)\n",
    "                results = self.model(\n",
    "                    frame,\n",
    "                    conf=self.conf_threshold,\n",
    "                    iou=self.iou_threshold,\n",
    "                    verbose=False\n",
    "                )\n",
    "            \n",
    "            # CURRENT FRAME: Count pills in ROI for THIS frame only\n",
    "            current_pills_in_roi = set()  # Track IDs currently in ROI\n",
    "            current_count = 0  # Current detection count\n",
    "            \n",
    "            if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "                boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "                \n",
    "                # Get track IDs if tracking is enabled\n",
    "                if use_tracking and results[0].boxes.id is not None:\n",
    "                    track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "                else:\n",
    "                    # Generate dummy IDs for detection-only mode\n",
    "                    track_ids = list(range(len(boxes)))\n",
    "                \n",
    "                # Get masks if available\n",
    "                masks = results[0].masks\n",
    "                \n",
    "                # Process each detection\n",
    "                for i, (box, track_id) in enumerate(zip(boxes, track_ids)):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    center_x = int((x1 + x2) / 2)\n",
    "                    center_y = int((y1 + y2) / 2)\n",
    "                    \n",
    "                    # Check if pill center is in ROI\n",
    "                    if self.point_in_roi((center_x, center_y)):\n",
    "                        current_pills_in_roi.add(track_id)\n",
    "                        current_count += 1\n",
    "                        \n",
    "                        # Get color for this track ID\n",
    "                        color = self.generate_color(track_id)\n",
    "                        \n",
    "                        # Draw segmentation if enabled\n",
    "                        if show_segmentation and masks is not None:\n",
    "                            mask = masks.data[i].cpu().numpy()\n",
    "                            mask = cv2.resize(mask, (width, height))\n",
    "                            mask = (mask > 0.5).astype(np.uint8)\n",
    "                            \n",
    "                            # Create colored mask\n",
    "                            colored_mask = np.zeros_like(frame)\n",
    "                            colored_mask[mask == 1] = color\n",
    "                            \n",
    "                            # Blend with original frame\n",
    "                            frame = cv2.addWeighted(frame, 1, colored_mask, 0.5, 0)\n",
    "                            \n",
    "                            # Draw contour\n",
    "                            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, \n",
    "                                                          cv2.CHAIN_APPROX_SIMPLE)\n",
    "                            cv2.drawContours(frame, contours, -1, color, 2)\n",
    "                            \n",
    "                            # Calculate centroid for label placement\n",
    "                            if show_labels and len(contours) > 0:\n",
    "                                M = cv2.moments(contours[0])\n",
    "                                if M[\"m00\"] != 0:\n",
    "                                    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                                    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                                else:\n",
    "                                    cX, cY = center_x, center_y\n",
    "                                \n",
    "                                # Draw track ID\n",
    "                                label = f'{track_id}'\n",
    "                                (label_w, label_h), _ = cv2.getTextSize(\n",
    "                                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2\n",
    "                                )\n",
    "                                \n",
    "                                # Background rectangle\n",
    "                                cv2.rectangle(frame,\n",
    "                                            (cX - label_w//2 - 5, cY - label_h//2 - 5),\n",
    "                                            (cX + label_w//2 + 5, cY + label_h//2 + 5),\n",
    "                                            (0, 0, 0), -1)\n",
    "                                \n",
    "                                # Text\n",
    "                                cv2.putText(frame, label, \n",
    "                                          (cX - label_w//2, cY + label_h//2),\n",
    "                                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                        \n",
    "                        elif show_segmentation:\n",
    "                            # Fallback: draw circle\n",
    "                            cv2.circle(frame, (center_x, center_y), 15, color, -1)\n",
    "                            \n",
    "                            if show_labels:\n",
    "                                label = f'{track_id}'\n",
    "                                (label_w, label_h), _ = cv2.getTextSize(\n",
    "                                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2\n",
    "                                )\n",
    "                                cv2.putText(frame, label,\n",
    "                                          (center_x - label_w//2, center_y + label_h//2),\n",
    "                                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                        \n",
    "                        elif show_labels:\n",
    "                            # Only labels, no segmentation\n",
    "                            label = f'{track_id}'\n",
    "                            (label_w, label_h), _ = cv2.getTextSize(\n",
    "                                label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2\n",
    "                            )\n",
    "                            \n",
    "                            # Background rectangle\n",
    "                            cv2.rectangle(frame,\n",
    "                                        (center_x - label_w//2 - 5, center_y - label_h//2 - 5),\n",
    "                                        (center_x + label_w//2 + 5, center_y + label_h//2 + 5),\n",
    "                                        (0, 0, 0), -1)\n",
    "                            \n",
    "                            # Text\n",
    "                            cv2.putText(frame, label, \n",
    "                                      (center_x - label_w//2, center_y + label_h//2),\n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Draw ROI polygon with light transparent overlay\n",
    "            if self.roi:\n",
    "                pts = np.array(self.roi, np.int32)\n",
    "                pts = pts.reshape((-1, 1, 2))\n",
    "                \n",
    "                # Create overlay with light color\n",
    "                overlay = frame.copy()\n",
    "                cv2.fillPoly(overlay, [pts], (144, 238, 144))  # Light green\n",
    "                \n",
    "                # Blend overlay with frame (light transparency)\n",
    "                cv2.addWeighted(overlay, 0.15, frame, 0.85, 0, frame)\n",
    "                \n",
    "                # Draw polygon border\n",
    "                cv2.polylines(frame, [pts], True, (0, 255, 0), 3)\n",
    "            \n",
    "            # DISPLAY CURRENT COUNT (not cumulative)\n",
    "            count_text = f'Current Pills: {current_count}'\n",
    "            \n",
    "            # Get text size for background\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                count_text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 4\n",
    "            )\n",
    "            \n",
    "            # Draw background rectangle in upper right\n",
    "            cv2.rectangle(frame, \n",
    "                         (width - text_width - 30, 10),\n",
    "                         (width - 10, text_height + baseline + 30),\n",
    "                         (0, 0, 0), -1)\n",
    "            \n",
    "            # Draw text\n",
    "            cv2.putText(frame, count_text,\n",
    "                       (width - text_width - 20, text_height + 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 4)\n",
    "            \n",
    "            # Optional: Display frame number\n",
    "            frame_info = f'Frame: {frame_count}/{total_frames}'\n",
    "            cv2.putText(frame, frame_info, (20, 40),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "            \n",
    "            # Update statistics\n",
    "            if current_count > max_pills_seen:\n",
    "                max_pills_seen = current_count\n",
    "            frame_counts.append(current_count)\n",
    "            \n",
    "            # Write frame\n",
    "            out.write(frame)\n",
    "            \n",
    "            # Display progress\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Frame {frame_count}/{total_frames} | Current pills: {current_count} | Max seen: {max_pills_seen}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total frames processed: {frame_count}\")\n",
    "        print(f\"Maximum pills seen simultaneously: {max_pills_seen}\")\n",
    "        print(f\"Average pills per frame: {np.mean(frame_counts):.2f}\")\n",
    "        print(f\"Minimum pills seen: {np.min(frame_counts)}\")\n",
    "        print(f\"Output saved to: {output_path}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return max_pills_seen\n",
    "\n",
    "\n",
    "def count_pills_in_video(video_path, model_path='best.pt', output_path='output_pill_count.mp4', \n",
    "                         show_labels=True, show_segmentation=True, use_tracking=True,\n",
    "                         conf_threshold=0.6, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Main function to count CURRENT pills in a video (not cumulative)\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        model_path: Path to YOLO model (default: 'best.pt')\n",
    "        output_path: Path to save output video (default: 'output_pill_count.mp4')\n",
    "        show_labels: If True, display track IDs on pills (default: True)\n",
    "        show_segmentation: If True, display segmentation masks and contours (default: True)\n",
    "        use_tracking: If True, use ByteTrack for stable counting (default: True)\n",
    "        conf_threshold: Confidence threshold for detections (default: 0.6)\n",
    "        iou_threshold: IOU threshold for NMS (default: 0.5)\n",
    "        \n",
    "    Returns:\n",
    "        max_pills: Maximum number of pills seen simultaneously in the video\n",
    "    \"\"\"\n",
    "    counter = RealTimePillCounter(model_path, conf_threshold, iou_threshold)\n",
    "    max_pills = counter.process_video(video_path, output_path, show_labels, \n",
    "                                      show_segmentation, use_tracking)\n",
    "    return max_pills\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7cb89",
   "metadata": {},
   "source": [
    "**Purpose**: Provides a simple interface to the pill counting system with sensible defaults.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Usage Examples\n",
    "\n",
    "### 6.1 Basic Usage - Full Visualization with Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c95edd",
   "metadata": {},
   "source": [
    "**Recommended Settings**:\n",
    "- `show_labels=True`: Display track IDs for verification\n",
    "- `show_segmentation=False`: Cleaner visualization\n",
    "- `use_tracking=True`: Stable counting across frames\n",
    "- `conf_threshold=0.6`: Balanced precision/recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Process a video with current count (not cumulative)\n",
    "    video_path = r\"video_dataset\\pill sample 2.mp4\"  \n",
    "    model_path = r\"yolo12_segmentation\\train_run\\weights\\best.pt\"\n",
    "    output_path = \"counted_pills_full_2.mp4\"\n",
    "    \n",
    "    # Full visualization with tracking (recommended)\n",
    "    max_pills = count_pills_in_video(\n",
    "        video_path, \n",
    "        model_path, \n",
    "        output_path, \n",
    "        show_labels=True, \n",
    "        show_segmentation=False,\n",
    "        use_tracking=True,\n",
    "        conf_threshold=0.6,  # Adjust based on your model\n",
    "        iou_threshold=0.5\n",
    "    )\n",
    "    print(f\"\\nMaximum pills seen simultaneously: {max_pills}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
