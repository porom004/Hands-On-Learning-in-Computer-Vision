{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e1cb7a",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Real-Time Pull-Up Counter using YOLO Pose Estimation**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15036",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates an end-to-end computer vision pipeline for automated fitness tracking using a YOLO-based Pose Estimation model. The workflow covers dataset preparation (converting JSON annotations to YOLO format), model training, and a real-time inference logic that utilizes geometric heuristics to verify form and count pull-up repetitions accurately.\n",
    "\n",
    "#### Real-World Applications:\n",
    "* **Smart Fitness Apps:** AI-powered virtual coaching for automated rep counting and form correction.\n",
    "* **Gym & Performance Analytics:** Objective tracking of athlete progress and consistency in training facilities.\n",
    "* **Physical Therapy & Rehab:** Monitoring patient range of motion (ROM) and recovery milestones automatically.\n",
    "* **Virtual Competitions:** Automated verification of repetitions for remote fitness challenges to prevent cheating.\n",
    "* **Home Workout Assistants:** Hands-free tracking for users exercising without a personal trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a0c38",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "This section imports all the required libraries used throughout the project for computer vision, visualization, deep learning, and structured coding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d848a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "039fca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolo_finetune_utils' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Labellerr/yolo_finetune_utils.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9bf99",
   "metadata": {},
   "source": [
    "## Random Frame Extraction from Video\n",
    "\n",
    "Extracts a fixed number of high-quality frames from one or more videos to create an image dataset for annotation and training.\n",
    "\n",
    "### üîπ Purpose\n",
    "- Convert raw manufacturing videos into individual image frames  \n",
    "- Perform random sampling to avoid frame bias  \n",
    "- Prepare data for annotation and YOLO training  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9201c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found export-#3zxrWUBsgGneicfNw5Yj.zip. Extracting...\n",
      "‚úÖ Success! Files extracted to: dataset/\n",
      "You can now proceed to Step 2 (Training).\n"
     ]
    }
   ],
   "source": [
    "from yolo_finetune_utils.frame_extractor import extract_random_frames\n",
    "\n",
    "extract_random_frames(\n",
    "    paths=['Updated Pull Ups - Made with Clipchamp.mp4'],\n",
    "    total_images=25,\n",
    "    out_dir=\"frames\",\n",
    "    jpg_quality=100,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ddc6fb",
   "metadata": {},
   "source": [
    "## Data Preprocessing: JSON to YOLO Conversion\n",
    "\n",
    "### **Script Purpose**\n",
    "This script is a critical ETL (Extract, Transform, Load) tool designed to prepare your custom dataset for training a YOLO11 Pose Estimation model. \n",
    "\n",
    "The raw data comes in a JSON format (typical of labeling tools like Labellerr or Labelbox), where keypoints are stored as absolute pixel coordinates with string labels (e.g., \"left shoulder\"). YOLO cannot read this format directly.\n",
    "\n",
    "**What this script does:**\n",
    "1.  **Structure Mapping:** It maps specific body part names (Nose, Shoulders, Elbows, Wrists) to the strict numerical index order required by the model.\n",
    "2.  **Normalization:** It converts absolute pixel coordinates ($x, y$) into normalized values ($0.0 - 1.0$) relative to the image size. This ensures the model works regardless of image resolution.\n",
    "3.  **Bounding Box Generation:** YOLO Pose requires a bounding box around every person. Since the input data only contains points, this script automatically calculates the smallest box that fits all points plus a small padding.\n",
    "4.  **File Generation:** It splits the single JSON file into individual `.txt` files for every image, formatted exactly as YOLO expects:\n",
    "    * Format: `<class_id> <box_x> <box_y> <box_w> <box_h> <kpt1_x> <kpt1_y> <vis> ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453117f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "input_json_file = 'export-#ckMv3PRDGGdTI19hYySq.json' \n",
    "output_folder = 'labels'  \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "KEYPOINT_ORDER = [\n",
    "    \"nose\",\n",
    "    \"left shoulder\",\n",
    "    \"right shoulder\",\n",
    "    \"left elbow\",\n",
    "    \"right elbow\",\n",
    "    \"left wrist\",\n",
    "    \"right wrist\"\n",
    "]\n",
    "\n",
    "def convert_to_yolo():\n",
    "    with open(input_json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Processing {len(data)} images...\")\n",
    "\n",
    "    for entry in data:\n",
    "        file_name = entry['file_name']\n",
    "        \n",
    "        img_w = entry['file_metadata']['image_width']\n",
    "        img_h = entry['file_metadata']['image_height']\n",
    "        \n",
    "        keypoints_map = {} \n",
    "\n",
    "        if 'latest_answer' in entry and entry['latest_answer']:\n",
    "            annotations = entry['latest_answer']\n",
    "            \n",
    "            for ann in annotations:\n",
    "                if isinstance(ann['answer'], list) and len(ann['answer']) > 0:\n",
    "                    pt_data = ann['answer'][0]\n",
    "                    label = pt_data['label']\n",
    "                    x = pt_data['answer']['x']\n",
    "                    y = pt_data['answer']['y']\n",
    "                    \n",
    "                    keypoints_map[label] = (x, y)\n",
    "        yolo_kpts = []\n",
    "        valid_xs = []\n",
    "        valid_ys = []\n",
    "\n",
    "        for k_name in KEYPOINT_ORDER:\n",
    "            if k_name in keypoints_map:\n",
    "                x, y = keypoints_map[k_name]\n",
    "                norm_x = x / img_w\n",
    "                norm_y = y / img_h\n",
    "                yolo_kpts.extend([f\"{norm_x:.6f}\", f\"{norm_y:.6f}\", \"2\"]) # 2 = visible\n",
    "                \n",
    "                valid_xs.append(x)\n",
    "                valid_ys.append(y)\n",
    "            else:\n",
    "                \n",
    "                yolo_kpts.extend([\"0.000000\", \"0.000000\", \"0\"])\n",
    "\n",
    "        if valid_xs and valid_ys:\n",
    "            min_x, max_x = min(valid_xs), max(valid_xs)\n",
    "            min_y, max_y = min(valid_ys), max(valid_ys)\n",
    "\n",
    "            pad = 20\n",
    "            min_x = max(0, min_x - pad)\n",
    "            min_y = max(0, min_y - pad)\n",
    "            max_x = min(img_w, max_x + pad)\n",
    "            max_y = min(img_h, max_y + pad)\n",
    "            \n",
    "            bbox_w = (max_x - min_x)\n",
    "            bbox_h = (max_y - min_y)\n",
    "            bbox_x_center = min_x + (bbox_w / 2)\n",
    "            bbox_y_center = min_y + (bbox_h / 2)\n",
    "\n",
    "            norm_bbox_x = bbox_x_center / img_w\n",
    "            norm_bbox_y = bbox_y_center / img_h\n",
    "            norm_bbox_w = bbox_w / img_w\n",
    "            norm_bbox_h = bbox_h / img_h\n",
    "\n",
    "            \n",
    "            class_id = 0 \n",
    "            \n",
    "            line_parts = [\n",
    "                str(class_id),\n",
    "                f\"{norm_bbox_x:.6f}\",\n",
    "                f\"{norm_bbox_y:.6f}\",\n",
    "                f\"{norm_bbox_w:.6f}\",\n",
    "                f\"{norm_bbox_h:.6f}\"\n",
    "            ] + yolo_kpts\n",
    "\n",
    "            line = \" \".join(line_parts)\n",
    "\n",
    "            txt_filename = os.path.splitext(file_name)[0] + \".txt\"\n",
    "            with open(os.path.join(output_folder, txt_filename), 'w') as out_f:\n",
    "                out_f.write(line + \"\\n\")\n",
    "\n",
    "    print(f\" Conversion complete. Labels saved in '{output_folder}/'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_to_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "\n",
    "source_labels = \"labels\" \n",
    "\n",
    "source_images = \"frames\"  \n",
    "dataset_root = \"datasets/pullups\"\n",
    "train_ratio = 0.8 \n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Creates the YOLO standard directory structure\"\"\"\n",
    "    dirs = [\n",
    "        f\"{dataset_root}/images/train\",\n",
    "        f\"{dataset_root}/images/val\",\n",
    "        f\"{dataset_root}/labels/train\",\n",
    "        f\"{dataset_root}/labels/val\"\n",
    "    ]\n",
    "    for d in dirs:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(f\" Created directories in {dataset_root}\")\n",
    "\n",
    "def create_yaml():\n",
    "    \"\"\"Generates the data.yaml file needed for training\"\"\"\n",
    "    yaml_content = {\n",
    "        'path': os.path.abspath(dataset_root),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': {\n",
    "            0: 'person' \n",
    "        },\n",
    "        \n",
    "        'kpt_shape': [7, 3],\n",
    "        'flip_idx': [0, 2, 1, 4, 3, 6, 5] \n",
    "    }\n",
    "    \n",
    "    with open(f\"{dataset_root}/data.yaml\", 'w') as f:\n",
    "        yaml.dump(yaml_content, f, sort_keys=False)\n",
    "    print(f\"‚úÖ Created data.yaml at {dataset_root}/data.yaml\")\n",
    "\n",
    "def organize_files():\n",
    "    setup_directories()\n",
    "    create_yaml()\n",
    "\n",
    "    label_files = [f for f in os.listdir(source_labels) if f.endswith('.txt')]\n",
    "    \n",
    "    random.shuffle(label_files)\n",
    "    \n",
    "    split_index = int(len(label_files) * train_ratio)\n",
    "    train_files = label_files[:split_index]\n",
    "    val_files = label_files[split_index:]\n",
    "\n",
    "    print(f\"üîÑ Moving files: {len(train_files)} Train, {len(val_files)} Val\")\n",
    "\n",
    "    def move_batch(files, split_name):\n",
    "        for label_file in files:\n",
    "            \n",
    "            src_lbl = os.path.join(source_labels, label_file)\n",
    "            dst_lbl = os.path.join(dataset_root, \"labels\", split_name, label_file)\n",
    "            shutil.copy(src_lbl, dst_lbl) # Using copy to be safe\n",
    "\n",
    "            image_name = None\n",
    "            base_name = os.path.splitext(label_file)[0]\n",
    "            for ext in ['.jpg', '.png', '.jpeg']:\n",
    "                potential_img = os.path.join(source_images, base_name + ext)\n",
    "                if os.path.exists(potential_img):\n",
    "                    image_name = base_name + ext\n",
    "                    break\n",
    "            \n",
    "            if image_name:\n",
    "                src_img = os.path.join(source_images, image_name)\n",
    "                dst_img = os.path.join(dataset_root, \"images\", split_name, image_name)\n",
    "                shutil.copy(src_img, dst_img)\n",
    "            else:\n",
    "                print(f\" Warning: Image not found for {label_file}\")\n",
    "\n",
    "    move_batch(train_files, \"train\")\n",
    "    move_batch(val_files, \"val\")\n",
    "    print(\" Dataset organization complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    organize_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6c261",
   "metadata": {},
   "source": [
    "## Model Training: Fine-Tuning YOLO11 Pose\n",
    "\n",
    "### **Script Purpose**\n",
    "This script initiates the **transfer learning** process. It loads a pre-trained **YOLO11n-pose** model (which already understands general human structure) and fine-tunes it specifically on your custom dataset to accurately track the upper-body keypoints needed for pull-up analysis.\n",
    "\n",
    "### **The Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.8  Python-3.11.9 torch-2.9.1+cpu CPU (12th Gen Intel Core(TM) i5-1235U)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=datasets/pullups/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-pose.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=pose_train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=pullup_project, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\runs\\pose\\pullup_project\\pose_train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=pose, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml kpt_shape=[17, 3] with kpt_shape=[7, 3]\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    529084  ultralytics.nn.modules.head.Pose             [1, [7, 3], 16, None, [64, 128, 256]]\n",
      "YOLO11n-pose summary: 197 layers, 2,688,252 parameters, 2,688,236 gradients, 6.8 GFLOPs\n",
      "\n",
      "Transferred 505/541 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 237.235.1 MB/s, size: 785.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\datasets\\pullups\\labels\\train.cache... 20 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 269.527.8 MB/s, size: 802.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\datasets\\pullups\\labels\\val.cache... 5 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5  0.0s\n",
      "Plotting labels to D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\runs\\pose\\pullup_project\\pose_train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 87 weight(decay=0.0), 97 weight(decay=0.0005), 96 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mD:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\runs\\pose\\pullup_project\\pose_train2\u001b[0m\n",
      "Starting training for 25 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/25         0G      2.185      9.501     0.7342      5.103      1.713         11        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 12.5s/it 37.5s16.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 2.1s/it 2.1s\n",
      "                   all          5          5     0.0469        0.6      0.055     0.0375          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/25         0G      1.624       9.67       0.71      2.213      1.353          6        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 8.6s/it 25.7s<13.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.5s/it 1.5s\n",
      "                   all          5          5        0.8          1      0.895      0.716          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/25         0G      1.103      9.206      0.708     0.8684      1.012         14        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 7.7s/it 23.0s<12.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.8s/it 1.8s\n",
      "                   all          5          5      0.986          1      0.995      0.836          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/25         0G      1.187      8.865     0.7054     0.7848      1.023          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.7s/it 20.2s<10.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.4s/it 1.4s\n",
      "                   all          5          5      0.988          1      0.995      0.816          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/25         0G     0.9976        8.4     0.7053     0.6763      0.987          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.8s/it 20.3s<10.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.5s/it 1.5s\n",
      "                   all          5          5      0.988          1      0.995      0.892          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/25         0G     0.9401       8.34     0.7041      0.574     0.9706          6        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 8.0s/it 24.0s<14.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.8s/it 1.8s\n",
      "                   all          5          5      0.988          1      0.995      0.864          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/25         0G     0.9484      8.056     0.6924       0.68     0.9908          6        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.9s/it 20.6s<10.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.5s/it 1.5s\n",
      "                   all          5          5      0.988          1      0.995      0.895          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/25         0G     0.8625      7.339     0.7042     0.5777     0.9148          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.7s/it 20.1s<10.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.3s/it 1.3s\n",
      "                   all          5          5      0.987          1      0.995      0.767          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/25         0G     0.8768      7.038     0.6888     0.6326      0.928          9        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.7s/it 20.2s<10.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.5s/it 1.5s\n",
      "                   all          5          5      0.987          1      0.995      0.767          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/25         0G     0.7712       6.99     0.6894     0.5415     0.8827         10        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.7s/it 20.2s<10.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.4s/it 1.4s\n",
      "                   all          5          5      0.988          1      0.995      0.799          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/25         0G      0.849      6.794     0.6978     0.5648     0.9511          9        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.8s/it 20.5s<10.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.4s/it 1.4s\n",
      "                   all          5          5      0.988          1      0.995      0.803          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/25         0G     0.8571      6.812       0.69     0.6205     0.9082          9        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.7s/it 20.2s<10.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.5s/it 1.5s\n",
      "                   all          5          5      0.988          1      0.995      0.796          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/25         0G     0.9092      6.706     0.7016     0.5965      0.988          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 8.1s/it 24.3s<11.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.6s/it 1.6s\n",
      "                   all          5          5      0.987          1      0.995       0.86          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/25         0G     0.7826      6.416     0.7007     0.5157     0.9061          7        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.6s/it 19.8s<10.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.3s/it 1.3s\n",
      "                   all          5          5      0.987          1      0.995       0.86          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/25         0G     0.8504      6.614     0.7133     0.5641     0.9401         10        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.7s/it 20.1s<10.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.5s/it 1.5s\n",
      "                   all          5          5      0.987          1      0.995       0.86          0          0          0          0\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/25         0G     0.7121      5.567      0.666     0.4862     0.8579          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.9s/it 20.6s<11.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.5s/it 1.5s\n",
      "                   all          5          5      0.985          1      0.995       0.86          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/25         0G     0.6793      5.448     0.6637     0.5203     0.9136          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.6s/it 19.9s<11.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.6s/it 1.6s\n",
      "                   all          5          5       0.98          1      0.995      0.832          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/25         0G     0.6489      5.142     0.6681     0.4659     0.8995          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.4s/it 19.3s<10.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.4s/it 1.4s\n",
      "                   all          5          5       0.98          1      0.995      0.832          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/25         0G     0.6534      5.404     0.6766     0.5145     0.8637          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 7.2s/it 21.6s<11.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.7s/it 1.7s\n",
      "                   all          5          5      0.987          1      0.995       0.84          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/25         0G        0.7      5.165     0.6522     0.4531      0.914          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.7s/it 20.1s<10.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.4s/it 1.4s\n",
      "                   all          5          5      0.989          1      0.995      0.864          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/25         0G     0.6474       5.09     0.6593      0.491     0.8852          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.7s/it 20.0s<10.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.2s/it 1.2s\n",
      "                   all          5          5      0.989          1      0.995      0.864          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/25         0G     0.5883      5.574      0.688     0.4531      0.885          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.6s/it 19.9s<10.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.5s/it 1.5s\n",
      "                   all          5          5      0.987          1      0.995      0.864          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/25         0G     0.5312       4.78     0.6593     0.4512     0.8404          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.4s/it 19.3s<10.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.6s/it 1.6s\n",
      "                   all          5          5      0.987          1      0.995      0.864          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/25         0G     0.5421      4.465     0.6391     0.4398     0.8604          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.6s/it 19.9s<10.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.3s/it 1.3s\n",
      "                   all          5          5      0.985          1      0.995      0.864          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/25         0G     0.5244      4.979      0.665     0.4052     0.8305          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 6.4s/it 19.1s<10.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.4s/it 1.4s\n",
      "                   all          5          5      0.985          1      0.995      0.864          0          0          0          0\n",
      "\n",
      "25 epochs completed in 0.167 hours.\n",
      "Optimizer stripped from D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\runs\\pose\\pullup_project\\pose_train2\\weights\\last.pt, 5.7MB\n",
      "Optimizer stripped from D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\runs\\pose\\pullup_project\\pose_train2\\weights\\best.pt, 5.7MB\n",
      "\n",
      "Validating D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\runs\\pose\\pullup_project\\pose_train2\\weights\\best.pt...\n",
      "Ultralytics 8.4.8  Python-3.11.9 torch-2.9.1+cpu CPU (12th Gen Intel Core(TM) i5-1235U)\n",
      "YOLO11n-pose summary (fused): 110 layers, 2,680,438 parameters, 0 gradients, 6.7 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 1.3s/it 1.3s\n",
      "                   all          5          5      0.988          1      0.995      0.895          0          0          0          0\n",
      "Speed: 4.1ms preprocess, 175.5ms inference, 0.0ms loss, 4.6ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\Pull Ups\\runs\\pose\\pullup_project\\pose_train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolo11n-pose.pt') \n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data='datasets/pullups/data.yaml', \n",
    "    epochs=25,                         \n",
    "    imgsz=640,                         \n",
    "    batch=8,\n",
    "    project='pullup_project',          \n",
    "    name='pose_train'                  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56bc778",
   "metadata": {},
   "source": [
    "# Step 1: System Calibration (Setting the Bar Height)\n",
    "\n",
    "## Overview\n",
    "This script acts as the **calibration phase** of the project. Before the AI can count pull-ups, it needs to know *where* the pull-up bar is located in the video frame.\n",
    "\n",
    "Instead of hardcoding a pixel coordinate (which would break if the camera moves), this code opens an interactive window that allows the user to **click on the bar**. The Y-coordinate of that click is saved and used as the threshold for counting repetitions.\n",
    "\n",
    "### **Key Features**\n",
    "* **Interactive UI:** Uses OpenCV's mouse callback functionality to detect user clicks.\n",
    "* **Visual Feedback:** Displays a static frame of the video with on-screen instructions.\n",
    "* **Fail-Safe:** Loops indefinitely until a valid point is selected or the user cancels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de04d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " INSTRUCTION: A window has opened. CLICK the Pull-Up Bar to set the line.\n",
      " Rod Height Set at Y=206\n",
      " Configuration Complete. Rod Y-Coordinate is: 206\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video_source = \"Updated Pull Ups - Made with Clipchamp.mp4\" \n",
    "\n",
    "rod_y = None \n",
    "\n",
    "def set_rod_height(event, x, y, flags, param):\n",
    "    \"\"\"Mouse callback to set the height of the pull-up bar\"\"\"\n",
    "    global rod_y\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        rod_y = y\n",
    "        print(f\" Rod Height Set at Y={rod_y}\")\n",
    "\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\" Error: Could not open '{video_source}'\")\n",
    "else:\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        window_name = 'Set Rod Height'\n",
    "        cv2.namedWindow(window_name)\n",
    "        cv2.setMouseCallback(window_name, set_rod_height)\n",
    "        \n",
    "        print(f\" INSTRUCTION: A window has opened. CLICK the Pull-Up Bar to set the line.\")\n",
    "        \n",
    "        while rod_y is None:\n",
    "            \n",
    "            display_frame = frame.copy()\n",
    "            cv2.putText(display_frame, \"CLICK BAR TO SET HEIGHT\", (50, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.imshow(window_name, display_frame)\n",
    "            \n",
    "            if cv2.waitKey(10) == 27: \n",
    "                print(\"Setup cancelled.\")\n",
    "                break\n",
    "        \n",
    "        cv2.destroyWindow(window_name)\n",
    "        cap.release()\n",
    "        print(f\" Configuration Complete. Rod Y-Coordinate is: {rod_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf2450",
   "metadata": {},
   "source": [
    "* **Purpose:** To verify full range of motion. A pull-up is only valid if the user starts from a \"dead hang\" (arms straight, angle $\\approx 180^\\circ$).\n",
    "\n",
    "\n",
    "\n",
    "#### **2. The State Machine (Counting Logic)**\n",
    "Instead of just counting every time the head moves up, the code uses a **State Machine** to prevent false positives (like half-reps or jitter):\n",
    "* **State: \"DOWN\"**\n",
    "    * Triggered when the average arm angle exceeds **160 degrees**.\n",
    "    * This ensures the user has fully extended their arms before attempting a rep.\n",
    "* **State: \"UP\" (The Count)**\n",
    "    * Triggered *only if* the current state is \"DOWN\" **AND** the Nose Y-coordinate goes *above* the Bar Y-coordinate (`nose[1] < rod_y`).\n",
    "    * This increments the counter and locks the state to \"UP\" until the user extends their arms again.\n",
    "\n",
    "\n",
    "\n",
    "#### **3. Visualization Pipeline**\n",
    "The script overlays rich visual feedback onto the video for debugging and user experience:\n",
    "* **Skeleton Tracking:** Draws lines between joints (Shoulder $\\to$ Elbow $\\to$ Wrist) using `cv2.line`.\n",
    "* **Joint Markers:** Places yellow circles on key joints using `cv2.circle`.\n",
    "* **Live Metrics:** Displays the real-time elbow angle and rep count directly on the screen.\n",
    "* **Dynamic Bar Line:** The horizontal bar line changes color from **Red** (Down/Reset) to **Green** (Up/Success).\n",
    "\n",
    "#### **4. Output Management**\n",
    "* **Video Writer:** It initializes `cv2.VideoWriter` to save the processed video with all overlays to `output_inference.mp4`.\n",
    "* **Resource Management:** A `try...finally` block ensures that the video file is properly saved and closed, even if the script is interrupted or encounters an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1035b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading YOLO Model...\n",
      "‚úÖ Model Loaded.\n",
      "üöÄ Starting Inference... Output will be saved to 'output_inference.mp4'\n",
      "üí™ Rep 1 (Angle: 31)\n",
      "üí™ Rep 2 (Angle: 27)\n",
      "üí™ Rep 3 (Angle: 22)\n",
      "üí™ Rep 4 (Angle: 40)\n",
      "üí™ Rep 5 (Angle: 42)\n",
      "üí™ Rep 6 (Angle: 41)\n",
      "üí™ Rep 7 (Angle: 50)\n",
      "üí™ Rep 8 (Angle: 50)\n",
      "üí™ Rep 9 (Angle: 42)\n",
      "üí™ Rep 10 (Angle: 46)\n",
      "üí™ Rep 11 (Angle: 48)\n",
      "üí™ Rep 12 (Angle: 45)\n",
      "üí™ Rep 13 (Angle: 39)\n",
      "üí™ Rep 14 (Angle: 41)\n",
      "‚úÖ Done! Saved as 'output_inference.mp4'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = 'runs/pose/pullup_project/pose_train2/weights/best.pt'\n",
    "if rod_y is None:\n",
    "    print(\" Error: Please run Cell 1 first to set the rod height!\")\n",
    "else:\n",
    "    print(\"Loading YOLO Model...\")\n",
    "    model = YOLO(model_path)\n",
    "    print(\" Model Loaded.\")\n",
    "    output_filename = \"outputinference.mp4\"\n",
    "    \n",
    "    count = 0\n",
    "    stage = \"down\"\n",
    "    def calculate_angle(a, b, c):\n",
    "        \"\"\"Calculates angle between three points (a, b, c)\"\"\"\n",
    "        a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "        radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "        angle = np.abs(radians * 180.0 / np.pi)\n",
    "        if angle > 180.0: angle = 360 - angle\n",
    "        return angle\n",
    "\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    out = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    print(f\" Starting Inference... Output will be saved to '{output_filename}'\")\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "\n",
    "            # 1. Draw Rod Line (FIXED LINE BELOW)\n",
    "            line_color = (0, 255, 0) if stage == \"up\" else (0, 0, 255)\n",
    "            cv2.line(frame, (0, rod_y), (width, rod_y), line_color, 3)\n",
    "\n",
    "            # 2. Run AI\n",
    "            results = model(frame, verbose=False)\n",
    "            \n",
    "            for r in results:\n",
    "                if r.keypoints and len(r.keypoints.data) > 0:\n",
    "                    kp = r.keypoints.data[0].cpu().numpy()\n",
    "                    \n",
    "                    if kp[5][2] > 0.5: \n",
    "                        nose = kp[0][:2]\n",
    "                        l_sh, l_elb, l_wr = kp[5][:2], kp[7][:2], kp[9][:2]\n",
    "                        r_sh, r_elb, r_wr = kp[6][:2], kp[8][:2], kp[10][:2]\n",
    "\n",
    "                        left_angle = calculate_angle(l_sh, l_elb, l_wr)\n",
    "                        right_angle = calculate_angle(r_sh, r_elb, r_wr)\n",
    "                        avg_angle = int((left_angle + right_angle) / 2)\n",
    "\n",
    "                        \n",
    "                        cv2.line(frame, (int(l_sh[0]), int(l_sh[1])), (int(l_elb[0]), int(l_elb[1])), (255, 0, 255), 3)\n",
    "                        cv2.line(frame, (int(l_elb[0]), int(l_elb[1])), (int(l_wr[0]), int(l_wr[1])), (255, 0, 255), 3)\n",
    "                        cv2.line(frame, (int(r_sh[0]), int(r_sh[1])), (int(r_elb[0]), int(r_elb[1])), (255, 255, 0), 3)\n",
    "                        cv2.line(frame, (int(r_elb[0]), int(r_elb[1])), (int(r_wr[0]), int(r_wr[1])), (255, 255, 0), 3)\n",
    "\n",
    "                        for j in [l_sh, l_elb, l_wr, r_sh, r_elb, r_wr]:\n",
    "                            cv2.circle(frame, (int(j[0]), int(j[1])), 6, (0, 255, 255), -1)\n",
    "\n",
    "                        cv2.putText(frame, f\"{avg_angle}\", (int(l_elb[0] - 40), int(l_elb[1])), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "                        if avg_angle > 160: stage = \"down\"\n",
    "                        \n",
    "                        if nose[1] < rod_y and stage == \"down\":\n",
    "                            stage = \"up\"\n",
    "                            count += 1\n",
    "                            print(f\" Rep {count} (Angle: {avg_angle})\")\n",
    "\n",
    "            cv2.rectangle(frame, (0, 0), (220, 90), (245, 117, 16), -1)\n",
    "            cv2.putText(frame, \"REPS\", (15, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)\n",
    "            cv2.putText(frame, str(count), (15, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 3)\n",
    "            \n",
    "            cv2.putText(frame, \"STAGE\", (100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)\n",
    "            cv2.putText(frame, stage, (95, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 3)\n",
    "\n",
    "            out.write(frame)\n",
    "            cv2.imshow('Pull-up AI', frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\" Done! Saved as '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba49c43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e0e40",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
