{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134b78e1",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO For Fruits Counting**\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "This notebook demonstrates how to build an automated fruit counting system using a fine-tuned YOLO segmentation model. The project covers the entire workflow: extracting image frames from a source video, annotating the data, training a model to recognize and track fruits, and implementing a custom line-crossing logic to count them as they pass a specific point.\n",
    "\n",
    "\n",
    "## üöÄ Key Features\n",
    "\n",
    "* **Frame Extraction**: Automatically sample frames from a video to create an image dataset for training.\n",
    "* **Format Conversion**: Convert annotations from COCO JSON to the YOLO segmentation format.\n",
    "* **Model Training**: Fine-tune a pre-trained YOLOv11 segmentation model on the custom fruit dataset.\n",
    "* **Object Tracking**: Use the BotSort tracker with the custom model to track individual fruits across frames.\n",
    "* **Line-Crossing Counter**: Implement a custom logic to count objects as their tracked path intersects a predefined line.\n",
    "\n",
    "\n",
    "## üìö Libraries & Prerequisites\n",
    "\n",
    "* **Core Libraries**: `ultralytics`, `opencv-python`, `matplotlib`, `numpy`.\n",
    "* **Environment**: A Python environment with GPU support is highly recommended for training.\n",
    "* **Dataset**: A source video of fruits (e.g., on a conveyor belt) and an `annotation.json` file for the extracted frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfd103",
   "metadata": {},
   "source": [
    "## Annotate your Custom dataset using Labellerr\n",
    "\n",
    " ***1. Visit the [Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) website and click **‚ÄúSign Up‚Äù**.*** \n",
    "\n",
    " ***2. After signing in, create your workspace by entering a unique name.***\n",
    "\n",
    " ***3. Navigate to your workspace‚Äôs API keys page (e.g., `https://<your-workspace>.labellerr.com/workspace/api-keys`) to generate your **API Key** and **API Secret**.***\n",
    "\n",
    " ***4. Store the credentials securely, and then use them to initialise the SDK or API client with `api_key`, `api_secret`.*** \n",
    "\n",
    "\n",
    "### Use Labellerr SDK for uploading and perform annotation of your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to install required packages in a Jupyter notebook environment\n",
    "\n",
    "# !pip install git+https://github.com/Labellerr/SDKPython.git\n",
    "# !pip install ipyfilechooser\n",
    "# !git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports required for this notebook\n",
    "from labellerr.client import LabellerrClient\n",
    "from labellerr.core.datasets import create_dataset_from_local\n",
    "from labellerr.core.annotation_templates import create_template\n",
    "from labellerr.core.projects import create_project\n",
    "from labellerr.core.schemas import DatasetConfig, AnnotationQuestion, QuestionType, CreateTemplateParams, DatasetDataType, CreateProjectParams, RotationConfig\n",
    "from labellerr.core.projects import LabellerrProject\n",
    "from labellerr.core.exceptions import LabellerrError\n",
    "\n",
    "import uuid\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"YOUR_API_KEY\")        # go to labellerr workspace to get your API key\n",
    "api_secret = input(\"YOUR_API_SECRET\")  # go to labellerr workspace to get your API secret\n",
    "client_id = input(\"YOUR_CLIENT_ID\")   # Contact labellerr support to get your client ID i.e. support@tensormatics.com\n",
    "\n",
    "client = LabellerrClient(api_key, api_secret, client_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974187b7",
   "metadata": {},
   "source": [
    "### ***STEP-1: Create a dataset on labellerr from your local folder***\n",
    "\n",
    "The SDK supports in creating dataset by uploading local files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b48617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bab602150d0462d8a724c7f48ac1984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='D:\\', filename='', title='Select a folder containing your dataset', show_hidden=False, selec‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a folder chooser starting from a directory (for example, your home directory)\n",
    "chooser = FileChooser('/')\n",
    "\n",
    "# Set the chooser to folder selection mode only\n",
    "chooser.title = 'Select a folder containing your dataset'\n",
    "chooser.show_only_dirs = True\n",
    "\n",
    "# Display the widget\n",
    "display(chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6123a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\frames_output\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = chooser.selected_path\n",
    "print(\"You selected:\", path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64156ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset type: video\n"
     ]
    }
   ],
   "source": [
    "my_dataset_type = input(\"Enter your dataset type (video or image): \").lower()\n",
    "print(\"Selected dataset type:\", my_dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset_from_local(\n",
    "    client=client,\n",
    "    dataset_config=DatasetConfig(dataset_name=\"My Dataset\", data_type=\"image\"),\n",
    "    folder_to_upload=path_to_dataset\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with ID: {dataset.dataset_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb8de5",
   "metadata": {},
   "source": [
    "### ***STEP-2: Create annotation project on labellerr of your created dataset***\n",
    "\n",
    "Create a annotation project of your uploaded dataset to start performing annotation on labellerr UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation guideline template for video annotation project (like classes to be annotated)\n",
    "\n",
    "template = create_template(\n",
    "    client=client,\n",
    "    params=CreateTemplateParams(\n",
    "        template_name=\"My Template\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        questions=[\n",
    "            AnnotationQuestion(\n",
    "                question_number=1,\n",
    "                question=\"Object\",\n",
    "                question_id=str(uuid.uuid4()),\n",
    "                question_type=QuestionType.polygon,\n",
    "                required=True,\n",
    "                color=\"#FF0000\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(f\"Annotation template created with ID: {template.annotation_template_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.status()        # wait until dataset is processed before creating project\n",
    "\n",
    "project = create_project(\n",
    "    client=client,\n",
    "    params=CreateProjectParams(\n",
    "        project_name=\"My Project\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        rotations=RotationConfig(\n",
    "            annotation_rotation_count=1,\n",
    "            review_rotation_count=1,\n",
    "            client_review_rotation_count=1\n",
    "        )\n",
    "    ),\n",
    "    datasets=[dataset],\n",
    "    annotation_template=template\n",
    ")\n",
    "\n",
    "print(f\"‚úì Project created: {project.project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49edaa01",
   "metadata": {},
   "source": [
    "Your project has been created now go to labellerr platform to perform annotation \n",
    "\n",
    "***click to go to labellerr.com***\n",
    "\n",
    "[![Labellerr](https://cdn.labellerr.com/1%20%20Documentation/1c9dc7ce-9a54-4111-8fd5-0363ba3e00e1.webp)](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks)\n",
    "Open the project you created (Projects ‚Üí select your project).\n",
    "\n",
    "Click Start Labeling to open the annotation interface. Use the configured labeling tools (bounding boxes, polygon, dot, classification, etc.) to annotate files.\n",
    "### ***STEP-3: Export your annotation in required format***\n",
    "\n",
    "Generate a temporary download URL to retrieve your exported JSON file:\n",
    "\n",
    "### Export Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `export_name` | string | Display name for the export |\n",
    "| `export_description` | string | Description of what this export contains |\n",
    "| `export_format` | string | Output format (e.g., `json`, `xml`, `coco`) |\n",
    "| `statuses` | list | Annotation statuses to include in export |\n",
    "\n",
    "### Common Annotation Statuses\n",
    "\n",
    "- **`review`**: Annotations pending review\n",
    "- **`r_assigned`**: Review assigned to a reviewer\n",
    "- **`client_review`**: Under client review\n",
    "- **`cr_assigned`**: Client review assigned\n",
    "- **`accepted`**: Annotations accepted and finalized\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62850a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_config = {\n",
    "    \"export_name\": \"Weekly Export\",\n",
    "    \"export_description\": \"Export of all accepted annotations\",\n",
    "    \"export_format\": \"coco_json\",\n",
    "    \"statuses\": ['review', 'r_assigned','client_review', 'cr_assigned','accepted']\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Get project instance\n",
    "    project = LabellerrProject(client=client, project_id=project.project_id)\n",
    "    \n",
    "    # Create export\n",
    "    result = project.create_local_export(export_config)\n",
    "    export_id = result[\"response\"]['report_id']\n",
    "    print(f\"Local export created successfully. Export ID: {export_id}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Local export creation failed: {str(e)}\")\n",
    "    \n",
    "    \n",
    "try:\n",
    "    download_url = client.fetch_download_url(\n",
    "        project_id=project.project_id,\n",
    "        uuid=str(uuid.uuid4()),\n",
    "        export_id=export_id\n",
    "    )\n",
    "    print(f\"Download URL: {download_url}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Failed to fetch download URL: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f452e4",
   "metadata": {},
   "source": [
    "Now you can download your annotations locally using given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac62125",
   "metadata": {},
   "source": [
    "## **Converting COCO-JSON to YOLO format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efdefe",
   "metadata": {},
   "source": [
    "The annotations from the previous step are in COCO JSON format, which needs to be converted into the YOLO segmentation format for training. We use a helper script from the cloned repository to perform this conversion automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421dca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.seg_converter import coco_to_yolo_converter\n",
    "\n",
    "coco_to_yolo_converter(json_path=\"annotation.json\", images_dir=\"dataset\", output_dir=\"yolo_format\", seed=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e3bf9",
   "metadata": {},
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a03d6",
   "metadata": {},
   "source": [
    "With the dataset correctly formatted, we can now fine-tune the YOLO model. We'll use a pre-trained `yolo11m-seg.pt` model as a starting point and train it on our custom fruit dataset for 250 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04222696",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=segment mode=train data=\"./yolo_format/data.yaml\" model=\"yolo11m-seg.pt\" epochs=250 imgsz=640 batch=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165797bc",
   "metadata": {},
   "source": [
    "### **Tracking using Custom Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfd96a",
   "metadata": {},
   "source": [
    "After training, we can use our custom model for tracking objects in a video. The `yolo` command provides a simple way to apply the model and a specified tracker (like `botsort.yaml`) to a video, saving the output with visual tracking information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=segment mode=track tracker=botsort.yaml model=\"./runs/segment/train/weights/best.pt\" conf=0.2 source=\"./assests/2.mp4\" save=True show_labels=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed461a9b",
   "metadata": {},
   "source": [
    "### **Drawing Counter Line**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5645a91",
   "metadata": {},
   "source": [
    "To implement our custom counter, we first need to define a virtual line on the video frame. This section of code defines the line's coordinates based on the video's dimensions and then visualizes it on a sample frame to confirm its position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r'assests\\2.mp4'  # ‚Üê VIDEO PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"‚ùå Error: Cannot open video file: {video_path}\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 100)  # Set to frame number 100\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a374e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"‚ùå Error: Cannot open video file: {video_path}\")\n",
    "else:\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "print(f\"Width: {width}, Height: {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d08354",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = (1500,0), (1500,1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899064d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_point, end_point = line\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"‚ùå Error: Cannot open video file: {video_path}\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 100)  # Set to frame number 100\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if ret:\n",
    "    \n",
    "    cv2.line(frame, start_point, end_point, (255, 120, 255), 10)\n",
    "    \n",
    "    TEXT = \"COUNTING LINE\"\n",
    "    cv2.putText(frame, TEXT, (1510, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 120, 255), 10)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f878f",
   "metadata": {},
   "source": [
    "### **Fruits Counting Logic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33c027",
   "metadata": {},
   "source": [
    "This is the main part of the application. We define a `process_video` function that handles everything: loading the model, reading the video, running the tracker on each frame, and implementing the counting logic. It tracks the center point of each detected fruit and uses a line intersection function to check if a fruit's path has crossed our predefined counter line. If it has, and the fruit hasn't been counted before, the total count is incremented. The final video is saved with visual overlays showing the masks, tracking IDs, the counting line, and the live count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# GLOBAL VARIABLES\n",
    "# =============================================================================\n",
    "product_counter = 0\n",
    "perform_segmentation = False  # Set to True to enable segmentation visualization\n",
    "counting_line = (line)  # line coordinates\n",
    "video_path = \"assests/2.mp4\"\n",
    "output_video_path = \"output5.mp4\"\n",
    "model_path = \"./runs/segment/train/weights/best.pt\"  # Trained segmentation model\n",
    "model_confidence = 0.9  # Confidence threshold for YOLO model\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCTIONS\n",
    "# =============================================================================\n",
    "def load_yolo_model(model_path):\n",
    "    \"\"\"Load YOLO model\"\"\"\n",
    "    global model\n",
    "    try:\n",
    "        print(f\"Loading YOLO segmentation model: {model_path}\")\n",
    "        model = YOLO(model_path)\n",
    "        print(\"YOLO model loaded successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO model: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def line_intersection(p1, p2, p3, p4):\n",
    "    \"\"\"Check if line p1-p2 intersects with line p3-p4\"\"\"\n",
    "    def ccw(A, B, C):\n",
    "        return (C[1] - A[1]) * (B[0] - A[0]) > (B[1] - A[1]) * (C[0] - A[0])\n",
    "    return ccw(p1, p3, p4) != ccw(p2, p3, p4) and ccw(p1, p2, p3) != ccw(p1, p2, p4)\n",
    "\n",
    "\n",
    "def check_line_crossing(prev_pos, curr_pos, obj_id):\n",
    "    \"\"\"Check if object crosses the counting line\"\"\"\n",
    "    global product_counter, counted_objects\n",
    "    if line_intersection(prev_pos, curr_pos, counting_line[0], counting_line[1]):\n",
    "        if obj_id not in counted_objects:  # Only count once per object\n",
    "            counted_objects.add(obj_id)\n",
    "            product_counter += 1\n",
    "            print(f\"üéØ Object {obj_id} crossed the line! Total count: {product_counter}\")\n",
    "\n",
    "\n",
    "def process_video():\n",
    "    \"\"\"Main function to process video\"\"\"\n",
    "    global product_counter, counted_objects\n",
    "    \n",
    "    print(f\"Starting Product Counter\")\n",
    "    print(f\"Input: {video_path}\")\n",
    "    print(f\"Output: {output_video_path}\")\n",
    "    print(f\"Model: {model_path}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Load model\n",
    "    if not load_yolo_model(model_path):\n",
    "        return False\n",
    "\n",
    "    # Open input video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ùå Cannot open video: {video_path}\")\n",
    "        return False\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print(f\"Video: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
    "\n",
    "    # Create output video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Reset counter and tracking\n",
    "    product_counter = 0\n",
    "    counted_objects = set()  # Track which objects have been counted\n",
    "    track_history = {}  # {id: (prev_center)}\n",
    "\n",
    "    frame_count = 0\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    print(\"Processing...\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # Run YOLO segmentation + tracking\n",
    "        results = model.track(frame, conf=model_confidence, tracker=\"botsort.yaml\", persist=True, verbose=False)\n",
    "\n",
    "        if results and results[0].boxes.id is not None:\n",
    "            # Draw segmentation masks with color coding\n",
    "            if results[0].masks is not None:\n",
    "                masks = results[0].masks.xy  # list of polygons\n",
    "                boxes = results[0].boxes\n",
    "                \n",
    "                if perform_segmentation == True:\n",
    "                    for i, seg in enumerate(masks):\n",
    "                        if i < len(boxes):\n",
    "                            obj_id = int(boxes[i].id[0].cpu().numpy())\n",
    "                            \n",
    "                            # Color based on counting status\n",
    "                            if obj_id in counted_objects:\n",
    "                                # Yellow translucent for counted objects\n",
    "                                color = (0, 255, 255)  # BGR format: Yellow\n",
    "                                fill_color = (0, 255, 255, 100)  # Yellow with alpha\n",
    "                            else:\n",
    "                                # Purple translucent for uncounted objects\n",
    "                                color = (255, 0, 255)  # BGR format: Purple/Magenta\n",
    "                                fill_color = (255, 0, 255, 100)  # Purple with alpha\n",
    "                            \n",
    "                            pts = np.array(seg, dtype=np.int32)\n",
    "                            \n",
    "                            # Create overlay for translucent fill\n",
    "                            overlay = frame.copy()\n",
    "                            cv2.fillPoly(overlay, [pts], color)\n",
    "                            cv2.addWeighted(overlay, 0.3, frame, 0.7, 0, frame)\n",
    "                            \n",
    "                            # Draw outline\n",
    "                            cv2.polylines(frame, [pts], True, color, 2)\n",
    "\n",
    "            # Draw tracked objects\n",
    "            for box in results[0].boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                obj_id = int(box.id[0].cpu().numpy())\n",
    "                cls_id = int(box.cls[0].cpu().numpy())\n",
    "                conf = float(box.conf[0].cpu().numpy())\n",
    "\n",
    "                # Object center\n",
    "                center_x = int((x1 + x2) / 2)\n",
    "                center_y = int((y1 + y2) / 2)\n",
    "                center = (center_x, center_y)\n",
    "\n",
    "                # Check line crossing\n",
    "                if obj_id in track_history:\n",
    "                    prev_center = track_history[obj_id]\n",
    "                    check_line_crossing(prev_center, center, obj_id)\n",
    "                track_history[obj_id] = center\n",
    "\n",
    "                # Draw bounding box with color coding\n",
    "                box_color = (0, 255, 255) if obj_id in counted_objects else (255, 0, 255)\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), box_color, 2)\n",
    "                cv2.putText(frame, f\"ID:{obj_id}\", (int(x1), int(y1) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "\n",
    "        # Draw counting line\n",
    "        cv2.line(frame, counting_line[0], counting_line[1], (0, 255, 0), 10)\n",
    "        cv2.putText(frame, \"COUNTING LINE\",\n",
    "                    ((counting_line[0][0] + counting_line[1][0]) // 2 + 20,\n",
    "                     (counting_line[0][1] + counting_line[1][1]) // 2),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 10)\n",
    "\n",
    "        # Draw counter\n",
    "        cv2.rectangle(frame, (10, 10), (200, 60), (0, 0, 0), -1)\n",
    "        cv2.putText(frame, f\"COUNT: {product_counter}\", (20, 45),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show progress every 10%\n",
    "        if total_frames > 0 and frame_count % max(1, total_frames // 10) == 0:\n",
    "            progress = (frame_count / total_frames) * 100\n",
    "            print(f\"üìà {progress:.0f}% - Frame {frame_count}/{total_frames} - Count: {product_counter}\")\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    # Results\n",
    "    print(\"=\"*50)\n",
    "    print(\"Processing completed!\")\n",
    "    print(f\"Total count: {product_counter}\")\n",
    "    print(f\"Processing time: {processing_time}\")\n",
    "    print(f\"Output saved: {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c676ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ffd07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
