{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d65885",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO for Quality Inspection**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d3271",
   "metadata": {},
   "source": [
    "This notebook implements a complete pipeline for training a computer vision model and using it to inspect bottle quality on a manufacturing assembly line. It covers data preparation, model training, and a video processing inference loop.\n",
    "\n",
    "#### **Key Features**\n",
    "*   **Data Preparation**: Includes utilities to extract frames from videos and convert COCO-JSON annotations to YOLO segmentation format for training.\n",
    "*   **Model Training**: Fine-tunes a **YOLOv11 segmentation model** (`yolo11m-seg.pt`) on a custom bottle dataset.\n",
    "*   **Interactive ROI Selector**: Provides a UI tool (`get_polygon_points`) to interactively draw and define a \"Quality Inspection Zone\" polygon on a video frame.\n",
    "*   **Quality Inspection Logic**:\n",
    "    *   **Component Detection**: Identifies bottles, caps, and labels.\n",
    "    *   **Spatial Verification**: Uses intersection-over-union calculations to verify if a bottle has both a cap and a label properly attached.\n",
    "    *   **Zone Filtering**: Only inspects bottles that pass through the defined polygon zone.\n",
    "*   **Visual Feedback**:\n",
    "    *   **Dynamic Annotations**: Draws color-coded bounding boxes (Green/Bottle, Blue/Cap, Orange/Label) with \"PASS\" status indicators.\n",
    "    *   **Zone Visualization**: Displays the inspection zone as a red dotted line.\n",
    "    *   **Counters**: Tracks and displays a running count of passed bottles on-screen.\n",
    "\n",
    "#### **Workflow**\n",
    "1.  **Setup**: Import libraries and install custom YOLO finetuning utilities.\n",
    "2.  **Train**: Prepare dataset and fine-tune the YOLO model (300 epochs).\n",
    "3.  **Configure**: Interactively select the region of interest (ROI) from the input video.\n",
    "4.  **Inference**: Run the `BottleQualityInspector` class to process the video, tracking bottles and validating their assembly quality in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d15e12",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Import necessary libraries for computer vision, visualization, and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc790a",
   "metadata": {},
   "source": [
    "## Install Utilities\n",
    "Clone the YOLO finetuning utilities repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f476319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommet the following line if you have not cloned the repository\n",
    "# !git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014f5e9",
   "metadata": {},
   "source": [
    "## Extract Frames (Optional)\n",
    "Extract frames from video for dataset creation. (Currently commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.frame_extractor import extract_random_frames\n",
    "\n",
    "extract_random_frames(\n",
    "        paths=[r\"videos\\manufacturing_video_data\"],\n",
    "        total_images=150,\n",
    "        out_dir=\"manufacturing_dataset_frames\",\n",
    "        jpg_quality=100,\n",
    "        seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489971f",
   "metadata": {},
   "source": [
    "## Convert Annotations\n",
    "Convert COCO JSON annotations to YOLO format for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.seg_converter import coco_to_yolo_converter\n",
    "\n",
    "ANNOTATION_JSON = \"annotations.json\"\n",
    "IMAGE_DIR = \"manufacturing_dataset_frames\"\n",
    "\n",
    "\n",
    "coco_to_yolo_converter(\n",
    "        json_path=ANNOTATION_JSON,\n",
    "        images_dir=IMAGE_DIR,\n",
    "        output_dir=\"yolo_dataset\",\n",
    "        use_split=True,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1,\n",
    "        shuffle=True,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b14845",
   "metadata": {},
   "source": [
    "## Train YOLO Model\n",
    "Fine-tune the YOLOv11 segmentation model on the custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ed5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11m-seg.pt\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=r\"yolo_dataset\\data.yaml\",    # Path to your dataset YAML file\n",
    "    epochs=300,                        # Number of training epochs\n",
    "    imgsz=640,                         # Image size\n",
    "    batch=-1,                          # Batch size\n",
    "    device=0,                          # GPU device (0 for first GPU, 'cpu' for CPU)\n",
    "    workers=4                          # Number of dataloader workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMG = r\"yolo_dataset\\images\\test\\bottle manufacturing_frame_000225_t7.50s_000036.jpg\"\n",
    "result = model.predict(TEST_IMG, show_labels=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(result[0].plot(conf=False, labels=False)[:, :, ::-1]\n",
    ")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(video_path, save=True, conf=0.5, show_labels=False, stream=True)\n",
    "for result in results:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c08aa",
   "metadata": {},
   "source": [
    "## Helper: Extract and Display Frame\n",
    "Define a function to extract and display a specific frame from a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b45bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_nth_frame(video_path, frame_number):\n",
    "    \"\"\"\n",
    "    Extract the nth frame from a video and display it using matplotlib.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    video_path : str\n",
    "        Path to the video file\n",
    "    frame_number : int\n",
    "        The frame number to extract (0-indexed)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    frame : numpy.ndarray\n",
    "        The extracted frame in RGB format, or None if extraction fails\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Get total number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Check if the requested frame number is valid\n",
    "    if frame_number < 0 or frame_number >= total_frames:\n",
    "        print(f\"Error: Frame number {frame_number} is out of range (0-{total_frames-1})\")\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    # Set the frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read frame {frame_number}\")\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    # Convert BGR (OpenCV format) to RGB (matplotlib format)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the frame using matplotlib\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(frame_rgb)\n",
    "    plt.title(f'Frame {frame_number}')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb1c05",
   "metadata": {},
   "source": [
    "## Visualize Reference Frame\n",
    "Extract and display specific frames to check the video content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = r\"manufacturing_video_data\\bottle manufacturing.mp4\"\n",
    "\n",
    "frame_no = [0, 30, 50, 100]\n",
    "\n",
    "for frame in frame_no:\n",
    "    extract_nth_frame(VIDEO_PATH, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e33361",
   "metadata": {},
   "source": [
    "## Helper: Interactive ROI Selector\n",
    "Define a function to interactively draw a polygon ROI on a video frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891dc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polygon_points(video_path, frame_no=0):\n",
    "    \"\"\"\n",
    "    Draw a polygon on a video frame using OpenCV interactive UI.\n",
    "    \n",
    "    Controls:\n",
    "    - Left-click: Add point to polygon\n",
    "    - Right-click: Finish polygon (minimum 3 points)\n",
    "    - Press 'r': Reset and start over\n",
    "    - Press 'c': Complete polygon (same as right-click)\n",
    "    - Press 'Esc': Cancel and exit\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    video_path : str\n",
    "        Path to the video file\n",
    "    frame_no : int\n",
    "        Frame number to extract (default: 0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of polygon points [(x1, y1), (x2, y2), ...]\n",
    "           Returns None if drawing failed or cancelled\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> polygon = get_polygon_points(\"video.mp4\", frame_no=100)\n",
    "    >>> print(polygon)\n",
    "    [(100, 200), (300, 200), (300, 400), (100, 400)]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract frame\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ùå Error: Could not open video {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        print(f\"‚ùå Error: Could not read frame {frame_no}\")\n",
    "        return None\n",
    "    \n",
    "    # State variables\n",
    "    points = []\n",
    "    drawing_complete = False\n",
    "    frame_copy = frame.copy()\n",
    "    \n",
    "    # Mouse callback function\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        nonlocal points, drawing_complete\n",
    "        \n",
    "        # Left click - add point\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            points.append((x, y))\n",
    "            print(f\"‚úì Point {len(points)}: ({x}, {y})\")\n",
    "        \n",
    "        # Right click - finish polygon\n",
    "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "            if len(points) >= 3:\n",
    "                drawing_complete = True\n",
    "                print(f\"‚úì Polygon complete: {len(points)} points\")\n",
    "            else:\n",
    "                print(f\"‚ö† Need at least 3 points (currently have {len(points)})\")\n",
    "    \n",
    "    # Create window\n",
    "    window_name = \"Draw Polygon Perimeter\"\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.setMouseCallback(window_name, mouse_callback)\n",
    "    \n",
    "    # Print instructions\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"POLYGON DRAWING - INTERACTIVE UI\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"  Left-click:  Add point to polygon\")\n",
    "    print(\"  Right-click: Finish polygon (minimum 3 points)\")\n",
    "    print(\"  Press 'r':   Reset and start over\")\n",
    "    print(\"  Press 'c':   Complete polygon\")\n",
    "    print(\"  Press 'Esc': Cancel and exit\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Create display frame\n",
    "        display_frame = frame_copy.copy()\n",
    "        \n",
    "        # Draw all points\n",
    "        for i, point in enumerate(points):\n",
    "            # Draw point\n",
    "            cv2.circle(display_frame, point, 6, (0, 255, 0), -1)\n",
    "            cv2.circle(display_frame, point, 8, (255, 255, 255), 2)\n",
    "            \n",
    "            # Add point number\n",
    "            cv2.putText(display_frame, str(i+1), \n",
    "                       (point[0] + 12, point[1] - 12),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(display_frame, str(i+1), \n",
    "                       (point[0] + 12, point[1] - 12),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)\n",
    "        \n",
    "        # Draw lines between consecutive points\n",
    "        if len(points) > 1:\n",
    "            for i in range(len(points) - 1):\n",
    "                cv2.line(display_frame, points[i], points[i+1], (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw closing line and fill if polygon is complete or has 3+ points\n",
    "        if len(points) >= 3:\n",
    "            # Draw closing line\n",
    "            cv2.line(display_frame, points[-1], points[0], (0, 255, 0), 2)\n",
    "            \n",
    "            # Fill polygon with semi-transparent overlay\n",
    "            overlay = display_frame.copy()\n",
    "            pts = np.array(points, dtype=np.int32)\n",
    "            cv2.fillPoly(overlay, [pts], (0, 255, 0))\n",
    "            cv2.addWeighted(overlay, 0.3, display_frame, 0.7, 0, display_frame)\n",
    "        \n",
    "        # Add status text at the top\n",
    "        status_text = f\"Points: {len(points)}\"\n",
    "        if drawing_complete:\n",
    "            status_text = f\"COMPLETE - {len(points)} points (Press any key to exit)\"\n",
    "            cv2.rectangle(display_frame, (5, 5), (750, 45), (0, 200, 0), -1)\n",
    "        else:\n",
    "            cv2.rectangle(display_frame, (5, 5), (600, 45), (0, 0, 0), -1)\n",
    "        \n",
    "        cv2.putText(display_frame, status_text, (15, 32),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "        \n",
    "        # Show frame\n",
    "        cv2.imshow(window_name, display_frame)\n",
    "        \n",
    "        # Handle keyboard input\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Esc - cancel\n",
    "        if key == 27:\n",
    "            print(\"‚ùå Cancelled by user\")\n",
    "            cv2.destroyAllWindows()\n",
    "            return None\n",
    "        \n",
    "        # 'r' - reset\n",
    "        elif key == ord('r') or key == ord('R'):\n",
    "            points = []\n",
    "            drawing_complete = False\n",
    "            print(\"üîÑ Reset - draw a new polygon\")\n",
    "        \n",
    "        # 'c' - complete\n",
    "        elif key == ord('c') or key == ord('C'):\n",
    "            if len(points) >= 3:\n",
    "                drawing_complete = True\n",
    "                print(f\"‚úì Polygon complete: {len(points)} points\")\n",
    "            else:\n",
    "                print(f\"‚ö† Need at least 3 points (currently have {len(points)})\")\n",
    "        \n",
    "        # Exit if drawing is complete\n",
    "        if drawing_complete:\n",
    "            cv2.waitKey(1500)  # Show final result for 1.5 seconds\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    if len(points) < 3:\n",
    "        print(f\"‚ö† No valid polygon drawn (need at least 3 points)\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ Polygon saved with {len(points)} vertices\")\n",
    "    print(f\"Coordinates: {points}\\n\")\n",
    "    return points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce17e9",
   "metadata": {},
   "source": [
    "## Define Region of Interest\n",
    "Launch the interactive tool to define the polygon ROI on a selected frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_of_interest = get_polygon_points(VIDEO_PATH, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f388d1",
   "metadata": {},
   "source": [
    "## Helper: Annotate ROI\n",
    "Define a function to verify the selected ROI by overlaying it on the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc475f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_frame_with_polygon(video_path, frame_number, polygon_coords):\n",
    "    \"\"\"\n",
    "    Annotate a specific video frame with a polygon region.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    video_path : str\n",
    "        Path to the video file.\n",
    "    frame_number : int\n",
    "        The frame number to extract and annotate.\n",
    "    polygon_coords : list\n",
    "        List of (x, y) tuples defining the polygon vertices.\n",
    "        Example: [(2, 2150), (1262, 1348), (3838, 1402), (3838, 2155), (15, 2158)]\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    # Set frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read frame {frame_number}\")\n",
    "        return\n",
    "    # Create a copy for drawing\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    # Convert polygon coordinates to numpy array\n",
    "    pts = np.array(polygon_coords, np.int32)\n",
    "    pts = pts.reshape((-1, 1, 2))\n",
    "    \n",
    "    # Draw polygon: Magenta color, thickness 5\n",
    "    cv2.polylines(annotated_frame, [pts], isClosed=True, color=(255, 0, 255), thickness=5)\n",
    "    \n",
    "    # Optional: Fill polygon with semi-transparent overlay\n",
    "    overlay = annotated_frame.copy()\n",
    "    cv2.fillPoly(overlay, [pts], (255, 0, 255))\n",
    "    cv2.addWeighted(overlay, 0.2, annotated_frame, 0.8, 0, annotated_frame)\n",
    "    # Convert to RGB for matplotlib display\n",
    "    frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(frame_rgb)\n",
    "    plt.title(f\"Frame {frame_number} with ROI\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4929a2b",
   "metadata": {},
   "source": [
    "## Verify ROI Annotation\n",
    "Display the frame with the defined polygon overlay to verify accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_frame_with_polygon(video_path, 50, region_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093cb6c",
   "metadata": {},
   "source": [
    "## Quality Inspection class\n",
    "Run the `BottleQualityInspector` class to process the video, tracking bottles and validating their assembly quality in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85844885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityInspector:\n",
    "    \"\"\"bottle quality inspection using YOLO segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, zone_polygon: List[Tuple[int, int]], \n",
    "                 conf_threshold: float = 0.5, tracker: str = 'bytetrack.yaml'):\n",
    "        self.model = YOLO(model_path)\n",
    "        self.zone_polygon = np.array(zone_polygon, dtype=np.int32)\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.tracker = tracker\n",
    "        self.pass_count = 0\n",
    "        self.tracked_bottles = {}  # {track_id: bool} passed status\n",
    "    \n",
    "    def _is_box_in_zone(self, box: np.ndarray) -> bool:\n",
    "        \"\"\"Check if box center-bottom is inside zone polygon.\"\"\"\n",
    "        x1, y1, x2, y2 = box\n",
    "        center_bottom = ((x1 + x2) / 2, y2)\n",
    "        return cv2.pointPolygonTest(self.zone_polygon, center_bottom, False) >= 0\n",
    "    \n",
    "    def _calc_overlap(self, bottle_box: np.ndarray, comp_box: np.ndarray) -> float:\n",
    "        \"\"\"Calculate overlap percentage of component with bottle.\"\"\"\n",
    "        b_x1, b_y1, b_x2, b_y2 = bottle_box\n",
    "        c_x1, c_y1, c_x2, c_y2 = comp_box\n",
    "        x_left, y_top = max(b_x1, c_x1), max(b_y1, c_y1)\n",
    "        x_right, y_bottom = min(b_x2, c_x2), min(b_y2, c_y2)\n",
    "        if x_right < x_left or y_bottom < y_top: return 0.0\n",
    "        intersection = (x_right - x_left) * (y_bottom - y_top)\n",
    "        comp_area = (c_x2 - c_x1) * (c_y2 - c_y1)\n",
    "        return (intersection / comp_area * 100) if comp_area > 0 else 0.0\n",
    "    \n",
    "    def _verify_quality(self, bottle_box, caps, labels, threshold=10.0) -> bool:\n",
    "        \"\"\"Check if bottle has cap AND label with sufficient overlap.\"\"\"\n",
    "        has_cap = any(self._calc_overlap(bottle_box, c) > threshold for c in caps)\n",
    "        has_label = any(self._calc_overlap(bottle_box, l) > threshold for l in labels)\n",
    "        return has_cap and has_label\n",
    "    \n",
    "    def _separate_by_class(self, results) -> Tuple[List, List, List]:\n",
    "        \"\"\"Separate detections: class 0=label, 1=bottle, 2=cap (from data.yaml)\"\"\"\n",
    "        bottles, caps, labels = [], [], []\n",
    "        if results[0].boxes is None or len(results[0].boxes) == 0:\n",
    "            return bottles, caps, labels\n",
    "        \n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        classes = results[0].boxes.cls.cpu().numpy()\n",
    "        confs = results[0].boxes.conf.cpu().numpy()\n",
    "        track_ids = results[0].boxes.id.cpu().numpy() if results[0].boxes.id is not None else np.arange(len(boxes))\n",
    "        \n",
    "        for box, cls, conf, tid in zip(boxes, classes, confs, track_ids):\n",
    "            if conf < self.conf_threshold: continue\n",
    "            cls, tid = int(cls), int(tid)\n",
    "            detection = (box, tid, conf)\n",
    "            if cls == 0: labels.append(detection)  # bottle label\n",
    "            elif cls == 1 and self._is_box_in_zone(box): bottles.append(detection)  # bottle\n",
    "            elif cls == 2: caps.append(detection)  # bottle cap\n",
    "        return bottles, caps, labels\n",
    "    \n",
    "    def _draw_dotted_line(self, frame, pt1, pt2, color, thickness=4, gap=15):\n",
    "        \"\"\"Draw dotted line between two points.\"\"\"\n",
    "        import math\n",
    "        dist = math.sqrt((pt2[0]-pt1[0])**2 + (pt2[1]-pt1[1])**2)\n",
    "        pts = [(int(pt1[0]*(1-i/dist)+pt2[0]*i/dist), int(pt1[1]*(1-i/dist)+pt2[1]*i/dist)) \n",
    "               for i in range(0, int(dist), gap)]\n",
    "        for i in range(0, len(pts)-1, 2):\n",
    "            if i+1 < len(pts): cv2.line(frame, pts[i], pts[i+1], color, thickness)\n",
    "    \n",
    "    def _annotate_frame(self, frame, bottles, caps, labels, quality_results) -> np.ndarray:\n",
    "        \"\"\"Draw annotations on frame.\"\"\"\n",
    "        annotated = frame.copy()\n",
    "        \n",
    "        # Red dotted zone polygon\n",
    "        pts = self.zone_polygon.reshape(-1, 2)\n",
    "        for i in range(len(pts)):\n",
    "            self._draw_dotted_line(annotated, tuple(pts[i]), tuple(pts[(i+1)%len(pts)]), (0,0,255), 4, 15)\n",
    "        \n",
    "        # Zone label \"Quality Inspection\"\n",
    "        zone_label = \"Quality Inspection\"\n",
    "        label_x, label_y = pts[0][0], pts[0][1] - 10  # Above top-left corner\n",
    "        cv2.putText(annotated, zone_label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Match caps/labels to bottles\n",
    "        cap_boxes, label_boxes = [c[0] for c in caps], [l[0] for l in labels]\n",
    "        confirmed_caps, confirmed_labels = set(), set()\n",
    "        for bx, _, _ in bottles:\n",
    "            for i, cb in enumerate(cap_boxes):\n",
    "                if self._calc_overlap(bx, cb) > 10: confirmed_caps.add(i)\n",
    "            for i, lb in enumerate(label_boxes):\n",
    "                if self._calc_overlap(bx, lb) > 10: confirmed_labels.add(i)\n",
    "        \n",
    "        # Draw bottles (GREEN)\n",
    "        for box, tid, _ in bottles:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            if quality_results.get(tid, False):\n",
    "                overlay = annotated.copy()\n",
    "                cv2.rectangle(overlay, (x1,y1), (x2,y2), (0,255,0), -1)\n",
    "                cv2.addWeighted(overlay, 0.3, annotated, 0.7, 0, annotated)\n",
    "            cv2.rectangle(annotated, (x1,y1), (x2,y2), (0,255,0), 3)\n",
    "        \n",
    "        # Draw caps (BLUE)\n",
    "        for i, (box, _, _) in enumerate(caps):\n",
    "            if i in confirmed_caps:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                overlay = annotated.copy()\n",
    "                cv2.rectangle(overlay, (x1,y1), (x2,y2), (255,100,0), -1)\n",
    "                cv2.addWeighted(overlay, 0.4, annotated, 0.6, 0, annotated)\n",
    "                cv2.rectangle(annotated, (x1,y1), (x2,y2), (255,100,0), 2)\n",
    "                # Text with background\n",
    "                text = \"CAP: PASS\"\n",
    "                (text_w, text_h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "                cv2.rectangle(annotated, (x1+4, y1-text_h-8), (x1+text_w+12, y1-4), (255, 100, 0), -1)\n",
    "                cv2.putText(annotated, text, (x1+8, y1-8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "        \n",
    "        # Draw labels (ORANGE)\n",
    "        for i, (box, _, _) in enumerate(labels):\n",
    "            if i in confirmed_labels:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                overlay = annotated.copy()\n",
    "                cv2.rectangle(overlay, (x1,y1), (x2,y2), (0,165,255), -1)\n",
    "                cv2.addWeighted(overlay, 0.4, annotated, 0.6, 0, annotated)\n",
    "                cv2.rectangle(annotated, (x1,y1), (x2,y2), (0,165,255), 2)\n",
    "                # Text with background\n",
    "                text = \"LABEL: PASS\"\n",
    "                (text_w, text_h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "                cv2.rectangle(annotated, (x1+4, y1-text_h-8), (x1+text_w+12, y1-4), (0, 165, 255), -1)\n",
    "                cv2.putText(annotated, text, (x1+8, y1-8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "        \n",
    "        return annotated\n",
    "    \n",
    "    def _draw_counter(self, frame) -> np.ndarray:\n",
    "        \"\"\"Draw pass counter in top-right corner.\"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        text = f\"Bottle Passed: {self.pass_count}\"\n",
    "        font, scale, thick = cv2.FONT_HERSHEY_SIMPLEX, 1.2, 2\n",
    "        (tw, th), _ = cv2.getTextSize(text, font, scale, thick)\n",
    "        x1, y1 = w - tw - 40, 20\n",
    "        overlay = frame.copy()\n",
    "        cv2.rectangle(overlay, (x1-10, y1), (x1+tw+10, y1+th+20), (0,0,0), -1)\n",
    "        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n",
    "        cv2.putText(frame, text, (x1, y1+th+10), font, scale, (255,255,255), thick)\n",
    "        return frame\n",
    "    \n",
    "    def process_frame(self, frame) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Process single frame and return annotated frame + results.\"\"\"\n",
    "        results = self.model.track(frame, conf=self.conf_threshold, persist=True, tracker=self.tracker, verbose=False)\n",
    "        bottles, caps, labels = self._separate_by_class(results)\n",
    "        cap_boxes, label_boxes = [c[0] for c in caps], [l[0] for l in labels]\n",
    "        \n",
    "        quality_results = {}\n",
    "        for box, tid, _ in bottles:\n",
    "            if tid not in self.tracked_bottles:\n",
    "                passed = self._verify_quality(box, cap_boxes, label_boxes)\n",
    "                self.tracked_bottles[tid] = passed\n",
    "                if passed: self.pass_count += 1\n",
    "            quality_results[tid] = self.tracked_bottles[tid]\n",
    "        \n",
    "        annotated = self._annotate_frame(frame, bottles, caps, labels, quality_results)\n",
    "        annotated = self._draw_counter(annotated)\n",
    "        return annotated, {'bottles': len(bottles), 'caps': len(caps), 'labels': len(labels), \n",
    "                           'passed': sum(quality_results.values()), 'total_passed': self.pass_count}\n",
    "    \n",
    "    def process_video(self, video_path: str, output_path: str = None, show_progress: bool = True) -> Dict:\n",
    "        \"\"\"Process video file and optionally save output.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps, total = int(cap.get(cv2.CAP_PROP_FPS)), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h)) if output_path else None\n",
    "        if show_progress: print(f\"Processing {video_path}: {w}x{h} @ {fps}fps, {total} frames\")\n",
    "        \n",
    "        frame_num = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            annotated, _ = self.process_frame(frame)\n",
    "            if writer: writer.write(annotated)\n",
    "            frame_num += 1\n",
    "            if show_progress and frame_num % 30 == 0:\n",
    "                print(f\"Progress: {frame_num/total*100:.1f}% ({frame_num}/{total})\")\n",
    "        \n",
    "        cap.release()\n",
    "        if writer: writer.release()\n",
    "        if show_progress: print(f\"\\nComplete! {self.pass_count}/20 bottles passed\")\n",
    "        return {'total_frames': frame_num, 'pass_count': self.pass_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1602e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inspection zone polygon\n",
    "zone = region_of_interest\n",
    "\n",
    "# Initialize inspector\n",
    "inspector = QualityInspector(\n",
    "    model_path='runs/segment/train/weights/best.pt',\n",
    "    zone_polygon=zone,\n",
    "    conf_threshold=0.35,\n",
    "    tracker='bytetrack.yaml'\n",
    ")\n",
    "\n",
    "# Process video\n",
    "stats = inspector.process_video(\n",
    "    video_path=VIDEO_PATH,\n",
    "    output_path='quality_check_output.mp4'\n",
    ")\n",
    "print(f\"Total bottles passed: {stats['pass_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157cded",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
