{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0ddc71",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Cell Segmentation Using YOLO**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a18238",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete workflow for segmenting and analyzing cells in microscopy video using the YOLO (You Only Look Once) segmentation model.\n",
    "\n",
    "#### Real-World Applications:\n",
    "- Cell culture analysis  \n",
    "- Automated microscopy workflows  \n",
    "- Biomedical image processing  \n",
    "- Quantifying cell morphology and movement over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce830a66",
   "metadata": {},
   "source": [
    "# COCO to YOLO Format Conversion\n",
    "\n",
    "Converts COCO-style segmentation annotations to YOLO segmentation dataset format.  \n",
    "- Requires: `annotation.json` and images in `frames_output` directory.\n",
    "- Output: Generated YOLO dataset folder.\n",
    "- Parameters: allows train/val split, shuffling, and verbose mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a9b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.frame_extractor import extract_random_frames\n",
    "\n",
    "extract_random_frames(\n",
    "        paths=[r\"D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\video\\cell segmentation_bw_trim.mp4\"],\n",
    "        total_images=5,\n",
    "        out_dir=\"frames_output\",\n",
    "        jpg_quality=100,\n",
    "        seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91813c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.seg_converter import coco_to_yolo_converter\n",
    "\n",
    "ANNOTATION_JSON = r\"annotation.json\"\n",
    "IMAGE_DIR = r\"frames_output\"\n",
    "\n",
    "\n",
    "coco_to_yolo_converter(\n",
    "        json_path=ANNOTATION_JSON,\n",
    "        images_dir=IMAGE_DIR,\n",
    "        output_dir=\"yolo_dataset\",\n",
    "        use_split=False,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.0,\n",
    "        shuffle=True,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcf71f",
   "metadata": {},
   "source": [
    "# Clear CUDA Cache\n",
    "\n",
    "Frees up GPU memory by clearing unused cached memory before starting new tasks or model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a10e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93d36a",
   "metadata": {},
   "source": [
    "# Check GPU Memory Status\n",
    "\n",
    "Prints the GPU memory currently allocated, cached, and free.  \n",
    "Useful for diagnosing memory usage before training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cabc90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n",
      "Free: 6.78 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory status\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
    "print(f\"Free: {torch.cuda.mem_get_info(0)[0]/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23584c04",
   "metadata": {},
   "source": [
    "# Load and Train YOLO Segmentation Model\n",
    "\n",
    "Loads the YOLO segmentation model and trains it using the converted YOLO dataset.\n",
    "- Data: Path to YOLO-style `data.yaml`\n",
    "- Parameters: epochs, image size, batch size, device, dataloader workers, experiment name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ad3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11m-seg.pt\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=r\"yolo_dataset\\data.yaml\",    # Path to your dataset YAML file\n",
    "    epochs=500,                        # Number of training epochs\n",
    "    imgsz=640,                         # Image size\n",
    "    batch=-1,                          # Batch size\n",
    "    device=0,                          # GPU device (0 for first GPU, 'cpu' for CPU)\n",
    "    workers=4,                         # Number of dataloader workers\n",
    "    name=\"Model m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a4cce",
   "metadata": {},
   "source": [
    "# CellSegmentationVideo Class Definition\n",
    "\n",
    "Defines the `CellSegmentationVideo` class for performing video cell segmentation using YOLO.\n",
    "- Initialization requires YOLO model path, confidence threshold, IOU threshold.\n",
    "- Contains methods for polygon area calculation, mask center localization, frame processing, and video processing.\n",
    "- Tracks cells, draws masks and borders, labels areas and track IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae749433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class CellSegmentationVideo:\n",
    "    def __init__(self, model_path, conf_threshold=0.25, iou_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the cell segmentation video inference class.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to trained YOLO segmentation model\n",
    "            conf_threshold: Confidence threshold for detections\n",
    "            iou_threshold: IOU threshold for NMS\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        \n",
    "        # Generate distinct colors for different tracked objects\n",
    "        np.random.seed(42)\n",
    "        self.colors = [tuple(map(int, np.random.randint(0, 255, 3))) \n",
    "                       for _ in range(100)]\n",
    "        \n",
    "        # Track history for smoothing\n",
    "        self.track_history = defaultdict(lambda: [])\n",
    "        \n",
    "    def calculate_polygon_area(self, points):\n",
    "        \"\"\"Calculate area of polygon using Shoelace formula.\"\"\"\n",
    "        x = points[:, 0]\n",
    "        y = points[:, 1]\n",
    "        return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n",
    "    \n",
    "    def get_polygon_center(self, points):\n",
    "        \"\"\"Calculate center of polygon.\"\"\"\n",
    "        moments = cv2.moments(points)\n",
    "        if moments['m00'] != 0:\n",
    "            cx = int(moments['m10'] / moments['m00'])\n",
    "            cy = int(moments['m01'] / moments['m00'])\n",
    "            return (cx, cy)\n",
    "        return (int(np.mean(points[:, 0])), int(np.mean(points[:, 1])))\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame with cell segmentation.\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame (BGR format)\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame with colored segmentation masks and areas\n",
    "        \"\"\"\n",
    "        # Run tracking (built-in YOLO tracker)\n",
    "        results = self.model.track(\n",
    "            frame, \n",
    "            persist=True,\n",
    "            conf=self.conf_threshold,\n",
    "            iou=self.iou_threshold,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        if results[0].masks is not None:\n",
    "            masks = results[0].masks.xy\n",
    "            boxes = results[0].boxes\n",
    "            \n",
    "            # Get track IDs if available\n",
    "            if boxes.id is not None:\n",
    "                track_ids = boxes.id.int().cpu().tolist()\n",
    "            else:\n",
    "                track_ids = list(range(len(masks)))\n",
    "            \n",
    "            for mask, track_id in zip(masks, track_ids):\n",
    "                if len(mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get color for this track\n",
    "                color = self.colors[track_id % len(self.colors)]\n",
    "                \n",
    "                # Convert mask to integer coordinates\n",
    "                mask_points = mask.astype(np.int32)\n",
    "                \n",
    "                # Draw filled polygon with transparency\n",
    "                overlay = annotated_frame.copy()\n",
    "                cv2.fillPoly(overlay, [mask_points], color)\n",
    "                cv2.addWeighted(overlay, 0.4, annotated_frame, 0.6, 0, annotated_frame)\n",
    "                \n",
    "                # Draw solid border\n",
    "                cv2.polylines(annotated_frame, [mask_points], True, color, 2)\n",
    "                \n",
    "                # Calculate area (in pixels squared)\n",
    "                area = self.calculate_polygon_area(mask_points)\n",
    "                \n",
    "                # Get center point\n",
    "                center = self.get_polygon_center(mask_points)\n",
    "                \n",
    "                # Display area at center\n",
    "                area_text = f\"{area:.0f} px sq\"\n",
    "                \n",
    "                # Get text size for background\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                font_scale = 0.5\n",
    "                thickness = 1\n",
    "                (text_w, text_h), baseline = cv2.getTextSize(\n",
    "                    area_text, font, font_scale, thickness\n",
    "                )\n",
    "                \n",
    "                # Draw background rectangle for text\n",
    "                cv2.rectangle(\n",
    "                    annotated_frame,\n",
    "                    (center[0] - text_w//2 - 5, center[1] - text_h//2 - 5),\n",
    "                    (center[0] + text_w//2 + 5, center[1] + text_h//2 + 5),\n",
    "                    (255, 255, 255),\n",
    "                    -1\n",
    "                )\n",
    "                \n",
    "                # Draw area text\n",
    "                cv2.putText(\n",
    "                    annotated_frame,\n",
    "                    area_text,\n",
    "                    (center[0] - text_w//2, center[1] + text_h//2),\n",
    "                    font,\n",
    "                    font_scale,\n",
    "                    (0, 0, 0),\n",
    "                    thickness\n",
    "                )\n",
    "                \n",
    "                # Optional: Draw track ID\n",
    "                cv2.putText(\n",
    "                    annotated_frame,\n",
    "                    f\"ID:{track_id}\",\n",
    "                    (center[0] - text_w//2, center[1] - text_h//2 - 10),\n",
    "                    font,\n",
    "                    0.4,\n",
    "                    color,\n",
    "                    1\n",
    "                )\n",
    "        \n",
    "        return annotated_frame\n",
    "    \n",
    "    def process_video(self, input_path, output_path, display=True):\n",
    "        \"\"\"\n",
    "        Process entire video file.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input video\n",
    "            output_path: Path to save output video\n",
    "            display: Whether to display video while processing\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Setup video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        print(f\"Processing video: {total_frames} frames at {fps} FPS\")\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Process frame\n",
    "            annotated_frame = self.process_frame(frame)\n",
    "            \n",
    "            # Write to output\n",
    "            out.write(annotated_frame)\n",
    "            \n",
    "            # Display if requested\n",
    "            if display:\n",
    "                cv2.imshow('Cell Segmentation', annotated_frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "        \n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"Video saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f4bb3",
   "metadata": {},
   "source": [
    "# Specify Model Path\n",
    "\n",
    "Specifies the file path to the trained YOLO segmentation model weights.  \n",
    "Update this path as needed to match your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712bb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\runs\\segment\\Model m\\weights\\last.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712cab0e",
   "metadata": {},
   "source": [
    "# Example Usageâ€”Run Segmentation on a Video\n",
    "\n",
    "Demonstrates how to use the `CellSegmentationVideo` class:\n",
    "- Initializes the processor.\n",
    "- Runs segmentation inference on a sample video.\n",
    "- Saves the output and optionally displays annotated frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03602aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 423 frames at 30 FPS\n",
      "Processed 30/423 frames\n",
      "Processed 60/423 frames\n",
      "Processed 90/423 frames\n",
      "Processed 120/423 frames\n",
      "Processed 150/423 frames\n",
      "Processed 180/423 frames\n",
      "Processed 210/423 frames\n",
      "Processed 240/423 frames\n",
      "Processed 270/423 frames\n",
      "Processed 300/423 frames\n",
      "Processed 330/423 frames\n",
      "Processed 360/423 frames\n",
      "Processed 390/423 frames\n",
      "Processed 420/423 frames\n",
      "Video saved to: output_segmented_2.mp4\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the segmentation processor\n",
    "    segmentor = CellSegmentationVideo(\n",
    "        model_path=model_path,\n",
    "        conf_threshold=0.5,\n",
    "        iou_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    # Process a video file\n",
    "    segmentor.process_video(\n",
    "        input_path=r\"D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\video\\cell segmentation_bw.mp4\",\n",
    "        output_path=\"output_segmented_2.mp4\",\n",
    "        display=False\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
