{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0ddc71",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Cell Segmentation Using YOLO**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a18238",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete workflow for segmenting and analyzing cells in microscopy video using the YOLO (You Only Look Once) segmentation model.\n",
    "\n",
    "#### Real-World Applications:\n",
    "- Cell culture analysis  \n",
    "- Automated microscopy workflows  \n",
    "- Biomedical image processing  \n",
    "- Quantifying cell morphology and movement over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f51c5",
   "metadata": {},
   "source": [
    "## Annotate your Custom dataset using Labellerr\n",
    "\n",
    " ***1. Visit the [Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) website and click **‚ÄúSign Up‚Äù**.*** \n",
    "\n",
    " ***2. After signing in, create your workspace by entering a unique name.***\n",
    "\n",
    " ***3. Navigate to your workspace‚Äôs API keys page (e.g., `https://<your-workspace>.labellerr.com/workspace/api-keys`) to generate your **API Key** and **API Secret**.***\n",
    "\n",
    " ***4. Store the credentials securely, and then use them to initialise the SDK or API client with `api_key`, `api_secret`.*** \n",
    "\n",
    "\n",
    "### Use Labellerr SDK for uploading and perform annotation of your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f31b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to install required packages in a Jupyter notebook environment\n",
    "\n",
    "# !pip install git+https://github.com/Labellerr/SDKPython.git\n",
    "# !pip install ipyfilechooser\n",
    "# !git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports required for this notebook\n",
    "from labellerr.client import LabellerrClient\n",
    "from labellerr.core.datasets import create_dataset_from_local\n",
    "from labellerr.core.annotation_templates import create_template\n",
    "from labellerr.core.projects import create_project\n",
    "from labellerr.core.schemas import DatasetConfig, AnnotationQuestion, QuestionType, CreateTemplateParams, DatasetDataType, CreateProjectParams, RotationConfig\n",
    "from labellerr.core.projects import LabellerrProject\n",
    "from labellerr.core.exceptions import LabellerrError\n",
    "\n",
    "import uuid\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"YOUR_API_KEY\")        # go to labellerr workspace to get your API key\n",
    "api_secret = input(\"YOUR_API_SECRET\")  # go to labellerr workspace to get your API secret\n",
    "client_id = input(\"YOUR_CLIENT_ID\")   # Contact labellerr support to get your client ID i.e. support@tensormatics.com\n",
    "\n",
    "client = LabellerrClient(api_key, api_secret, client_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a03806",
   "metadata": {},
   "source": [
    "### ***STEP-1: Create a dataset on labellerr from your local folder***\n",
    "\n",
    "The SDK supports in creating dataset by uploading local files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777c4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bab602150d0462d8a724c7f48ac1984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='D:\\', filename='', title='Select a folder containing your dataset', show_hidden=False, selec‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a folder chooser starting from a directory (for example, your home directory)\n",
    "chooser = FileChooser('/')\n",
    "\n",
    "# Set the chooser to folder selection mode only\n",
    "chooser.title = 'Select a folder containing your dataset'\n",
    "chooser.show_only_dirs = True\n",
    "\n",
    "# Display the widget\n",
    "display(chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f14c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\frames_output\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = chooser.selected_path\n",
    "print(\"You selected:\", path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d2bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset type: video\n"
     ]
    }
   ],
   "source": [
    "my_dataset_type = input(\"Enter your dataset type (video or image): \").lower()\n",
    "print(\"Selected dataset type:\", my_dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset_from_local(\n",
    "    client=client,\n",
    "    dataset_config=DatasetConfig(dataset_name=\"My Dataset\", data_type=\"image\"),\n",
    "    folder_to_upload=path_to_dataset\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with ID: {dataset.dataset_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706926d0",
   "metadata": {},
   "source": [
    "### ***STEP-2: Create annotation project on labellerr of your created dataset***\n",
    "\n",
    "Create a annotation project of your uploaded dataset to start performing annotation on labellerr UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dda2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation guideline template for video annotation project (like classes to be annotated)\n",
    "\n",
    "template = create_template(\n",
    "    client=client,\n",
    "    params=CreateTemplateParams(\n",
    "        template_name=\"My Template\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        questions=[\n",
    "            AnnotationQuestion(\n",
    "                question_number=1,\n",
    "                question=\"Object\",\n",
    "                question_id=str(uuid.uuid4()),\n",
    "                question_type=QuestionType.polygon,\n",
    "                required=True,\n",
    "                color=\"#FF0000\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(f\"Annotation template created with ID: {template.annotation_template_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.status()        # wait until dataset is processed before creating project\n",
    "\n",
    "project = create_project(\n",
    "    client=client,\n",
    "    params=CreateProjectParams(\n",
    "        project_name=\"My Project\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        rotations=RotationConfig(\n",
    "            annotation_rotation_count=1,\n",
    "            review_rotation_count=1,\n",
    "            client_review_rotation_count=1\n",
    "        )\n",
    "    ),\n",
    "    datasets=[dataset],\n",
    "    annotation_template=template\n",
    ")\n",
    "\n",
    "print(f\"‚úì Project created: {project.project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3963b2",
   "metadata": {},
   "source": [
    "Your project has been created now go to labellerr platform to perform annotation \n",
    "\n",
    "***click to go to labellerr.com***\n",
    "\n",
    "[![Labellerr](https://cdn.labellerr.com/1%20%20Documentation/1c9dc7ce-9a54-4111-8fd5-0363ba3e00e1.webp)](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks)\n",
    "Open the project you created (Projects ‚Üí select your project).\n",
    "\n",
    "Click Start Labeling to open the annotation interface. Use the configured labeling tools (bounding boxes, polygon, dot, classification, etc.) to annotate files.\n",
    "### ***STEP-3: Export your annotation in required format***\n",
    "\n",
    "Generate a temporary download URL to retrieve your exported JSON file:\n",
    "\n",
    "### Export Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `export_name` | string | Display name for the export |\n",
    "| `export_description` | string | Description of what this export contains |\n",
    "| `export_format` | string | Output format (e.g., `json`, `xml`, `coco`) |\n",
    "| `statuses` | list | Annotation statuses to include in export |\n",
    "\n",
    "### Common Annotation Statuses\n",
    "\n",
    "- **`review`**: Annotations pending review\n",
    "- **`r_assigned`**: Review assigned to a reviewer\n",
    "- **`client_review`**: Under client review\n",
    "- **`cr_assigned`**: Client review assigned\n",
    "- **`accepted`**: Annotations accepted and finalized\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_config = {\n",
    "    \"export_name\": \"Weekly Export\",\n",
    "    \"export_description\": \"Export of all accepted annotations\",\n",
    "    \"export_format\": \"coco_json\",\n",
    "    \"statuses\": ['review', 'r_assigned','client_review', 'cr_assigned','accepted']\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Get project instance\n",
    "    project = LabellerrProject(client=client, project_id=project.project_id)\n",
    "    \n",
    "    # Create export\n",
    "    result = project.create_local_export(export_config)\n",
    "    export_id = result[\"response\"]['report_id']\n",
    "    print(f\"Local export created successfully. Export ID: {export_id}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Local export creation failed: {str(e)}\")\n",
    "    \n",
    "    \n",
    "try:\n",
    "    download_url = client.fetch_download_url(\n",
    "        project_id=project.project_id,\n",
    "        uuid=str(uuid.uuid4()),\n",
    "        export_id=export_id\n",
    "    )\n",
    "    print(f\"Download URL: {download_url}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Failed to fetch download URL: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3e419",
   "metadata": {},
   "source": [
    "Now you can download your annotations locally using given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce830a66",
   "metadata": {},
   "source": [
    "# COCO to YOLO Format Conversion\n",
    "\n",
    "Converts COCO-style segmentation annotations to YOLO segmentation dataset format.  \n",
    "- Requires: `annotation.json` and images in `frames_output` directory.\n",
    "- Output: Generated YOLO dataset folder.\n",
    "- Parameters: allows train/val split, shuffling, and verbose mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91813c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.seg_converter import coco_to_yolo_converter\n",
    "\n",
    "ANNOTATION_JSON = r\"annotation.json\"\n",
    "IMAGE_DIR = r\"image_dataser_dir\"\n",
    "\n",
    "\n",
    "coco_to_yolo_converter(\n",
    "        json_path=ANNOTATION_JSON,\n",
    "        images_dir=IMAGE_DIR,\n",
    "        output_dir=\"yolo_dataset\",\n",
    "        use_split=False,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.0,\n",
    "        shuffle=True,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcf71f",
   "metadata": {},
   "source": [
    "# Clear CUDA Cache\n",
    "\n",
    "Frees up GPU memory by clearing unused cached memory before starting new tasks or model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93d36a",
   "metadata": {},
   "source": [
    "# Check GPU Memory Status\n",
    "\n",
    "Prints the GPU memory currently allocated, cached, and free.  \n",
    "Useful for diagnosing memory usage before training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabc90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory status\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
    "print(f\"Free: {torch.cuda.mem_get_info(0)[0]/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23584c04",
   "metadata": {},
   "source": [
    "# Load and Train YOLO Segmentation Model\n",
    "\n",
    "Loads the YOLO segmentation model and trains it using the converted YOLO dataset.\n",
    "- Data: Path to YOLO-style `data.yaml`\n",
    "- Parameters: epochs, image size, batch size, device, dataloader workers, experiment name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ad3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11m-seg.pt\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=r\"yolo_dataset\\data.yaml\",    # Path to your dataset YAML file\n",
    "    epochs=500,                        # Number of training epochs\n",
    "    imgsz=640,                         # Image size\n",
    "    batch=-1,                          # Batch size\n",
    "    device=0,                          # GPU device (0 for first GPU, 'cpu' for CPU)\n",
    "    workers=4,                         # Number of dataloader workers\n",
    "    name=\"Model m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a4cce",
   "metadata": {},
   "source": [
    "# CellSegmentationVideo Class Definition\n",
    "\n",
    "Defines the `CellSegmentationVideo` class for performing video cell segmentation using YOLO.\n",
    "- Initialization requires YOLO model path, confidence threshold, IOU threshold.\n",
    "- Contains methods for polygon area calculation, mask center localization, frame processing, and video processing.\n",
    "- Tracks cells, draws masks and borders, labels areas and track IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae749433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class CellSegmentationVideo:\n",
    "    def __init__(self, model_path, conf_threshold=0.25, iou_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the cell segmentation video inference class.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to trained YOLO segmentation model\n",
    "            conf_threshold: Confidence threshold for detections\n",
    "            iou_threshold: IOU threshold for NMS\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        \n",
    "        # Generate distinct colors for different tracked objects\n",
    "        np.random.seed(42)\n",
    "        self.colors = [tuple(map(int, np.random.randint(0, 255, 3))) \n",
    "                       for _ in range(100)]\n",
    "        \n",
    "        # Track history for smoothing\n",
    "        self.track_history = defaultdict(lambda: [])\n",
    "        \n",
    "    def calculate_polygon_area(self, points):\n",
    "        \"\"\"Calculate area of polygon using Shoelace formula.\"\"\"\n",
    "        x = points[:, 0]\n",
    "        y = points[:, 1]\n",
    "        return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n",
    "    \n",
    "    def get_polygon_center(self, points):\n",
    "        \"\"\"Calculate center of polygon.\"\"\"\n",
    "        moments = cv2.moments(points)\n",
    "        if moments['m00'] != 0:\n",
    "            cx = int(moments['m10'] / moments['m00'])\n",
    "            cy = int(moments['m01'] / moments['m00'])\n",
    "            return (cx, cy)\n",
    "        return (int(np.mean(points[:, 0])), int(np.mean(points[:, 1])))\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame with cell segmentation.\n",
    "        \n",
    "        Args:\n",
    "            frame: Input frame (BGR format)\n",
    "            \n",
    "        Returns:\n",
    "            Annotated frame with colored segmentation masks and areas\n",
    "        \"\"\"\n",
    "        # Run tracking (built-in YOLO tracker)\n",
    "        results = self.model.track(\n",
    "            frame, \n",
    "            persist=True,\n",
    "            conf=self.conf_threshold,\n",
    "            iou=self.iou_threshold,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        if results[0].masks is not None:\n",
    "            masks = results[0].masks.xy\n",
    "            boxes = results[0].boxes\n",
    "            \n",
    "            # Get track IDs if available\n",
    "            if boxes.id is not None:\n",
    "                track_ids = boxes.id.int().cpu().tolist()\n",
    "            else:\n",
    "                track_ids = list(range(len(masks)))\n",
    "            \n",
    "            for mask, track_id in zip(masks, track_ids):\n",
    "                if len(mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get color for this track\n",
    "                color = self.colors[track_id % len(self.colors)]\n",
    "                \n",
    "                # Convert mask to integer coordinates\n",
    "                mask_points = mask.astype(np.int32)\n",
    "                \n",
    "                # Draw filled polygon with transparency\n",
    "                overlay = annotated_frame.copy()\n",
    "                cv2.fillPoly(overlay, [mask_points], color)\n",
    "                cv2.addWeighted(overlay, 0.4, annotated_frame, 0.6, 0, annotated_frame)\n",
    "                \n",
    "                # Draw solid border\n",
    "                cv2.polylines(annotated_frame, [mask_points], True, color, 2)\n",
    "                \n",
    "                # Calculate area (in pixels squared)\n",
    "                area = self.calculate_polygon_area(mask_points)\n",
    "                \n",
    "                # Get center point\n",
    "                center = self.get_polygon_center(mask_points)\n",
    "                \n",
    "                # Display area at center\n",
    "                area_text = f\"{area:.0f} px sq\"\n",
    "                \n",
    "                # Get text size for background\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                font_scale = 0.5\n",
    "                thickness = 1\n",
    "                (text_w, text_h), baseline = cv2.getTextSize(\n",
    "                    area_text, font, font_scale, thickness\n",
    "                )\n",
    "                \n",
    "                # Draw background rectangle for text\n",
    "                cv2.rectangle(\n",
    "                    annotated_frame,\n",
    "                    (center[0] - text_w//2 - 5, center[1] - text_h//2 - 5),\n",
    "                    (center[0] + text_w//2 + 5, center[1] + text_h//2 + 5),\n",
    "                    (255, 255, 255),\n",
    "                    -1\n",
    "                )\n",
    "                \n",
    "                # Draw area text\n",
    "                cv2.putText(\n",
    "                    annotated_frame,\n",
    "                    area_text,\n",
    "                    (center[0] - text_w//2, center[1] + text_h//2),\n",
    "                    font,\n",
    "                    font_scale,\n",
    "                    (0, 0, 0),\n",
    "                    thickness\n",
    "                )\n",
    "                \n",
    "                # Optional: Draw track ID\n",
    "                cv2.putText(\n",
    "                    annotated_frame,\n",
    "                    f\"ID:{track_id}\",\n",
    "                    (center[0] - text_w//2, center[1] - text_h//2 - 10),\n",
    "                    font,\n",
    "                    0.4,\n",
    "                    color,\n",
    "                    1\n",
    "                )\n",
    "        \n",
    "        return annotated_frame\n",
    "    \n",
    "    def process_video(self, input_path, output_path, display=True):\n",
    "        \"\"\"\n",
    "        Process entire video file.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input video\n",
    "            output_path: Path to save output video\n",
    "            display: Whether to display video while processing\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Setup video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        print(f\"Processing video: {total_frames} frames at {fps} FPS\")\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Process frame\n",
    "            annotated_frame = self.process_frame(frame)\n",
    "            \n",
    "            # Write to output\n",
    "            out.write(annotated_frame)\n",
    "            \n",
    "            # Display if requested\n",
    "            if display:\n",
    "                cv2.imshow('Cell Segmentation', annotated_frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "        \n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"Video saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f4bb3",
   "metadata": {},
   "source": [
    "# Specify Model Path\n",
    "\n",
    "Specifies the file path to the trained YOLO segmentation model weights.  \n",
    "Update this path as needed to match your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712bb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\runs\\segment\\Model m\\weights\\last.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712cab0e",
   "metadata": {},
   "source": [
    "# Example Usage‚ÄîRun Segmentation on a Video\n",
    "\n",
    "Demonstrates how to use the `CellSegmentationVideo` class:\n",
    "- Initializes the processor.\n",
    "- Runs segmentation inference on a sample video.\n",
    "- Saves the output and optionally displays annotated frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03602aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the segmentation processor\n",
    "    segmentor = CellSegmentationVideo(\n",
    "        model_path=model_path,\n",
    "        conf_threshold=0.5,\n",
    "        iou_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    # Process a video file\n",
    "    segmentor.process_video(\n",
    "        input_path=r\"D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\video\\cell segmentation_bw.mp4\",\n",
    "        output_path=\"output_segmented_2.mp4\",\n",
    "        display=False\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f7b88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
