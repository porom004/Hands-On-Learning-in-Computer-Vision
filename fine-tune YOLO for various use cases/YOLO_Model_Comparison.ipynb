{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07759d0c",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **YOLO Model Comparison**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c8659",
   "metadata": {},
   "source": [
    "This notebook provides a tool to compare the performance of different YOLO models on the same video. It generates a side-by-side comparison video, performance statistics, and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3890f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb368d9",
   "metadata": {},
   "source": [
    "## `YOLOComparator` Class\n",
    "\n",
    "This cell defines the main class for the comparison tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOComparator:\n",
    "    \"\"\"Master class for comparing multiple YOLO models on same task\"\"\"\n",
    "    \n",
    "    SUPPORTED_TASKS = ['detect', 'segment', 'pose', 'track', 'obb', 'claQssify']\n",
    "    \n",
    "    # Predefined model configurations\n",
    "    PRESET_MODELS = {\n",
    "        # Detection models\n",
    "        'yolov8n': 'yolov8n.pt',\n",
    "        'yolov8s': 'yolov8s.pt',\n",
    "        'yolov8m': 'yolov8m.pt',\n",
    "        'yolov8l': 'yolov8l.pt',\n",
    "        'yolov8x': 'yolov8x.pt',\n",
    "        'yolo11n': 'yolo11n.pt',\n",
    "        'yolo11s': 'yolo11s.pt',\n",
    "        'yolo11m': 'yolo11m.pt',\n",
    "        'yolo11l': 'yolo11l.pt',\n",
    "        'yolo11x': 'yolo11x.pt',\n",
    "        \n",
    "        # Segmentation models\n",
    "        'yolov8n-seg': 'yolov8n-seg.pt',\n",
    "        'yolov8s-seg': 'yolov8s-seg.pt',\n",
    "        'yolov8m-seg': 'yolov8m-seg.pt',\n",
    "        'yolov8l-seg': 'yolov8l-seg.pt',\n",
    "        'yolov8x-seg': 'yolov8x-seg.pt',\n",
    "        'yolo11n-seg': 'yolo11n-seg.pt',\n",
    "        'yolo11s-seg': 'yolo11s-seg.pt',\n",
    "        'yolo11m-seg': 'yolo11m-seg.pt',\n",
    "        \n",
    "        # Pose models\n",
    "        'yolov8n-pose': 'yolov8n-pose.pt',\n",
    "        'yolov8s-pose': 'yolov8s-pose.pt',\n",
    "        'yolov8m-pose': 'yolov8m-pose.pt',\n",
    "        'yolov8l-pose': 'yolov8l-pose.pt',\n",
    "        'yolo11n-pose': 'yolo11n-pose.pt',\n",
    "        'yolo11s-pose': 'yolo11s-pose.pt',\n",
    "        'yolo11m-pose': 'yolo11m-pose.pt',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"yolo_comparison_results\"):\n",
    "        \"\"\"Initialize the comparator\"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.loaded_models = {}\n",
    "        \n",
    "    def _load_model(self, model_path: str, model_name: str) -> YOLO:\n",
    "        \"\"\"Load a YOLO model with caching\"\"\"\n",
    "        if model_name not in self.loaded_models:\n",
    "            print(f\"  Loading {model_name}...\")\n",
    "            # Check if it's a preset model\n",
    "            if model_path in self.PRESET_MODELS:\n",
    "                model_path = self.PRESET_MODELS[model_path]\n",
    "            self.loaded_models[model_name] = YOLO(model_path)\n",
    "        return self.loaded_models[model_name]\n",
    "    \n",
    "    def _get_video_info(self, video_path: str) -> Dict:\n",
    "        \"\"\"Extract video metadata\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        info = {\n",
    "            'fps': int(cap.get(cv2.CAP_PROP_FPS)),\n",
    "            'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "            'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "            'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        }\n",
    "        cap.release()\n",
    "        info['duration'] = info['total_frames'] / info['fps'] if info['fps'] > 0 else 0\n",
    "        return info\n",
    "    \n",
    "    def _create_grid_layout(self, frames: List[np.ndarray], titles: List[str], \n",
    "                           times: List[float], counts: List[int]) -> np.ndarray:\n",
    "        \"\"\"Create a grid layout for multiple frames\"\"\"\n",
    "        n_models = len(frames)\n",
    "        \n",
    "        if n_models == 1:\n",
    "            rows, cols = 1, 1\n",
    "        elif n_models == 2:\n",
    "            rows, cols = 1, 2\n",
    "        elif n_models <= 4:\n",
    "            rows, cols = 2, 2\n",
    "        elif n_models <= 6:\n",
    "            rows, cols = 2, 3\n",
    "        elif n_models <= 9:\n",
    "            rows, cols = 3, 3\n",
    "        else:\n",
    "            rows = int(np.ceil(np.sqrt(n_models)))\n",
    "            cols = int(np.ceil(n_models / rows))\n",
    "        \n",
    "        h, w = frames[0].shape[:2]\n",
    "        \n",
    "        # Create empty grid\n",
    "        grid = np.zeros((h * rows, w * cols, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Place frames in grid\n",
    "        for idx, (frame, title, inf_time, count) in enumerate(zip(frames, titles, times, counts)):\n",
    "            row = idx // cols\n",
    "            col = idx % cols\n",
    "            \n",
    "            y_start = row * h\n",
    "            y_end = (row + 1) * h\n",
    "            x_start = col * w\n",
    "            x_end = (col + 1) * w\n",
    "            \n",
    "            grid[y_start:y_end, x_start:x_end] = frame\n",
    "            \n",
    "            # Add info overlay\n",
    "            info_text = f\"{title} | {inf_time*1000:.1f}ms | {count} obj\"\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.6\n",
    "            thickness = 2\n",
    "            \n",
    "            (text_w, text_h), _ = cv2.getTextSize(info_text, font, font_scale, thickness)\n",
    "            cv2.rectangle(grid, (x_start + 5, y_start + 5), \n",
    "                         (x_start + text_w + 15, y_start + text_h + 15), \n",
    "                         (0, 0, 0), -1)\n",
    "            cv2.putText(grid, info_text, (x_start + 10, y_start + text_h + 10),\n",
    "                       font, font_scale, (0, 255, 0), thickness)\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def _process_frame(self, frame: np.ndarray, model: YOLO, task: str, \n",
    "                      conf_threshold: float) -> Tuple[np.ndarray, float, int]:\n",
    "        \"\"\"Process a single frame with a model\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if task == 'track':\n",
    "            results = model.track(frame, conf=conf_threshold, persist=True, verbose=False)[0]\n",
    "        else:\n",
    "            results = model(frame, conf=conf_threshold, verbose=False)[0]\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        processed_frame = results.plot()\n",
    "        \n",
    "        # Count detections\n",
    "        if task == 'track':\n",
    "            count = len(results.boxes.id) if results.boxes.id is not None else 0\n",
    "        else:\n",
    "            count = len(results.boxes) if results.boxes else 0\n",
    "        \n",
    "        return processed_frame, inference_time, count\n",
    "    \n",
    "    def compare_models(self,\n",
    "                      video_path: str,\n",
    "                      models: List[str],\n",
    "                      task: str = 'detect',\n",
    "                      conf_threshold: float = 0.25,\n",
    "                      max_frames: Optional[int] = None,\n",
    "                      model_names: Optional[List[str]] = None,\n",
    "                      generate_plots: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare multiple YOLO models on the same task\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video file\n",
    "            models: List of model paths or preset names (e.g., ['yolov8n', 'yolov8s', 'yolo11n'])\n",
    "            task: Task to perform ('detect', 'segment', 'pose', 'track')\n",
    "            conf_threshold: Confidence threshold for detections (default: 0.25)\n",
    "            max_frames: Maximum frames to process (None for entire video)\n",
    "            model_names: Optional custom names for models (defaults to model paths)\n",
    "            generate_plots: Whether to generate visualization plots\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing results and statistics\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate inputs\n",
    "        if not Path(video_path).exists():\n",
    "            raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "        \n",
    "        if task not in self.SUPPORTED_TASKS:\n",
    "            raise ValueError(f\"Invalid task: {task}. Supported: {self.SUPPORTED_TASKS}\")\n",
    "        \n",
    "        if len(models) < 1:\n",
    "            raise ValueError(\"At least one model must be provided\")\n",
    "        \n",
    "        # Set default model names\n",
    "        if model_names is None:\n",
    "            model_names = [Path(m).stem if m not in self.PRESET_MODELS else m \n",
    "                          for m in models]\n",
    "        \n",
    "        if len(model_names) != len(models):\n",
    "            raise ValueError(\"Number of model names must match number of models\")\n",
    "        \n",
    "        # Get video info\n",
    "        video_info = self._get_video_info(video_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"YOLO MODEL COMPARISON - SAME TASK\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Video: {video_path}\")\n",
    "        print(f\"Resolution: {video_info['width']}x{video_info['height']}\")\n",
    "        print(f\"FPS: {video_info['fps']}\")\n",
    "        print(f\"Total Frames: {video_info['total_frames']}\")\n",
    "        print(f\"Duration: {video_info['duration']:.2f}s\")\n",
    "        print(f\"Task: {task.upper()}\")\n",
    "        print(f\"Models: {', '.join(model_names)}\")\n",
    "        print(f\"Max Frames: {max_frames if max_frames else 'All'}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load all models\n",
    "        print(\"\\nLoading models...\")\n",
    "        loaded_models = []\n",
    "        for model_path, name in zip(models, model_names):\n",
    "            model = self._load_model(model_path, name)\n",
    "            loaded_models.append(model)\n",
    "        print(\"✓ All models loaded\\n\")\n",
    "        \n",
    "        # Setup video processing\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        output_filename = f'{task}_comparison_{\"_\".join(model_names)}.mp4'\n",
    "        out_path = self.output_dir / output_filename\n",
    "        writer = None\n",
    "        \n",
    "        # Metrics storage\n",
    "        metrics = {name: {'times': [], 'counts': []} for name in model_names}\n",
    "        \n",
    "        frame_count = 0\n",
    "        print(f\"Processing video...\")\n",
    "        \n",
    "        while cap.isOpened() and (max_frames is None or frame_count < max_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Process frame with all models\n",
    "            processed_frames = []\n",
    "            times = []\n",
    "            counts = []\n",
    "            \n",
    "            for model, name in zip(loaded_models, model_names):\n",
    "                proc_frame, inf_time, count = self._process_frame(\n",
    "                    frame, model, task, conf_threshold\n",
    "                )\n",
    "                processed_frames.append(proc_frame)\n",
    "                times.append(inf_time)\n",
    "                counts.append(count)\n",
    "                \n",
    "                # Store metrics\n",
    "                metrics[name]['times'].append(inf_time)\n",
    "                metrics[name]['counts'].append(count)\n",
    "            \n",
    "            # Create grid comparison\n",
    "            comparison_grid = self._create_grid_layout(\n",
    "                processed_frames, model_names, times, counts\n",
    "            )\n",
    "            \n",
    "            # Initialize writer\n",
    "            if writer is None:\n",
    "                h, w = comparison_grid.shape[:2]\n",
    "                writer = cv2.VideoWriter(str(out_path),\n",
    "                                        cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                                        video_info['fps'], (w, h))\n",
    "            \n",
    "            writer.write(comparison_grid)\n",
    "            frame_count += 1\n",
    "            \n",
    "            if frame_count % 30 == 0:\n",
    "                print(f\"  Processed {frame_count} frames...\")\n",
    "        \n",
    "        cap.release()\n",
    "        if writer:\n",
    "            writer.release()\n",
    "        \n",
    "        print(f\"\\n✓ Comparison video saved: {out_path}\")\n",
    "        \n",
    "        # Generate statistics\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PERFORMANCE STATISTICS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        summary_data = []\n",
    "        for name in model_names:\n",
    "            times = np.array(metrics[name]['times']) * 1000\n",
    "            counts = metrics[name]['counts']\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Model': name,\n",
    "                'Task': task.capitalize(),\n",
    "                'Avg Time (ms)': f\"{np.mean(times):.2f}\",\n",
    "                'Std Time (ms)': f\"{np.std(times):.2f}\",\n",
    "                'FPS': f\"{1000/np.mean(times):.2f}\",\n",
    "                'Min Time (ms)': f\"{np.min(times):.2f}\",\n",
    "                'Max Time (ms)': f\"{np.max(times):.2f}\",\n",
    "                'Avg Count': f\"{np.mean(counts):.1f}\"\n",
    "            })\n",
    "        \n",
    "        df_summary = pd.DataFrame(summary_data)\n",
    "        csv_path = self.output_dir / f'{task}_performance_summary.csv'\n",
    "        df_summary.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(\"\\n\" + df_summary.to_string(index=False))\n",
    "        print(f\"\\n✓ Summary saved: {csv_path}\")\n",
    "        \n",
    "        # Generate visualizations\n",
    "        if generate_plots:\n",
    "            self._create_visualizations(metrics, model_names, task)\n",
    "        \n",
    "        # Speedup analysis\n",
    "        self._print_speedup_analysis(metrics, model_names)\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPARISON COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nOutput directory: {self.output_dir}/\")\n",
    "        print(f\"\\nGenerated files:\")\n",
    "        print(f\"  • {output_filename}\")\n",
    "        print(f\"  • {task}_performance_summary.csv\")\n",
    "        if generate_plots:\n",
    "            print(f\"  • {task}_performance_comparison.png\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            'video_info': video_info,\n",
    "            'task': task,\n",
    "            'models': model_names,\n",
    "            'metrics': metrics,\n",
    "            'summary': df_summary,\n",
    "            'output_path': str(out_path),\n",
    "            'output_dir': str(self.output_dir),\n",
    "            'frames_processed': frame_count\n",
    "        }\n",
    "    \n",
    "    def _create_visualizations(self, metrics: Dict, model_names: List[str], task: str):\n",
    "        \"\"\"Create performance visualization plots\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig.suptitle(f'{task.capitalize()} Performance Comparison', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(model_names)))\n",
    "        \n",
    "        # Plot 1: Inference time over frames\n",
    "        for idx, name in enumerate(model_names):\n",
    "            frames = range(len(metrics[name]['times']))\n",
    "            times = np.array(metrics[name]['times']) * 1000\n",
    "            ax1.plot(frames, times, label=name, color=colors[idx], \n",
    "                    alpha=0.7, linewidth=1.5)\n",
    "        \n",
    "        ax1.set_xlabel('Frame Number', fontsize=12)\n",
    "        ax1.set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "        ax1.set_title('Inference Time per Frame', fontsize=14)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Average performance comparison\n",
    "        avg_times = [np.mean(metrics[name]['times']) * 1000 for name in model_names]\n",
    "        avg_fps = [1000 / t for t in avg_times]\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax2_twin = ax2.twinx()\n",
    "        bars1 = ax2.bar(x - width/2, avg_times, width, label='Avg Time (ms)',\n",
    "                       color=colors, alpha=0.7)\n",
    "        bars2 = ax2_twin.bar(x + width/2, avg_fps, width, label='FPS',\n",
    "                            color=colors, alpha=0.4, edgecolor='black', linewidth=2)\n",
    "        \n",
    "        ax2.set_xlabel('Model', fontsize=12)\n",
    "        ax2.set_ylabel('Average Time (ms)', fontsize=12)\n",
    "        ax2_twin.set_ylabel('FPS', fontsize=12)\n",
    "        ax2.set_title('Average Performance', fontsize=14)\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars1, avg_times):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        for bar, val in zip(bars2, avg_fps):\n",
    "            height = bar.get_height()\n",
    "            ax2_twin.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                         f'{val:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = self.output_dir / f'{task}_performance_comparison.png'\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\n✓ Performance plot saved: {plot_path}\")\n",
    "    \n",
    "    def _print_speedup_analysis(self, metrics: Dict, model_names: List[str]):\n",
    "        \"\"\"Print speedup analysis relative to baseline (first model)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SPEEDUP ANALYSIS (relative to baseline)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        baseline_name = model_names[0]\n",
    "        baseline_time = np.mean(metrics[baseline_name]['times'])\n",
    "        \n",
    "        print(f\"Baseline: {baseline_name} ({baseline_time*1000:.2f}ms)\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for name in model_names[1:]:\n",
    "            model_time = np.mean(metrics[name]['times'])\n",
    "            speedup = ((baseline_time - model_time) / baseline_time) * 100\n",
    "            \n",
    "            if speedup > 0:\n",
    "                print(f\"{name}: {speedup:.1f}% faster than baseline\")\n",
    "            else:\n",
    "                print(f\"{name}: {abs(speedup):.1f}% slower than baseline\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ccc6e",
   "metadata": {},
   "source": [
    "##  Example 1 - Compare YOLOv8 Sizes\n",
    "\n",
    "This cell demonstrates how to use the `YOLOComparator` to compare different sizes of the YOLOv8 model (nano, small, medium, and large) on a detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b1ddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare YOLOv8 model sizes on detection\n",
      "\n",
      "\n",
      "======================================================================\n",
      "YOLO MODEL COMPARISON - SAME TASK\n",
      "======================================================================\n",
      "Video: sample_1.mp4\n",
      "Resolution: 1280x720\n",
      "FPS: 29\n",
      "Total Frames: 471\n",
      "Duration: 16.24s\n",
      "Task: DETECT\n",
      "Models: yolov8n, yolov8s, yolov8m, yolov8l\n",
      "Max Frames: 200\n",
      "======================================================================\n",
      "\n",
      "Loading models...\n",
      "  Loading yolov8n...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 1.6MB/s 4.0s 4.0s<0.0s4s0s\n",
      "  Loading yolov8s...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% ━━━━━━━━━━━━ 21.5MB 7.4MB/s 2.9s2.9s<1.0ss1s\n",
      "  Loading yolov8m...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt': 100% ━━━━━━━━━━━━ 49.7MB 11.8MB/s 4.2s4.2s<0.0ss59\n",
      "  Loading yolov8l...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt': 100% ━━━━━━━━━━━━ 83.7MB 13.3MB/s 6.3s6.2s<0.1s7s\n",
      "✓ All models loaded\n",
      "\n",
      "Processing video...\n",
      "  Processed 30 frames...\n",
      "  Processed 60 frames...\n",
      "  Processed 90 frames...\n",
      "  Processed 120 frames...\n",
      "  Processed 150 frames...\n",
      "  Processed 180 frames...\n",
      "\n",
      "✓ Comparison video saved: results_yolov8_sizes\\detect_comparison_yolov8n_yolov8s_yolov8m_yolov8l.mp4\n",
      "\n",
      "======================================================================\n",
      "PERFORMANCE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "  Model   Task Avg Time (ms) Std Time (ms)   FPS Min Time (ms) Max Time (ms) Avg Count\n",
      "yolov8n Detect         23.44         67.26 42.67         12.00        971.31      16.7\n",
      "yolov8s Detect         20.84         10.16 47.99         14.66        159.02      20.6\n",
      "yolov8m Detect         24.99         17.73 40.01         17.82        271.95      20.1\n",
      "yolov8l Detect         31.20         28.52 32.06         24.02        432.35      21.2\n",
      "\n",
      "✓ Summary saved: results_yolov8_sizes\\detect_performance_summary.csv\n",
      "\n",
      "✓ Performance plot saved: results_yolov8_sizes\\detect_performance_comparison.png\n",
      "\n",
      "======================================================================\n",
      "SPEEDUP ANALYSIS (relative to baseline)\n",
      "======================================================================\n",
      "Baseline: yolov8n (23.44ms)\n",
      "----------------------------------------------------------------------\n",
      "yolov8s: 11.1% faster than baseline\n",
      "yolov8m: 6.6% slower than baseline\n",
      "yolov8l: 33.1% slower than baseline\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "COMPARISON COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Output directory: results_yolov8_sizes/\n",
      "\n",
      "Generated files:\n",
      "  • detect_comparison_yolov8n_yolov8s_yolov8m_yolov8l.mp4\n",
      "  • detect_performance_summary.csv\n",
      "  • detect_performance_comparison.png\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example 1: Compare YOLOv8 sizes (n, s, m, l) on detection\n",
    "    print(\"Compare YOLOv8 model sizes on detection\\n\")\n",
    "    comparator = YOLOComparator(output_dir=\"results_yolov8_sizes\")\n",
    "    results = comparator.compare_models(\n",
    "        video_path=\"sample_1.mp4\",\n",
    "        models=['yolov8n', 'yolov8s', 'yolov8m', 'yolov8l'],\n",
    "        task='detect',\n",
    "        conf_threshold=0.25,\n",
    "        max_frames=200\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478834b",
   "metadata": {},
   "source": [
    "## Example 2 - Compare YOLOv11 Sizes\n",
    "\n",
    "This cell shows how to compare different sizes of the YOLOv11 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf96c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare YOLOv8 model sizes on detection\n",
      "\n",
      "\n",
      "======================================================================\n",
      "YOLO MODEL COMPARISON - SAME TASK\n",
      "======================================================================\n",
      "Video: sample_1.mp4\n",
      "Resolution: 1280x720\n",
      "FPS: 29\n",
      "Total Frames: 471\n",
      "Duration: 16.24s\n",
      "Task: DETECT\n",
      "Models: yolo11n, yolo11s, yolo11m, yolo11l\n",
      "Max Frames: 800\n",
      "======================================================================\n",
      "\n",
      "Loading models...\n",
      "  Loading yolo11n...\n",
      "  Loading yolo11s...\n",
      "  Loading yolo11m...\n",
      "  Loading yolo11l...\n",
      "✓ All models loaded\n",
      "\n",
      "Processing video...\n",
      "  Processed 30 frames...\n",
      "  Processed 60 frames...\n",
      "  Processed 90 frames...\n",
      "  Processed 120 frames...\n",
      "  Processed 150 frames...\n",
      "  Processed 180 frames...\n",
      "  Processed 210 frames...\n",
      "  Processed 240 frames...\n",
      "  Processed 270 frames...\n",
      "  Processed 300 frames...\n",
      "  Processed 330 frames...\n",
      "  Processed 360 frames...\n",
      "  Processed 390 frames...\n",
      "  Processed 420 frames...\n",
      "  Processed 450 frames...\n",
      "\n",
      "✓ Comparison video saved: results_yolo11_sizes\\detect_comparison_yolo11n_yolo11s_yolo11m_yolo11l.mp4\n",
      "\n",
      "======================================================================\n",
      "PERFORMANCE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "  Model   Task Avg Time (ms) Std Time (ms)   FPS Min Time (ms) Max Time (ms) Avg Count\n",
      "yolo11n Detect         22.12          7.36 45.20         13.70         89.79      16.5\n",
      "yolo11s Detect         23.77          9.30 42.07         13.87        150.42      20.9\n",
      "yolo11m Detect         27.12         14.53 36.87         15.40        297.13      22.0\n",
      "yolo11l Detect         33.01         16.88 30.29         17.88        333.40      21.7\n",
      "\n",
      "✓ Summary saved: results_yolo11_sizes\\detect_performance_summary.csv\n",
      "\n",
      "✓ Performance plot saved: results_yolo11_sizes\\detect_performance_comparison.png\n",
      "\n",
      "======================================================================\n",
      "SPEEDUP ANALYSIS (relative to baseline)\n",
      "======================================================================\n",
      "Baseline: yolo11n (22.12ms)\n",
      "----------------------------------------------------------------------\n",
      "yolo11s: 7.5% slower than baseline\n",
      "yolo11m: 22.6% slower than baseline\n",
      "yolo11l: 49.2% slower than baseline\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "COMPARISON COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Output directory: results_yolo11_sizes/\n",
      "\n",
      "Generated files:\n",
      "  • detect_comparison_yolo11n_yolo11s_yolo11m_yolo11l.mp4\n",
      "  • detect_performance_summary.csv\n",
      "  • detect_performance_comparison.png\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Compare YOLO11 sizes (n, s, m, l) on detection\n",
    "    print(\"Compare YOLOv8 model sizes on detection\\n\")\n",
    "    comparator = YOLOComparator(output_dir=\"results_yolo11_sizes\")\n",
    "    results = comparator.compare_models(\n",
    "        video_path=\"sample_1.mp4\",\n",
    "        models=['yolo11n', 'yolo11s', 'yolo11m', 'yolo11l'],\n",
    "        task='detect',\n",
    "        conf_threshold=0.25,\n",
    "        max_frames=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ecb18",
   "metadata": {},
   "source": [
    "## Example 3 - Compare YOLOv8n vs. YOLOv11n\n",
    "\n",
    "This cell compares the \"nano\" versions of YOLOv8 and YOLOv11 on a detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7cb4f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare YOLOv8 vs YOLO11 on detection\n",
      "\n",
      "\n",
      "======================================================================\n",
      "YOLO MODEL COMPARISON - SAME TASK\n",
      "======================================================================\n",
      "Video: sample_1.mp4\n",
      "Resolution: 1280x720\n",
      "FPS: 29\n",
      "Total Frames: 471\n",
      "Duration: 16.24s\n",
      "Task: DETECT\n",
      "Models: yolov8n, yolo11n\n",
      "Max Frames: 800\n",
      "======================================================================\n",
      "\n",
      "Loading models...\n",
      "  Loading yolov8n...\n",
      "  Loading yolo11n...\n",
      "✓ All models loaded\n",
      "\n",
      "Processing video...\n",
      "  Processed 30 frames...\n",
      "  Processed 60 frames...\n",
      "  Processed 90 frames...\n",
      "  Processed 120 frames...\n",
      "  Processed 150 frames...\n",
      "  Processed 180 frames...\n",
      "  Processed 210 frames...\n",
      "  Processed 240 frames...\n",
      "  Processed 270 frames...\n",
      "  Processed 300 frames...\n",
      "  Processed 330 frames...\n",
      "  Processed 360 frames...\n",
      "  Processed 390 frames...\n",
      "  Processed 420 frames...\n",
      "  Processed 450 frames...\n",
      "\n",
      "✓ Comparison video saved: results_v8_vs_v11_model-n_detect\\detect_comparison_yolov8n_yolo11n.mp4\n",
      "\n",
      "======================================================================\n",
      "PERFORMANCE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "  Model   Task Avg Time (ms) Std Time (ms)   FPS Min Time (ms) Max Time (ms) Avg Count\n",
      "yolov8n Detect         16.28          5.11 61.44          5.88         88.52      17.4\n",
      "yolo11n Detect         17.32          4.86 57.72          6.72         78.76      16.5\n",
      "\n",
      "✓ Summary saved: results_v8_vs_v11_model-n_detect\\detect_performance_summary.csv\n",
      "\n",
      "✓ Performance plot saved: results_v8_vs_v11_model-n_detect\\detect_performance_comparison.png\n",
      "\n",
      "======================================================================\n",
      "SPEEDUP ANALYSIS (relative to baseline)\n",
      "======================================================================\n",
      "Baseline: yolov8n (16.28ms)\n",
      "----------------------------------------------------------------------\n",
      "yolo11n: 6.4% slower than baseline\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "COMPARISON COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Output directory: results_v8_vs_v11_model-n_detect/\n",
      "\n",
      "Generated files:\n",
      "  • detect_comparison_yolov8n_yolo11n.mp4\n",
      "  • detect_performance_summary.csv\n",
      "  • detect_performance_comparison.png\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example 2: Compare YOLOv8 vs YOLO11 MODEL n on detection\n",
    "    print(\"Compare YOLOv8 vs YOLO11 on detection\\n\")\n",
    "    comparator2 = YOLOComparator(output_dir=\"results_v8_vs_v11_model-n_detect\")\n",
    "    results2 = comparator2.compare_models(\n",
    "        video_path=\"sample_1.mp4\",\n",
    "        models=['yolov8n', 'yolo11n'],\n",
    "        task='detect',\n",
    "        conf_threshold=0.25,\n",
    "        max_frames=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb3637",
   "metadata": {},
   "source": [
    "## Example 4 - Compare YOLOv8l vs. YOLOv11l\n",
    "\n",
    "This cell compares the \"large\" versions of YOLOv8 and YOLOv11 on a detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad61ae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example 2: Compare YOLOv8 vs YOLO11 on segmentation\n",
      "\n",
      "\n",
      "======================================================================\n",
      "YOLO MODEL COMPARISON - SAME TASK\n",
      "======================================================================\n",
      "Video: sample_1.mp4\n",
      "Resolution: 1280x720\n",
      "FPS: 29\n",
      "Total Frames: 471\n",
      "Duration: 16.24s\n",
      "Task: DETECT\n",
      "Models: yolov8l, yolo11l\n",
      "Max Frames: 800\n",
      "======================================================================\n",
      "\n",
      "Loading models...\n",
      "  Loading yolov8l...\n",
      "  Loading yolo11l...\n",
      "✓ All models loaded\n",
      "\n",
      "Processing video...\n",
      "  Processed 30 frames...\n",
      "  Processed 60 frames...\n",
      "  Processed 90 frames...\n",
      "  Processed 120 frames...\n",
      "  Processed 150 frames...\n",
      "  Processed 180 frames...\n",
      "  Processed 210 frames...\n",
      "  Processed 240 frames...\n",
      "  Processed 270 frames...\n",
      "  Processed 300 frames...\n",
      "  Processed 330 frames...\n",
      "  Processed 360 frames...\n",
      "  Processed 390 frames...\n",
      "  Processed 420 frames...\n",
      "  Processed 450 frames...\n",
      "\n",
      "✓ Comparison video saved: results_v8_vs_v11_model-l_detect\\detect_comparison_yolov8l_yolo11l.mp4\n",
      "\n",
      "======================================================================\n",
      "PERFORMANCE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "  Model   Task Avg Time (ms) Std Time (ms)   FPS Min Time (ms) Max Time (ms) Avg Count\n",
      "yolov8l Detect         26.31         25.20 38.01         17.34        568.55      22.6\n",
      "yolo11l Detect         25.99         15.63 38.47         15.55        357.03      21.7\n",
      "\n",
      "✓ Summary saved: results_v8_vs_v11_model-l_detect\\detect_performance_summary.csv\n",
      "\n",
      "✓ Performance plot saved: results_v8_vs_v11_model-l_detect\\detect_performance_comparison.png\n",
      "\n",
      "======================================================================\n",
      "SPEEDUP ANALYSIS (relative to baseline)\n",
      "======================================================================\n",
      "Baseline: yolov8l (26.31ms)\n",
      "----------------------------------------------------------------------\n",
      "yolo11l: 1.2% faster than baseline\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "COMPARISON COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Output directory: results_v8_vs_v11_model-l_detect/\n",
      "\n",
      "Generated files:\n",
      "  • detect_comparison_yolov8l_yolo11l.mp4\n",
      "  • detect_performance_summary.csv\n",
      "  • detect_performance_comparison.png\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example 2: Compare YOLOv8 vs YOLO11 MODEL l on detection\n",
    "    print(\"\\n\\nExample 2: Compare YOLOv8 vs YOLO11 on segmentation\\n\")\n",
    "    comparator2 = YOLOComparator(output_dir=\"results_v8_vs_v11_model-l_detect\")\n",
    "    results2 = comparator2.compare_models(\n",
    "        video_path=\"sample_1.mp4\",\n",
    "        models=['yolov8l', 'yolo11l'],\n",
    "        task='detect',\n",
    "        conf_threshold=0.25,\n",
    "        max_frames=800\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
