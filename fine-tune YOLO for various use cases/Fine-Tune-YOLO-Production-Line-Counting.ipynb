{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1facf9e0",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO for Automated Product Counting**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "[![Scientific Paper](https://img.shields.io/badge/Official-Paper-blue.svg)](<PAPER LINK>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98372293",
   "metadata": {},
   "source": [
    "## Annotate your Custom dataset using Labellerr\n",
    "\n",
    " ***1. Visit the [Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) website and click **‚ÄúSign Up‚Äù**.*** \n",
    "\n",
    " ***2. After signing in, create your workspace by entering a unique name.***\n",
    "\n",
    " ***3. Navigate to your workspace‚Äôs API keys page (e.g., `https://<your-workspace>.labellerr.com/workspace/api-keys`) to generate your **API Key** and **API Secret**.***\n",
    "\n",
    " ***4. Store the credentials securely, and then use them to initialise the SDK or API client with `api_key`, `api_secret`.*** \n",
    "\n",
    "\n",
    "### Use Labellerr SDK for uploading and perform annotation of your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to install required packages in a Jupyter notebook environment\n",
    "\n",
    "# !pip install git+https://github.com/Labellerr/SDKPython.git\n",
    "# !pip install ipyfilechooser\n",
    "# !git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports required for this notebook\n",
    "from labellerr.client import LabellerrClient\n",
    "from labellerr.core.datasets import create_dataset_from_local\n",
    "from labellerr.core.annotation_templates import create_template\n",
    "from labellerr.core.projects import create_project\n",
    "from labellerr.core.schemas import DatasetConfig, AnnotationQuestion, QuestionType, CreateTemplateParams, DatasetDataType, CreateProjectParams, RotationConfig\n",
    "from labellerr.core.projects import LabellerrProject\n",
    "from labellerr.core.exceptions import LabellerrError\n",
    "\n",
    "import uuid\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"YOUR_API_KEY\")        # go to labellerr workspace to get your API key\n",
    "api_secret = input(\"YOUR_API_SECRET\")  # go to labellerr workspace to get your API secret\n",
    "client_id = input(\"YOUR_CLIENT_ID\")   # Contact labellerr support to get your client ID i.e. support@tensormatics.com\n",
    "\n",
    "client = LabellerrClient(api_key, api_secret, client_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fdc4c",
   "metadata": {},
   "source": [
    "### ***STEP-1: Create a dataset on labellerr from your local folder***\n",
    "\n",
    "The SDK supports in creating dataset by uploading local files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bab602150d0462d8a724c7f48ac1984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='D:\\', filename='', title='Select a folder containing your dataset', show_hidden=False, selec‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a folder chooser starting from a directory (for example, your home directory)\n",
    "chooser = FileChooser('/')\n",
    "\n",
    "# Set the chooser to folder selection mode only\n",
    "chooser.title = 'Select a folder containing your dataset'\n",
    "chooser.show_only_dirs = True\n",
    "\n",
    "# Display the widget\n",
    "display(chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b35fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\frames_output\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = chooser.selected_path\n",
    "print(\"You selected:\", path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6189b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset type: video\n"
     ]
    }
   ],
   "source": [
    "my_dataset_type = input(\"Enter your dataset type (video or image): \").lower()\n",
    "print(\"Selected dataset type:\", my_dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset_from_local(\n",
    "    client=client,\n",
    "    dataset_config=DatasetConfig(dataset_name=\"My Dataset\", data_type=\"image\"),\n",
    "    folder_to_upload=path_to_dataset\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with ID: {dataset.dataset_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf930433",
   "metadata": {},
   "source": [
    "### ***STEP-2: Create annotation project on labellerr of your created dataset***\n",
    "\n",
    "Create a annotation project of your uploaded dataset to start performing annotation on labellerr UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation guideline template for video annotation project (like classes to be annotated)\n",
    "\n",
    "template = create_template(\n",
    "    client=client,\n",
    "    params=CreateTemplateParams(\n",
    "        template_name=\"My Template\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        questions=[\n",
    "            AnnotationQuestion(\n",
    "                question_number=1,\n",
    "                question=\"Object\",\n",
    "                question_id=str(uuid.uuid4()),\n",
    "                question_type=QuestionType.polygon,\n",
    "                required=True,\n",
    "                color=\"#FF0000\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(f\"Annotation template created with ID: {template.annotation_template_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f307de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.status()        # wait until dataset is processed before creating project\n",
    "\n",
    "project = create_project(\n",
    "    client=client,\n",
    "    params=CreateProjectParams(\n",
    "        project_name=\"My Project\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        rotations=RotationConfig(\n",
    "            annotation_rotation_count=1,\n",
    "            review_rotation_count=1,\n",
    "            client_review_rotation_count=1\n",
    "        )\n",
    "    ),\n",
    "    datasets=[dataset],\n",
    "    annotation_template=template\n",
    ")\n",
    "\n",
    "print(f\"‚úì Project created: {project.project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e18b8",
   "metadata": {},
   "source": [
    "Your project has been created now go to labellerr platform to perform annotation \n",
    "\n",
    "***click to go to labellerr.com***\n",
    "\n",
    "[![Labellerr](https://cdn.labellerr.com/1%20%20Documentation/1c9dc7ce-9a54-4111-8fd5-0363ba3e00e1.webp)](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks)\n",
    "Open the project you created (Projects ‚Üí select your project).\n",
    "\n",
    "Click Start Labeling to open the annotation interface. Use the configured labeling tools (bounding boxes, polygon, dot, classification, etc.) to annotate files.\n",
    "### ***STEP-3: Export your annotation in required format***\n",
    "\n",
    "Generate a temporary download URL to retrieve your exported JSON file:\n",
    "\n",
    "### Export Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `export_name` | string | Display name for the export |\n",
    "| `export_description` | string | Description of what this export contains |\n",
    "| `export_format` | string | Output format (e.g., `json`, `xml`, `coco`) |\n",
    "| `statuses` | list | Annotation statuses to include in export |\n",
    "\n",
    "### Common Annotation Statuses\n",
    "\n",
    "- **`review`**: Annotations pending review\n",
    "- **`r_assigned`**: Review assigned to a reviewer\n",
    "- **`client_review`**: Under client review\n",
    "- **`cr_assigned`**: Client review assigned\n",
    "- **`accepted`**: Annotations accepted and finalized\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e94012",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_config = {\n",
    "    \"export_name\": \"Weekly Export\",\n",
    "    \"export_description\": \"Export of all accepted annotations\",\n",
    "    \"export_format\": \"coco_json\",\n",
    "    \"statuses\": ['review', 'r_assigned','client_review', 'cr_assigned','accepted']\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Get project instance\n",
    "    project = LabellerrProject(client=client, project_id=project.project_id)\n",
    "    \n",
    "    # Create export\n",
    "    result = project.create_local_export(export_config)\n",
    "    export_id = result[\"response\"]['report_id']\n",
    "    print(f\"Local export created successfully. Export ID: {export_id}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Local export creation failed: {str(e)}\")\n",
    "    \n",
    "    \n",
    "try:\n",
    "    download_url = client.fetch_download_url(\n",
    "        project_id=project.project_id,\n",
    "        uuid=str(uuid.uuid4()),\n",
    "        export_id=export_id\n",
    "    )\n",
    "    print(f\"Download URL: {download_url}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Failed to fetch download URL: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b69d6",
   "metadata": {},
   "source": [
    "Now you can download your annotations locally using given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86ca03",
   "metadata": {},
   "source": [
    "## **Convert COCO Annotations to YOLO Format**\n",
    "Transform COCO JSON annotations to YOLO's required format with normalized bounding box coordinates in separate .txt files. This prepares the dataset for YOLO model fine-tuning on product counting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67657f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.bbox_converter import coco_to_yolo_converter\n",
    "\n",
    "result = coco_to_yolo_converter(\n",
    "            json_path=r'./dataset-2/train/annotations.json',\n",
    "            images_dir=r'./dataset-2/train',\n",
    "            output_dir='yolo_format-3',\n",
    "            use_split=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b486031",
   "metadata": {},
   "source": [
    "### **Setup Environment and Import Libraries**\n",
    "Install and verify Ultralytics YOLO package and import required modules for model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6945c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb78c0",
   "metadata": {},
   "source": [
    "### **Fine-tune YOLO11 Model on Custom Dataset**\n",
    "Train a YOLO11x model on the converted dataset for 400 epochs with batch size 20 and 640x640 image resolution. The model learns to detect and classify products on the production line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c12560",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train data=\"path/to/dataset.yaml\" model=\"yolo11x.pt\" epochs=400 imgsz=640 batch=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af552c",
   "metadata": {},
   "source": [
    "### **Test Fine-tuned Model on Sample Video**\n",
    "Run tracking inference on a test video using the fine-tuned model. Generates predictions with confidence threshold 0.25 and saves annotated output to verify model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d420830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=track model=\"./runs/detect/train/weights/last.pt\" source=\"./video/1.mp4\" conf=0.25 save=True show_labels=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b08994",
   "metadata": {},
   "source": [
    "### **Import Libraries and Initialize Variables**\n",
    "Import necessary packages (OpenCV, NumPy, YOLO, datetime) and set up global variables for product counting and object tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cac40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3863b7",
   "metadata": {},
   "source": [
    "## **Manual line input from user for Production Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfbf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics opencv-python numpy matplotlib ipywidgets\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e82e4",
   "metadata": {},
   "source": [
    "### **Interactive Counting Line Setup**\n",
    "Define functions to interactively draw a counting line on a video frame in fullscreen mode. Users can click and drag to define where products should be counted as they cross the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9cdfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global variables to store line coordinates\n",
    "counting_line = None\n",
    "line_drawn = False\n",
    "drawing = False\n",
    "start_point = None\n",
    "end_point = None\n",
    "sample_frame = None\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    \"\"\"Mouse callback function to draw counting line on sample frame\"\"\"\n",
    "    global counting_line, line_drawn, drawing, start_point, end_point, sample_frame\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        start_point = (x, y)\n",
    "        drawing = True\n",
    "        print(f\"üñ±Ô∏è Line start: {start_point}\")\n",
    "        \n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing and sample_frame is not None:\n",
    "            temp_frame = sample_frame.copy()\n",
    "            cv2.line(temp_frame, start_point, (x, y), (0, 255, 255), 3)\n",
    "            cv2.putText(temp_frame, \"Release mouse to set counting line\", (50, 80), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 3)\n",
    "            cv2.imshow('Draw Counting Line - Video Sample', temp_frame)\n",
    "            \n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        if drawing:\n",
    "            end_point = (x, y)\n",
    "            counting_line = (start_point, end_point)\n",
    "            line_drawn = True\n",
    "            drawing = False\n",
    "            print(f\"‚úÖ Line end: {end_point}\")\n",
    "            print(f\"‚úÖ Counting line coordinates: {counting_line}\")\n",
    "\n",
    "def setup_line_from_video(video_path):\n",
    "    \"\"\"Extract sample frame from video and setup counting line in FULLSCREEN\"\"\"\n",
    "    global sample_frame, counting_line, line_drawn\n",
    "    \n",
    "    # Open video to get sample frame\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ùå Error: Cannot open video file: {video_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Read first frame\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"‚ùå Error: Cannot read frame from video\")\n",
    "        return False\n",
    "    \n",
    "    sample_frame = frame.copy()\n",
    "    print(f\"‚úÖ Sample frame extracted from: {video_path}\")\n",
    "    print(f\"üìê Frame size: {frame.shape[1]}x{frame.shape[0]}\")\n",
    "    \n",
    "    print(\"üìã Instructions:\")\n",
    "    print(\"   1. Click and drag on the frame to draw counting line\")\n",
    "    print(\"   2. Press SPACE to confirm line\")\n",
    "    print(\"   3. Press 'r' to redraw line\")\n",
    "    print(\"   4. Press 'q' to cancel\")\n",
    "    print(\"   5. Press 'f' to toggle fullscreen\")\n",
    "    print(\"   6. Press ESC to exit fullscreen\")\n",
    "    \n",
    "    # Create window with fullscreen capability\n",
    "    cv2.namedWindow('Draw Counting Line - Video Sample', cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    # Set to fullscreen mode\n",
    "    cv2.setWindowProperty('Draw Counting Line - Video Sample', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "    \n",
    "    cv2.setMouseCallback('Draw Counting Line - Video Sample', mouse_callback)\n",
    "    \n",
    "    fullscreen_mode = True\n",
    "    \n",
    "    while True:\n",
    "        display_frame = sample_frame.copy()\n",
    "        \n",
    "        # Draw the counting line if exists\n",
    "        if line_drawn and counting_line:\n",
    "            cv2.line(display_frame, counting_line[0], counting_line[1], (0, 255, 0), 4)\n",
    "            cv2.putText(display_frame, \"COUNTING LINE SET - Press SPACE to confirm\", \n",
    "                       (50, display_frame.shape[0] - 60),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "            \n",
    "            # Show line coordinates (larger text for fullscreen)\n",
    "            cv2.putText(display_frame, f\"Line: {counting_line[0]} to {counting_line[1]}\", \n",
    "                       (50, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(display_frame, \"Click and drag to draw counting line\", \n",
    "                       (50, display_frame.shape[0] - 60),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 0), 3)\n",
    "        \n",
    "        # Add control instructions on screen (larger for fullscreen)\n",
    "        cv2.putText(display_frame, \"Controls: SPACE=Confirm | R=Reset | Q=Cancel | F=Toggle Fullscreen\", \n",
    "                   (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.imshow('Draw Counting Line - Video Sample', display_frame)\n",
    "        \n",
    "        key = cv2.waitKey(30) & 0xFF\n",
    "        if key == ord(' '):  # Space to confirm\n",
    "            if line_drawn:\n",
    "                print(\"‚úÖ Counting line confirmed!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Please draw a line first!\")\n",
    "        elif key == ord('r'):  # Reset line\n",
    "            counting_line = None\n",
    "            line_drawn = False\n",
    "            print(\"üîÑ Line reset - draw again\")\n",
    "        elif key == ord('q'):  # Quit\n",
    "            print(\"‚ùå Setup cancelled\")\n",
    "            cv2.destroyAllWindows()\n",
    "            return False\n",
    "        elif key == ord('f'):  # Toggle fullscreen\n",
    "            if fullscreen_mode:\n",
    "                cv2.setWindowProperty('Draw Counting Line - Video Sample', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_NORMAL)\n",
    "                fullscreen_mode = False\n",
    "                print(\"ü™ü Windowed mode\")\n",
    "            else:\n",
    "                cv2.setWindowProperty('Draw Counting Line - Video Sample', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "                fullscreen_mode = True\n",
    "                print(\"üñ•Ô∏è Fullscreen mode\")\n",
    "        elif key == 27:  # ESC key to exit fullscreen\n",
    "            cv2.setWindowProperty('Draw Counting Line - Video Sample', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_NORMAL)\n",
    "            fullscreen_mode = False\n",
    "            print(\"ü™ü Exited fullscreen mode\")\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfc3ef",
   "metadata": {},
   "source": [
    "### **Extract Sample Frame and Draw Counting Line**\n",
    "Extract the first frame from the video and launch the interactive fullscreen tool to manually define the counting line by clicking and dragging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00958935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_path = r'Manufacturing\\1.mp4'  # ‚Üê VIDEO PATH\n",
    "\n",
    "print(\"üé• Video Sample Frame Setup\")\n",
    "print(f\"üìÅ Video path: {video_path}\")\n",
    "print()\n",
    "print(\"To setup counting line:\")\n",
    "print(f\"setup_line_from_video('{video_path}')\")\n",
    "\n",
    "# Call the function to setup counting line from video\n",
    "setup_line_from_video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef61803",
   "metadata": {},
   "source": [
    "### **Display Counting Line Coordinates**\n",
    "Show the stored coordinates of the counting line that was drawn interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cfc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf41daa",
   "metadata": {},
   "source": [
    "### **Configure Counting Parameters**\n",
    "Set configuration parameters including counting line coordinates, video paths, model path, and tracking settings (max distance and disappearance frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6aadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Set your counting line coordinates (start_point, end_point)\n",
    "counting_line = ((482, 759), (1264, 730))\n",
    "\n",
    "# Set file paths\n",
    "video_path = r'Manufacturing\\1.mp4'  # ‚Üê CHANGE THIS TO YOUR VIDEO PATH\n",
    "model_path = r'runs\\detect\\train\\weights\\last.pt'  # ‚Üê CHANGE THIS TO YOUR YOLO MODEL PATH\n",
    "output_video_path = 'output_counted_video.mp4'  # ‚Üê OUTPUT VIDEO PATH\n",
    "\n",
    "# Tracking settings\n",
    "max_distance = 100    # Max distance to consider same object\n",
    "max_disappeared = 30  # Max frames an object can disappear before removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233272e",
   "metadata": {},
   "source": [
    "### **Define Tracking and Detection Functions**\n",
    "Implement core functions for YOLO model loading, distance calculation, line intersection detection, object tracking, and frame processing with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# GLOBAL VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "product_counter = 0\n",
    "model = None\n",
    "object_tracker = {}  # Store tracked objects: {id: {'centers': [], 'bbox': (), 'class_id': int, 'disappeared': int, 'counted': bool}}\n",
    "next_object_id = 1\n",
    "\n",
    "def load_yolo_model(model_path):\n",
    "    \"\"\"Load YOLO model\"\"\"\n",
    "    global model\n",
    "    try:\n",
    "        print(f\"üì¶ Loading YOLO model: {model_path}\")\n",
    "        model = YOLO(model_path)\n",
    "        print(\"‚úÖ YOLO model loaded successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading YOLO model: {e}\")\n",
    "        return False\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    \"\"\"Calculate distance between two points\"\"\"\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "def line_intersection(p1, p2, p3, p4):\n",
    "    \"\"\"Check if line p1-p2 intersects with line p3-p4\"\"\"\n",
    "    def ccw(A, B, C):\n",
    "        return (C[1] - A[1]) * (B[0] - A[0]) > (B[1] - A[1]) * (C[0] - A[0])\n",
    "    return ccw(p1, p3, p4) != ccw(p2, p3, p4) and ccw(p1, p2, p3) != ccw(p1, p2, p4)\n",
    "\n",
    "def check_line_crossing(obj_id):\n",
    "    \"\"\"Check if object crossed the counting line\"\"\"\n",
    "    global counting_line, product_counter, object_tracker\n",
    "    \n",
    "    obj = object_tracker[obj_id]\n",
    "    if obj['counted'] or len(obj['centers']) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Check if trajectory crosses the counting line\n",
    "    prev_pos = obj['centers'][-2]\n",
    "    curr_pos = obj['centers'][-1]\n",
    "    \n",
    "    if line_intersection(prev_pos, curr_pos, counting_line[0], counting_line[1]):\n",
    "        obj['counted'] = True\n",
    "        product_counter += 1\n",
    "        print(f\"üéØ Object #{product_counter} (ID: {obj_id}) crossed the line!\")\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def update_tracker(detections):\n",
    "    \"\"\"Update object tracker with new detections\"\"\"\n",
    "    global object_tracker, next_object_id, max_distance, max_disappeared\n",
    "    \n",
    "    # Mark all existing objects as potentially disappeared\n",
    "    for obj in object_tracker.values():\n",
    "        obj['disappeared'] += 1\n",
    "    \n",
    "    # Match detections with existing tracked objects\n",
    "    for center, bbox, class_id, confidence in detections:\n",
    "        best_match = None\n",
    "        best_distance = float('inf')\n",
    "        \n",
    "        # Find closest existing object of same class\n",
    "        for obj_id, obj in object_tracker.items():\n",
    "            if obj['class_id'] == class_id:\n",
    "                distance = calculate_distance(center, obj['centers'][-1])\n",
    "                if distance < max_distance and distance < best_distance:\n",
    "                    best_distance = distance\n",
    "                    best_match = obj_id\n",
    "        \n",
    "        if best_match is not None:\n",
    "            # Update existing object\n",
    "            object_tracker[best_match]['centers'].append(center)\n",
    "            object_tracker[best_match]['bbox'] = bbox\n",
    "            object_tracker[best_match]['confidence'] = confidence\n",
    "            object_tracker[best_match]['disappeared'] = 0\n",
    "            \n",
    "            # Keep only last 5 positions\n",
    "            if len(object_tracker[best_match]['centers']) > 5:\n",
    "                object_tracker[best_match]['centers'].pop(0)\n",
    "        else:\n",
    "            # Create new tracked object\n",
    "            object_tracker[next_object_id] = {\n",
    "                'centers': [center],\n",
    "                'bbox': bbox,\n",
    "                'class_id': class_id,\n",
    "                'confidence': confidence,\n",
    "                'disappeared': 0,\n",
    "                'counted': False\n",
    "            }\n",
    "            next_object_id += 1\n",
    "    \n",
    "    # Remove objects that disappeared for too long\n",
    "    to_remove = [obj_id for obj_id, obj in object_tracker.items() if obj['disappeared'] > max_disappeared]\n",
    "    for obj_id in to_remove:\n",
    "        del object_tracker[obj_id]\n",
    "\n",
    "def process_frame(frame):\n",
    "    \"\"\"Process single frame for detection and counting\"\"\"\n",
    "    global model, counting_line, object_tracker\n",
    "    \n",
    "    frame_copy = frame.copy()\n",
    "    \n",
    "    # Draw counting line\n",
    "    cv2.line(frame_copy, counting_line[0], counting_line[1], (0, 255, 0), 4)\n",
    "    mid_x = (counting_line[0][0] + counting_line[1][0]) // 2\n",
    "    mid_y = (counting_line[0][1] + counting_line[1][1]) // 2\n",
    "    cv2.putText(frame_copy, \"COUNTING LINE\", (mid_x - 80, mid_y - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    # Run YOLO detection\n",
    "    detections = []\n",
    "    try:\n",
    "        results = model(frame, conf=0.5, verbose=False)\n",
    "        for result in results:\n",
    "            if result.boxes is not None:\n",
    "                for box in result.boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                    confidence = box.conf[0].cpu().numpy()\n",
    "                    class_id = int(box.cls[0].cpu().numpy())\n",
    "                    \n",
    "                    center_x = int((x1 + x2) / 2)\n",
    "                    center_y = int((y1 + y2) / 2)\n",
    "                    center_point = (center_x, center_y)\n",
    "                    bbox = (int(x1), int(y1), int(x2), int(y2))\n",
    "                    \n",
    "                    detections.append((center_point, bbox, class_id, confidence))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Detection error: {e}\")\n",
    "    \n",
    "    # Update tracker and check crossings\n",
    "    update_tracker(detections)\n",
    "    \n",
    "    for obj_id, obj in object_tracker.items():\n",
    "        check_line_crossing(obj_id)\n",
    "        \n",
    "        # Draw bounding box with color based on status\n",
    "        x1, y1, x2, y2 = obj['bbox']\n",
    "        if obj['counted']:\n",
    "            color = (0, 255, 0)  # Green: counted\n",
    "            thickness = 3\n",
    "        else:\n",
    "            color = (255, 0, 0)  # Blue: not counted\n",
    "            thickness = 2\n",
    "        \n",
    "        cv2.rectangle(frame_copy, (x1, y1), (x2, y2), color, thickness)\n",
    "        \n",
    "        # Draw center and ID\n",
    "        center = obj['centers'][-1]\n",
    "        cv2.circle(frame_copy, center, 5, (0, 0, 255), -1)\n",
    "        cv2.putText(frame_copy, f\"ID:{obj_id}\", (x1, y1 - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        \n",
    "        # Draw trajectory\n",
    "        if len(obj['centers']) > 1:\n",
    "            for i in range(1, len(obj['centers'])):\n",
    "                cv2.line(frame_copy, obj['centers'][i-1], obj['centers'][i], (255, 255, 0), 2)\n",
    "    \n",
    "    # Draw counter\n",
    "    cv2.rectangle(frame_copy, (10, 10), (200, 60), (0, 0, 0), -1)\n",
    "    cv2.putText(frame_copy, f\"COUNT: {product_counter}\", (20, 40),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame_copy\n",
    "\n",
    "def process_video():\n",
    "    \"\"\"Main function to process video\"\"\"\n",
    "    global product_counter, object_tracker, next_object_id\n",
    "    \n",
    "    print(f\"üé• Starting Product Counter\")\n",
    "    print(f\"üìÅ Input: {video_path}\")\n",
    "    print(f\"üìÅ Output: {output_video_path}\")\n",
    "    print(f\"ü§ñ Model: {model_path}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load model\n",
    "    if not load_yolo_model(model_path):\n",
    "        return False\n",
    "    \n",
    "    # Open input video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ùå Cannot open video: {video_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"üìä Video: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
    "    \n",
    "    # Create output video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Reset counters\n",
    "    product_counter = 0\n",
    "    object_tracker = {}\n",
    "    next_object_id = 1\n",
    "    \n",
    "    frame_count = 0\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(\"üöÄ Processing...\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Process frame\n",
    "        processed_frame = process_frame(frame)\n",
    "        out.write(processed_frame)\n",
    "        \n",
    "        # Show progress every 10%\n",
    "        if frame_count % (total_frames // 10) == 0:\n",
    "            progress = (frame_count / total_frames) * 100\n",
    "            print(f\"üìà {progress:.0f}% - Frame {frame_count}/{total_frames} - Count: {product_counter}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Results\n",
    "    print(\"=\"*50)\n",
    "    print(\"üèÅ Processing completed!\")\n",
    "    print(f\"üìä Total count: {product_counter}\")\n",
    "    print(f\"üìä Processing time: {processing_time}\")\n",
    "    print(f\"üìÅ Output saved: {output_video_path}\")\n",
    "    \n",
    "    # Save report\n",
    "    report_path = output_video_path.replace('.mp4', '_report.txt')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(f\"Product Counting Report\\n\")\n",
    "        f.write(f\"======================\\n\")\n",
    "        f.write(f\"Date: {datetime.now()}\\n\")\n",
    "        f.write(f\"Input: {video_path}\\n\")\n",
    "        f.write(f\"Output: {output_video_path}\\n\")\n",
    "        f.write(f\"Total Count: {product_counter}\\n\")\n",
    "        f.write(f\"Processing Time: {processing_time}\\n\")\n",
    "        f.write(f\"Counting Line: {counting_line}\\n\")\n",
    "    \n",
    "    print(f\"üìÑ Report saved: {report_path}\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d55336",
   "metadata": {},
   "source": [
    "### **Execute Product Counting on Video**\n",
    "Run the complete product counting pipeline on the input video. Processes each frame with YOLO detection, tracks objects, counts crossings, and saves annotated output video with count report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ad524",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc400d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
