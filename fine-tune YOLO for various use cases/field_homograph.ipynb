{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0773afe9",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **SportMap AI: Real-Time Homographic Projection for Tactical Field Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182c8aa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**HoloField** is an advanced computer vision pipeline designed to transform standard sports broadcast footage into a real-time, 2D tactical miniature. By combining **YOLO11 instance segmentation** with **Planar Homography**, the system projects dynamic entities from a distorted camera perspective onto a metric-accurate \"digital twin\" of the court.\n",
    "\n",
    "\n",
    "\n",
    "The workflow covers a specialized end-to-end pipeline: from high-resolution retina mask extraction and perspective correction to real-time kinematic calculations. The result is a smooth, jitter-free \"God-view\" that tracks player positioning and velocity without the need for multiple camera angles or expensive wearable sensors.\n",
    "\n",
    "#### Key Technical Features:\n",
    "* **Deep Learning Inference:** Utilizing YOLO11x-Seg for pixel-perfect player and ball detection.\n",
    "* **Perspective Transformation:** RANSAC-based Homography to map pixel coordinates to real-world meters.\n",
    "* **Tactical Visualization:** Team-based color coding (Team A vs. Team B) and trajectory tail mapping.\n",
    "\n",
    "\n",
    "\n",
    "#### Real-World Applications:\n",
    "* **Professional Sports Analytics:** Automated tactical \"God-view\" generation for coaches and performance scouts.\n",
    "* **Broadcasting & Fan Engagement:** Real-time AR overlays showing player speed and court coverage heatmaps.\n",
    "* **Performance Monitoring:** Metric-accurate distance and velocity tracking for athletic training.\n",
    "* **Digital Twin Synthesis:** Creating virtual environments for game replay and strategic simulation.\n",
    "* **Automated Officiating:** Assisting in spatial positioning analysis and historical play review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030a1e4",
   "metadata": {},
   "source": [
    "## Annotate your Custom dataset using Labellerr\n",
    "\n",
    " ***1. Visit the [Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) website and click **‚ÄúSign Up‚Äù**.*** \n",
    "\n",
    " ***2. After signing in, create your workspace by entering a unique name.***\n",
    "\n",
    " ***3. Navigate to your workspace‚Äôs API keys page (e.g., `https://<your-workspace>.labellerr.com/workspace/api-keys`) to generate your **API Key** and **API Secret**.***\n",
    "\n",
    " ***4. Store the credentials securely, and then use them to initialise the SDK or API client with `api_key`, `api_secret`.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c23f05",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "This section imports all the required libraries used throughout the project for computer vision, visualization, deep learning, and structured coding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba8b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16741128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolo_finetune_utils' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d4363",
   "metadata": {},
   "source": [
    "## Random Frame Extraction from Video\n",
    "\n",
    "Extracts a fixed number of high-quality frames from one or more videos to create an image dataset for annotation and training.\n",
    "\n",
    "### üîπ Purpose\n",
    "- Convert raw manufacturing videos into individual image frames  \n",
    "- Perform random sampling to avoid frame bias  \n",
    "- Prepare data for annotation and YOLO training  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e94cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Extracted 50 frames to folder: dataset_frames\n"
     ]
    }
   ],
   "source": [
    "from yolo_finetune_utils.frame_extractor import extract_random_frames\n",
    "\n",
    "extract_random_frames(\n",
    "    paths=['Untitled design.mp4'],\n",
    "    total_images=50,\n",
    "    out_dir=\"dataset_frames\",\n",
    "    jpg_quality=100,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849321d",
   "metadata": {},
   "source": [
    "## Download Annotations from Labellerr\n",
    "\n",
    "After completing data labeling on the **Labellerr** platform, export the annotations in **COCO JSON format**.\n",
    "\n",
    "Download the COCO JSON file from the Labellerr website and upload it into this project workspace to use it for further dataset preparation and training.\n",
    "\n",
    "This COCO JSON file will be used in the next steps for:\n",
    "- Frame‚Äìannotation alignment\n",
    "- COCO ‚Üí YOLO format conversion\n",
    "- Model training and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e52edb",
   "metadata": {},
   "source": [
    "# COCO to YOLO Format Conversion\n",
    "\n",
    "Converts COCO-style segmentation annotations to YOLO segmentation dataset format.  \n",
    "- Requires: `annotation.json` and images in `frames_output` directory.\n",
    "- Output: Generated YOLO dataset folder.\n",
    "- Parameters: allows train/val split, shuffling, and verbose mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1479cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Stats: {'train': 35, 'val': 10, 'test': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'stats': {'train': 35, 'val': 10, 'test': 5}, 'output_dir': 'yolo_dataset'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.seg_converter import coco_to_yolo_converter\n",
    "\n",
    "coco_to_yolo_converter(\n",
    "    json_path=\"export-#602ada0d-f6ae-4958-aaf7-845c0797c060.json\",\n",
    "    images_dir=\"dataset_frames\",\n",
    "    output_dir=\"yolo_dataset\",\n",
    "    use_split=True,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.1,\n",
    "    shuffle=True,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b83bb",
   "metadata": {},
   "source": [
    "# Load and Train YOLO Segmentation Model\n",
    "\n",
    "Loads the YOLO segmentation model and trains it using the converted YOLO dataset.\n",
    "- Data: Path to YOLO-style `data.yaml`\n",
    "- Parameters: epochs, image size, batch size, device, dataloader workers, experiment name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315311c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the nano segmentation model\n",
    "model = YOLO('yolo11x-seg.pt')\n",
    "\n",
    "# Start training\n",
    "model.train(\n",
    "    data='/content/holo/yolo_dataset/data.yaml',\n",
    "    epochs=150,\n",
    "    imgsz=640,\n",
    "    device=0,\n",
    "    project='/content/drive/MyDrive/holo',\n",
    "    name='holo_final_run'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5f623",
   "metadata": {},
   "source": [
    "### 1. High-Resolution Instance Segmentation\n",
    "This stage handles the core computer vision task: detecting and segmenting players using **YOLO11**. By enabling `retina_masks`, we ensure the green silhouettes are sharp and pixel-accurate, rather than blocky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your trained model\n",
    "model = YOLO('/content/drive/MyDrive/holo/holo_final_run3/weights/best.pt')\n",
    "\n",
    "# Run tracking with High-Resolution (Retina) masks\n",
    "results_generator = model.track(\n",
    "    source='/content/drive/MyDrive/holo/Untitled design.mp4',\n",
    "    conf=0.15,\n",
    "    iou=0.5,\n",
    "    persist=True,\n",
    "    stream=True,         # Keeps RAM low\n",
    "    retina_masks=True,   # CRITICAL: Forces high-resolution pixel masks\n",
    "    boxes=False,         # Hides the bounding boxes\n",
    "    save=True,           # Automatically saves the high-quality video\n",
    "    project='/content/drive/MyDrive/holo',\n",
    "    name='high_res_segmentation'\n",
    ")\n",
    "\n",
    "# Iterate to process\n",
    "for result in results_generator:\n",
    "    pass\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac202049",
   "metadata": {},
   "source": [
    "### 2. High-Resolution Kinematic Tracking & Trajectories\n",
    "\n",
    "This module focuses on the visual representation of player movement. It utilizes **YOLO11 Retina Masks** for sharp silhouettes and implements a custom trajectory system. By anchoring motion \"tails\" to the player's ground-contact point (the feet), the system provides a clean, professional aesthetic suitable for broadcast analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Load Model\n",
    "model = YOLO('/content/drive/MyDrive/holo/holo_final_run3/weights/best.pt')\n",
    "\n",
    "# 2. Paths\n",
    "video_path = '/content/drive/MyDrive/holo/Untitled design.mp4'\n",
    "output_path = '/content/drive/MyDrive/holo/clean_mask_trajectory.mp4'\n",
    "\n",
    "# 3. Setup Video Properties\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "# 4. Storage for Trajectories\n",
    "trajectories = defaultdict(list)\n",
    "mask_color = (0, 255, 0) # Green in BGR\n",
    "\n",
    "# 5. Run Tracking\n",
    "results_generator = model.track(\n",
    "    source=video_path,\n",
    "    conf=0.5,\n",
    "    persist=True,\n",
    "    stream=True,\n",
    "    retina_masks=True, # High resolution\n",
    "    boxes=False\n",
    ")\n",
    "\n",
    "for res in results_generator:\n",
    "    frame = res.orig_img.copy()\n",
    "\n",
    "    if res.masks is None or res.boxes.id is None:\n",
    "        out.write(frame)\n",
    "        continue\n",
    "\n",
    "    track_ids = res.boxes.id.cpu().numpy().astype(int)\n",
    "    class_ids = res.boxes.cls.cpu().numpy().astype(int)\n",
    "    masks = res.masks.xy\n",
    "\n",
    "    for i, (tid, cid) in enumerate(zip(track_ids, class_ids)):\n",
    "        if cid == 1: # Only Players\n",
    "            # --- 1. Draw Green Mask ---\n",
    "            overlay = frame.copy()\n",
    "            polygon = masks[i].astype(np.int32)\n",
    "            cv2.fillPoly(overlay, [polygon], mask_color)\n",
    "            cv2.addWeighted(overlay, 0.4, frame, 0.6, 0, frame)\n",
    "            cv2.polylines(frame, [polygon], True, mask_color, 2)\n",
    "\n",
    "            # --- 2. Update Trajectory (using feet) ---\n",
    "            foot_point = polygon[np.argmax(polygon[:, 1])]\n",
    "            cx, cy = int(foot_point[0]), int(foot_point[1])\n",
    "            trajectories[tid].append((cx, cy))\n",
    "\n",
    "            # Keep tail length manageable (30 frames)\n",
    "            if len(trajectories[tid]) > 30:\n",
    "                trajectories[tid].pop(0)\n",
    "\n",
    "            # --- 3. Draw Trajectory Line ---\n",
    "            if len(trajectories[tid]) >= 2:\n",
    "                for j in range(1, len(trajectories[tid])):\n",
    "                    cv2.line(frame, trajectories[tid][j-1], trajectories[tid][j], mask_color, 2)\n",
    "\n",
    "            # --- 4. Track ID Drawing Removed ---\n",
    "            # No text labels will be drawn on the frame\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"‚úÖ Clean video saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52003e3",
   "metadata": {},
   "source": [
    "### 1. Interactive Geometric Calibration\n",
    "\n",
    "The first phase of the pipeline involves establishing a spatial relationship between the video pixels and the real-world dimensions of the tennis court. This script provides an interactive GUI for manual coordinate marking, allowing the user to define key court intersections that serve as the anchor points for the **Homography Matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5755fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click points in order. Press 'q' to save and exit.\n",
      "Point Marked: 531, 1689\n",
      "Point Marked: 1230, 636\n",
      "Point Marked: 2586, 639\n",
      "Point Marked: 3306, 1692\n",
      "Point Marked: 1008, 981\n",
      "Point Marked: 1911, 981\n",
      "Point Marked: 2814, 984\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "points = []\n",
    "\n",
    "def click_event(event, x, y, flags, params):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # Scale back the coordinates to original resolution if you scaled the frame\n",
    "        print(f\"Point Marked: {x}, {y}\")\n",
    "        points.append((x, y))\n",
    "        cv2.circle(display_frame, (x, y), 5, (0, 0, 255), -1)\n",
    "        cv2.imshow(\"Mark Points\", display_frame)\n",
    "\n",
    "cap = cv2.VideoCapture('Untitled design.mp4')\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if ret:\n",
    "    display_frame = frame.copy()\n",
    "    \n",
    "    # 1. Create a resizable window\n",
    "    cv2.namedWindow(\"Mark Points\", cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    # 2. (Optional) Resize the window to a viewable size (e.g., 1280x720)\n",
    "    cv2.resizeWindow(\"Mark Points\", 1280, 720)\n",
    "    \n",
    "    cv2.imshow(\"Mark Points\", display_frame)\n",
    "    cv2.setMouseCallback(\"Mark Points\", click_event)\n",
    "\n",
    "    print(\"Click points in order. Press 'q' to save and exit.\")\n",
    "    while True:\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b798332",
   "metadata": {},
   "source": [
    "### 3. Digital Homography & Tactical Mapping\n",
    "\n",
    "This core module acts as the \"Brain\" of the **HoloField** system. It translates raw pixel detections into a metric-accurate, top-down tactical miniature. By applying geometric transformation and temporal smoothing, it creates a professional-grade \"God-view\" of the match.\n",
    "\n",
    "\n",
    "\n",
    "#### I. Mathematical Core: Planar Homography\n",
    "The system establishes a relationship between the distorted camera view and a flat, metric representation of a tennis court ($10.97m \\times 23.77m$). \n",
    "\n",
    "* **RANSAC Homography:** Using 7 calibrated points, the system calculates a $3 \\times 3$ matrix ($H$) that maps any pixel $(x, y)$ to a real-world coordinate $(M_x, M_y)$. The use of RANSAC ensures that slight inaccuracies in manual point selection do not ruin the overall geometric projection.\n",
    "* **Coordinate Buffering:** A $2.5m$ padding is added to all sides of the metric court, ensuring that players running behind the baseline or wide of the sidelines remain visible on the tactical map.\n",
    "\n",
    "#### II. Kinematic Smoothing (EMA)\n",
    "To eliminate \"jitter\" caused by pixel-level fluctuations in the YOLO segmentation masks, the system implements an **Exponential Moving Average (EMA)** filter:\n",
    "\n",
    "$$Position_{smooth} = \\alpha \\cdot Position_{current} + (1 - \\alpha) \\cdot Position_{previous}$$\n",
    "\n",
    "By setting $\\alpha = 0.2$, the system prioritizes fluid motion over raw pixel noise, resulting in \"broadcast-quality\" movement of the player markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a89fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. SETUP HOMOGRAPHY (Keeping your verified points)\n",
    "src_pts = np.array([[531, 1689], [1230, 636], [2586, 639], [3306, 1692],\n",
    "                    [1008, 981], [1911, 981], [2814, 984]], dtype=np.float32)\n",
    "pad = 2.5\n",
    "dst_pts = np.array([[0+pad, 23.77+pad], [0+pad, 0+pad], [10.97+pad, 0+pad], [10.97+pad, 23.77+pad],\n",
    "                    [0+pad, 11.885+pad], [5.485+pad, 11.885+pad], [10.97+pad, 11.885+pad]], dtype=np.float32)\n",
    "H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# 2. MINIATURE SETUP\n",
    "scale = 40\n",
    "map_w, map_h = int((10.97 + 2*pad) * scale), int((23.77 + 2*pad) * scale)\n",
    "net_line_y = 11.885\n",
    "\n",
    "# --- SMOOTHING SETUP ---\n",
    "# Dictionary to store the previous position of each player ID\n",
    "# smoothed_pos = alpha * current + (1 - alpha) * previous\n",
    "smoothed_positions = {}\n",
    "alpha = 0.2  # Lower alpha = smoother movement, but more \"lag\". 0.1 to 0.3 is the sweet spot.\n",
    "\n",
    "def get_miniature_base():\n",
    "    court = np.zeros((map_h, map_w, 3), dtype=np.uint8)\n",
    "    court[:] = (34, 139, 34)\n",
    "    m_to_p = lambda x, y: (int((x + pad) * scale), int((y + pad) * scale))\n",
    "    white = (255, 255, 255)\n",
    "    cv2.rectangle(court, m_to_p(0, 0), m_to_p(10.97, 23.77), white, 2)\n",
    "    for x_val in [1.37, 9.6]: cv2.line(court, m_to_p(x_val, 0), m_to_p(x_val, 23.77), white, 1)\n",
    "    for y_val in [5.485, 18.285]: cv2.line(court, m_to_p(1.37, y_val), m_to_p(9.6, y_val), white, 1)\n",
    "    cv2.line(court, m_to_p(5.485, 5.485), m_to_p(5.485, 18.285), white, 1)\n",
    "    cv2.line(court, m_to_p(0, 11.885), m_to_p(10.97, 11.885), (0, 0, 0), 2)\n",
    "    return court\n",
    "\n",
    "# 3. STREAMING INFERENCE\n",
    "model = YOLO('/content/drive/MyDrive/holo/holo_final_run3/weights/best.pt')\n",
    "out = cv2.VideoWriter('smooth_holograph.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (map_w, map_h))\n",
    "\n",
    "for result in model.track(source='/content/drive/MyDrive/holo/Untitled design.mp4', stream=True, persist=True, conf=0.15, retina_masks=True):\n",
    "    mini_map = get_miniature_base()\n",
    "\n",
    "    if result.masks is not None and result.boxes.id is not None:\n",
    "        boxes = result.boxes\n",
    "        ids = boxes.id.cpu().numpy().astype(int)\n",
    "        clss = boxes.cls.cpu().numpy().astype(int)\n",
    "        confs = boxes.conf.cpu().numpy()\n",
    "\n",
    "        for i, obj_id in enumerate(ids):\n",
    "            cls = clss[i]\n",
    "            conf = confs[i]\n",
    "\n",
    "            # REMOVED BALL: Only process players with high confidence\n",
    "            if cls == 1 and conf >= 0.5:\n",
    "                mask = result.masks.xy[i]\n",
    "                foot_pixel = mask[np.argmax(mask[:, 1])]\n",
    "\n",
    "                # Homography\n",
    "                pt = np.array([foot_pixel[0], foot_pixel[1], 1.0]).reshape(3, 1)\n",
    "                m_pt = np.dot(H, pt)\n",
    "                mx, my = (m_pt[0]/m_pt[2]).item(), (m_pt[1]/m_pt[2]).item()\n",
    "\n",
    "                # --- APPLY SMOOTHING ---\n",
    "                if obj_id not in smoothed_positions:\n",
    "                    smoothed_positions[obj_id] = (mx, my)\n",
    "                else:\n",
    "                    prev_x, prev_y = smoothed_positions[obj_id]\n",
    "                    # EMA Formula\n",
    "                    mx = alpha * mx + (1 - alpha) * prev_x\n",
    "                    my = alpha * my + (1 - alpha) * prev_y\n",
    "                    smoothed_positions[obj_id] = (mx, my)\n",
    "\n",
    "                # --- DRAWING ---\n",
    "                # Team A (Far): Red (0,0,255) | Team B (Near): Blue (255,0,0)\n",
    "                dot_color = (0, 0, 255) if (my - pad) < net_line_y else (255, 0, 0)\n",
    "\n",
    "                center = (int(mx * scale), int(my * scale))\n",
    "                cv2.circle(mini_map, center, 15, dot_color, -1) # INCREASED SIZE to 15\n",
    "                cv2.circle(mini_map, center, 15, (255, 255, 255), 2) # White border for contrast\n",
    "\n",
    "    out.write(mini_map)\n",
    "\n",
    "out.release()\n",
    "print(\"Success! Smooth tracking video saved to smooth_holograph.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089233e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
