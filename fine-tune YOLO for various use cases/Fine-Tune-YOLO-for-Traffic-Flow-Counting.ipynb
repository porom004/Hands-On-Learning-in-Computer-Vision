{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a44c088",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune YOLO for Traffic Flow Counting**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "[![Scientific Paper](https://img.shields.io/badge/Official-Paper-blue.svg)](<PAPER LINK>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d2422",
   "metadata": {},
   "source": [
    "## **Setup and Imports**\n",
    "Import required libraries for computer vision, deep learning, and video processing. This includes OpenCV for video handling, NumPy for array operations, and Ultralytics YOLO for object detection and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict, deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016976d",
   "metadata": {},
   "source": [
    "## **Plotting Region to Track Object**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decda8f",
   "metadata": {},
   "source": [
    "## **Interactive Polygon Drawing**\n",
    "Define mouse callback functions to draw polygon regions on video frames interactively. Users can left-click to add vertices and right-click to close polygons. These regions will be used to count vehicles passing through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = []\n",
    "current_polygon = []\n",
    "\n",
    "def draw_polygon(event, x, y, flags, param):\n",
    "    global current_polygon\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        current_polygon.append((x, y))\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        if len(current_polygon) > 2:\n",
    "            polygons.append(current_polygon.copy())\n",
    "        current_polygon = []\n",
    "\n",
    "def create_polygons_fullscreen(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "    \n",
    "    _, frame = cap.read()  # Read single frame to get video resolution\n",
    "    if frame is None:\n",
    "        print(\"Error: Could not read video frame.\")\n",
    "        return []\n",
    "\n",
    "    cv2.namedWindow('Create Polygons', cv2.WINDOW_NORMAL)\n",
    "    cv2.setWindowProperty('Create Polygons', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "    cv2.setMouseCallback('Create Polygons', draw_polygon)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Loop video\n",
    "            continue\n",
    "\n",
    "        # Draw existing polygons in green\n",
    "        for poly in polygons:\n",
    "            pts = np.array(poly, np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(frame, [pts], True, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw current polygon being drawn in red\n",
    "        if len(current_polygon) > 1:\n",
    "            pts = np.array(current_polygon, np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(frame, [pts], False, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, 'Left click: Add point', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "        cv2.putText(frame, 'Right click: Close polygon', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "        cv2.putText(frame, 'Press Q to Quit', (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "\n",
    "        cv2.imshow('Create Polygons', frame)\n",
    "        key = cv2.waitKey(20) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return polygons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a842ed4",
   "metadata": {},
   "source": [
    "### **Polygon Creation in Fullscreen**\n",
    "Create a fullscreen window that allows users to draw multiple polygon regions on the first video frame. The function loops through the video and displays it while waiting for polygon input via mouse clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = create_polygons_fullscreen('assests/1.mp4')\n",
    "print(polygons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77780c",
   "metadata": {},
   "source": [
    "### **Create Polygons from Video**\n",
    "Launch the interactive polygon drawing tool on the video. Draw counting regions by left-clicking vertices and right-clicking to close each polygon. Press Q to finish and save polygon coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065df409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_polygons_on_video_matplotlib(video_path, polygons):\n",
    "    # Read the first frame from the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        print(\"Failed to read video frame.\")\n",
    "        return\n",
    "\n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(frame_rgb)\n",
    "\n",
    "    # Draw polygons in different colors\n",
    "    for poly in polygons:\n",
    "        # Generate a random color for each polygon, with 0.5 alpha\n",
    "        color = [random.random(), random.random(), random.random(), 0.5]\n",
    "        patch = patches.Polygon(poly, closed=True, facecolor=color, edgecolor=color[:3], linewidth=2)\n",
    "        ax.add_patch(patch)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85095f5",
   "metadata": {},
   "source": [
    "### **Visualize Polygons on Video**\n",
    "Display the created polygon regions overlaid on the first video frame using matplotlib. Each polygon is shown with a semi-transparent color for easy visualization and verification before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "polygons_1 = [\n",
    "    [(687, 870), (834, 729), (1437, 732), (1455, 1374), (102, 1329)],\n",
    "    [(1707, 720), (2118, 732), (2274, 723), (3204, 1191), (2085, 1230)]\n",
    "]\n",
    "show_polygons_on_video_matplotlib('assests/1.mp4', polygons_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77267f03",
   "metadata": {},
   "source": [
    "## **Counting the vehicle which passed through the region**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008277e",
   "metadata": {},
   "source": [
    "### **Vehicle Counter Class**\n",
    "Core class that performs YOLO-based object detection and tracking on video frames. It manages polygon regions, tracks vehicle movements, counts vehicles passing through each region, and renders results with colored overlays and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleCounter:\n",
    "    def __init__(self, polygons, model_path=None, target_classes=None):\n",
    "        \"\"\"\n",
    "        Initialize the vehicle counter with polygon regions\n",
    "        \n",
    "        Args:\n",
    "            polygons: List of polygon regions as [(x1,y1), (x2,y2), ...]\n",
    "            model_path: Path to custom YOLO model (optional)\n",
    "            target_classes: Dict of class_id: class_name for custom models (optional)\n",
    "        \"\"\"\n",
    "        self.polygons = polygons\n",
    "        self.region_counters = [0] * len(polygons)\n",
    "        self.region_colors = self._generate_region_colors()\n",
    "        self.tracked_objects = {}  # track_id: {last_region: int, history: deque}\n",
    "        \n",
    "        # Set up model and target classes\n",
    "        self.model_path = model_path\n",
    "        self._setup_model_and_classes(model_path, target_classes)\n",
    "        \n",
    "    def _setup_model_and_classes(self, model_path, target_classes):\n",
    "        \"\"\"Setup YOLO model and target classes\"\"\"\n",
    "        if model_path is None:\n",
    "            # Default: Use pre-trained YOLO model with COCO classes\n",
    "            self.model = YOLO('yolov8x.pt')\n",
    "            self.target_classes = {\n",
    "                2: 'car',\n",
    "                5: 'bus', \n",
    "                7: 'truck'\n",
    "            }\n",
    "            print(\"Using default YOLOv8x model with COCO dataset classes\")\n",
    "        else:\n",
    "            # Custom model\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Custom model file not found: {model_path}\")\n",
    "            \n",
    "            self.model = YOLO(model_path)\n",
    "            \n",
    "            if target_classes is None:\n",
    "                # Try to get class names from model\n",
    "                if hasattr(self.model.model, 'names'):\n",
    "                    model_names = self.model.model.names\n",
    "                    self.target_classes = {i: name for i, name in model_names.items()}\n",
    "                    print(f\"Using all classes from custom model: {list(self.target_classes.values())}\")\n",
    "                else:\n",
    "                    # Fallback: assume single class or ask user to provide\n",
    "                    print(\"Warning: Could not determine class names from custom model.\")\n",
    "                    print(\"Assuming single class 'object'. Consider providing target_classes parameter.\")\n",
    "                    self.target_classes = {0: 'object'}\n",
    "            else:\n",
    "                # Use provided target classes\n",
    "                self.target_classes = target_classes\n",
    "                print(f\"Using custom model with specified classes: {list(target_classes.values())}\")\n",
    "            \n",
    "            print(f\"Loaded custom model from: {model_path}\")\n",
    "        \n",
    "    def _generate_region_colors(self):\n",
    "        \"\"\"Generate unique colors for each polygon region\"\"\"\n",
    "        color_names = ['red', 'green', 'blue', 'yellow', 'purple', 'orange', 'cyan', 'magenta', 'gray', 'pink', 'dark orange', 'teal', 'deep pink', 'hot pink', 'red orange']\n",
    "        colors = [\n",
    "            (0, 0, 255),    # red\n",
    "            (0, 255, 0),    # green\n",
    "            (255, 0, 0),    # blue\n",
    "            (0, 255, 255),  # yellow\n",
    "            (128, 0, 128),  # purple\n",
    "            (0, 165, 255),  # orange\n",
    "            (255, 255, 0),  # cyan\n",
    "            (255, 0, 255),   # magenta\n",
    "            (128, 128, 128),  # gray\n",
    "            (255, 192, 203), # pink\n",
    "            (255, 140, 0),   # dark orange\n",
    "            (0, 128, 128),   # teal\n",
    "            (255, 20, 147),  # deep pink\n",
    "            (255, 105, 180), # hot pink\n",
    "            (255, 69, 0)    # red orange\n",
    "        ]\n",
    "        \n",
    "        region_colors = []\n",
    "        for i in range(len(self.polygons)):\n",
    "            color_idx = i % len(colors)\n",
    "            region_colors.append({\n",
    "                'color': colors[color_idx],\n",
    "                'name': color_names[color_idx] if i < len(color_names) else f'region_{i}'\n",
    "            })\n",
    "        \n",
    "        return region_colors\n",
    "    \n",
    "    def _point_in_polygon(self, point, polygon):\n",
    "        \"\"\"Check if a point is inside a polygon using ray casting algorithm\"\"\"\n",
    "        x, y = point\n",
    "        n = len(polygon)\n",
    "        inside = False\n",
    "        \n",
    "        p1x, p1y = polygon[0]\n",
    "        for i in range(1, n + 1):\n",
    "            p2x, p2y = polygon[i % n]\n",
    "            if y > min(p1y, p2y):\n",
    "                if y <= max(p1y, p2y):\n",
    "                    if x <= max(p1x, p2x):\n",
    "                        if p1y != p2y:\n",
    "                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x\n",
    "                        if p1x == p2x or x <= xinters:\n",
    "                            inside = not inside\n",
    "            p1x, p1y = p2x, p2y\n",
    "        \n",
    "        return inside\n",
    "    \n",
    "    def _get_object_region(self, center_point):\n",
    "        \"\"\"Determine which region (if any) contains the object center point\"\"\"\n",
    "        for region_idx, polygon in enumerate(self.polygons):\n",
    "            if self._point_in_polygon(center_point, polygon):\n",
    "                return region_idx\n",
    "        return -1  # Not in any region\n",
    "    \n",
    "    def _draw_polygons(self, frame):\n",
    "        \"\"\"Draw polygon regions on the frame with colors and counters\"\"\"\n",
    "        overlay = frame.copy()\n",
    "        \n",
    "        for i, polygon in enumerate(self.polygons):\n",
    "            # Convert polygon to numpy array for OpenCV\n",
    "            pts = np.array(polygon, np.int32)\n",
    "            pts = pts.reshape((-1, 1, 2))\n",
    "            \n",
    "            # Draw filled polygon with transparency\n",
    "            cv2.fillPoly(overlay, [pts], self.region_colors[i]['color'])\n",
    "            \n",
    "            # Draw polygon outline\n",
    "            cv2.polylines(frame, [pts], True, self.region_colors[i]['color'], 3)\n",
    "            \n",
    "            # Calculate centroid for text placement\n",
    "            moments = cv2.moments(pts)\n",
    "            if moments['m00'] != 0:\n",
    "                cx = int(moments['m10'] / moments['m00'])\n",
    "                cy = int(moments['m01'] / moments['m00'])\n",
    "            else:\n",
    "                cx, cy = polygon[0]  # fallback to first point\n",
    "            \n",
    "            # Draw region info at centroid\n",
    "            region_text = f\"{self.region_colors[i]['name']}: {self.region_counters[i]}\"\n",
    "            text_size = cv2.getTextSize(region_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "            \n",
    "            # Background rectangle for text at centroid\n",
    "            cv2.rectangle(frame, \n",
    "                        (cx - text_size[0]//2 - 5, cy - text_size[1] - 5),\n",
    "                        (cx + text_size[0]//2 + 5, cy + 5),\n",
    "                        (0, 0, 0), -1)\n",
    "            \n",
    "            # Text at centroid\n",
    "            cv2.putText(frame, region_text, \n",
    "                    (cx - text_size[0]//2, cy),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        # Blend overlay with original frame for transparency effect\n",
    "        cv2.addWeighted(overlay, 0.3, frame, 0.7, 0, frame)\n",
    "        \n",
    "        # Draw region information on top right side\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        font_scale = 1.0  \n",
    "        font_thickness = 2\n",
    "        padding = 20\n",
    "        line_spacing = 100\n",
    "        \n",
    "        # Start position for top right display\n",
    "        start_y = padding + 90  \n",
    "        \n",
    "        for i, polygon in enumerate(self.polygons):\n",
    "            region_text = f\"{self.region_colors[i]['name']}: {self.region_counters[i]}\"\n",
    "            text_size = cv2.getTextSize(region_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]\n",
    "            text_width, text_height = text_size\n",
    "            \n",
    "            # Calculate position (right aligned with padding)\n",
    "            text_x = frame_width - text_width - padding\n",
    "            text_y = start_y + (i * line_spacing)\n",
    "            \n",
    "            # Draw background rectangle for better visibility\n",
    "            rect_padding = 15  \n",
    "            cv2.rectangle(frame,\n",
    "                        (text_x - rect_padding, text_y - text_height - rect_padding),\n",
    "                        (text_x + text_width + rect_padding, text_y + rect_padding),\n",
    "                        (0, 0, 0), -1)\n",
    "            \n",
    "            # Draw colored indicator bar next to text\n",
    "            indicator_width = 24  \n",
    "            cv2.rectangle(frame,\n",
    "                        (text_x - rect_padding - indicator_width - 15, text_y - text_height - rect_padding),\n",
    "                        (text_x - rect_padding - 15, text_y + rect_padding),\n",
    "                        self.region_colors[i]['color'], -1)\n",
    "            \n",
    "            # Draw the text\n",
    "            cv2.putText(frame, region_text,\n",
    "                    (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def _update_tracking(self, track_id, current_region):\n",
    "        \"\"\"Update tracking information and count if object crosses into new region\"\"\"\n",
    "        if track_id not in self.tracked_objects:\n",
    "            self.tracked_objects[track_id] = {\n",
    "                'last_region': current_region,\n",
    "                'history': deque(maxlen=10)  # Keep last 10 region positions\n",
    "            }\n",
    "        \n",
    "        obj_data = self.tracked_objects[track_id]\n",
    "        obj_data['history'].append(current_region)\n",
    "        \n",
    "        # Check if object moved from outside/different region into current region\n",
    "        if (obj_data['last_region'] != current_region and \n",
    "            current_region != -1 and \n",
    "            len(obj_data['history']) >= 2):\n",
    "            \n",
    "            # Count only if object was previously outside this region or in a different region\n",
    "            if obj_data['last_region'] != current_region:\n",
    "                self.region_counters[current_region] += 1\n",
    "                print(f\"Object {track_id} entered {self.region_colors[current_region]['name']} region. Count: {self.region_counters[current_region]}\")\n",
    "        \n",
    "        obj_data['last_region'] = current_region\n",
    "    \n",
    "    def process_video(self, video_path, output_path, confidence_threshold=0.2):\n",
    "        \"\"\"\n",
    "        Process video and save with region counting\n",
    "        \n",
    "        Args:\n",
    "            video_path: Path to input video\n",
    "            output_path: Path to save output video\n",
    "            confidence_threshold: Minimum confidence threshold for detections\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Error opening video file: {video_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Setup video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        print(f\"Processing video: {video_path}\")\n",
    "        print(f\"Output will be saved to: {output_path}\")\n",
    "        print(f\"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "        print(f\"Using model: {self.model_path if self.model_path else 'YOLOv8x (default)'}\")\n",
    "        print(f\"Target classes: {list(self.target_classes.values())}\")\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame_count += 1\n",
    "                \n",
    "                # Run YOLO tracking on the frame\n",
    "                # For custom models, we might need to specify classes or use all classes\n",
    "                if self.model_path is None:\n",
    "                    # Default model - specify target classes\n",
    "                    results = self.model.track(frame, persist=True, classes=list(self.target_classes.keys()))\n",
    "                else:\n",
    "                    # Custom model - use all classes or specified ones\n",
    "                    results = self.model.track(frame, persist=True)\n",
    "                \n",
    "                if results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "                    # Get detections\n",
    "                    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "                    track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "                    classes = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "                    confidences = results[0].boxes.conf.cpu().numpy()\n",
    "                    \n",
    "                    # Process each detection\n",
    "                    for box, track_id, cls, conf in zip(boxes, track_ids, classes, confidences):\n",
    "                        # Check if this class is in our target classes and meets confidence threshold\n",
    "                        if cls in self.target_classes and conf > confidence_threshold:\n",
    "                            # Get center point of bounding box\n",
    "                            x1, y1, x2, y2 = box\n",
    "                            center_x = int((x1 + x2) / 2)\n",
    "                            center_y = int((y1 + y2) / 2)\n",
    "                            center_point = (center_x, center_y)\n",
    "                            \n",
    "                            # Determine which region the object is in\n",
    "                            current_region = self._get_object_region(center_point)\n",
    "                            \n",
    "                            # Update tracking and counting\n",
    "                            self._update_tracking(track_id, current_region)\n",
    "                            \n",
    "                            # Draw bounding box and label\n",
    "                            color = (0, 255, 0)  # Green for detections\n",
    "                            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "                            \n",
    "                            class_name = self.target_classes.get(cls, f'class_{cls}')\n",
    "                            label = f\"{class_name} ID:{track_id} {conf:.2f}\"\n",
    "                            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "                            cv2.rectangle(frame, (int(x1), int(y1) - label_size[1] - 5), \n",
    "                                        (int(x1) + label_size[0], int(y1)), color, -1)\n",
    "                            cv2.putText(frame, label, (int(x1), int(y1) - 5),\n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                            \n",
    "                            # Draw center point\n",
    "                            cv2.circle(frame, center_point, 3, (0, 0, 255), -1)\n",
    "                \n",
    "                # Draw polygon regions and counters\n",
    "                frame = self._draw_polygons(frame)\n",
    "                \n",
    "                # Add frame info\n",
    "                info_text = f\"Frame: {frame_count}/{total_frames}\"\n",
    "                cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "                \n",
    "                # Write frame to output video\n",
    "                out.write(frame)\n",
    "                \n",
    "                # Progress update\n",
    "                if frame_count % 100 == 0:\n",
    "                    print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during processing: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            \n",
    "        print(\"Processing completed!\")\n",
    "        print(\"Final counts per region:\")\n",
    "        for i, count in enumerate(self.region_counters):\n",
    "            print(f\"  {self.region_colors[i]['name']} region: {count}\")\n",
    "\n",
    "def count_vehicles_in_regions(polygons, video_path, output_path=\"output_with_counting.mp4\", \n",
    "                            model_path=None, target_classes=None, confidence_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Main function to count vehicles passing through polygon regions\n",
    "    \n",
    "    Args:\n",
    "        polygons: List of polygon regions as [[(x1,y1), (x2,y2), ...], ...]\n",
    "        video_path: Path to input video file\n",
    "        output_path: Path to save output video (default: \"output_with_counting.mp4\")\n",
    "        model_path: Path to custom YOLO model (optional, uses YOLOv8x if None)\n",
    "        target_classes: Dict of class_id: class_name for custom models (optional)\n",
    "        confidence_threshold: Minimum confidence threshold for detections (default: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Final counts for each region\n",
    "    \n",
    "    Examples:\n",
    "        # Using default YOLO model\n",
    "        counts = count_vehicles_in_regions(polygons, \"input_video.mp4\")\n",
    "        \n",
    "        # Using custom YOLO model with all classes\n",
    "        counts = count_vehicles_in_regions(polygons, \"input_video.mp4\", \n",
    "                                         model_path=\"my_custom_model.pt\")\n",
    "        \n",
    "        # Using custom YOLO model with specific target classes\n",
    "        custom_classes = {0: 'vehicle', 1: 'person', 2: 'bicycle'}\n",
    "        counts = count_vehicles_in_regions(polygons, \"input_video.mp4\", \n",
    "                                         model_path=\"my_custom_model.pt\",\n",
    "                                         target_classes=custom_classes)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create vehicle counter instance with custom model support\n",
    "        counter = VehicleCounter(polygons, model_path=model_path, target_classes=target_classes)\n",
    "        \n",
    "        # Process the video\n",
    "        counter.process_video(video_path, output_path, confidence_threshold=confidence_threshold)\n",
    "        \n",
    "        # Return final counts\n",
    "        final_counts = {}\n",
    "        for i, count in enumerate(counter.region_counters):\n",
    "            region_name = counter.region_colors[i]['name']\n",
    "            final_counts[region_name] = count\n",
    "        \n",
    "        return final_counts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in count_vehicles_in_regions: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7494c69",
   "metadata": {},
   "source": [
    "### **Process Video with Default Model**\n",
    "Execute the vehicle counting pipeline using the default pre-trained YOLOv8x model (COCO dataset). Detects cars, buses, and trucks, tracks them across frames, and counts how many pass through each polygon region. Outputs an annotated video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default YOLO model\n",
    "# polygon regions\n",
    "polygons = polygons_1\n",
    "\n",
    "counts1 = count_vehicles_in_regions(\n",
    "    polygons=polygons,\n",
    "    video_path=\"./assests/1.mp4\",\n",
    "    output_path=\"1_result.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee2586",
   "metadata": {},
   "source": [
    "## **Custom Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bff2e9",
   "metadata": {},
   "source": [
    "## Annotate your Custom dataset using Labellerr\n",
    "\n",
    " ***1. Visit the [Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) website and click **‚ÄúSign Up‚Äù**.*** \n",
    "\n",
    " ***2. After signing in, create your workspace by entering a unique name.***\n",
    "\n",
    " ***3. Navigate to your workspace‚Äôs API keys page (e.g., `https://<your-workspace>.labellerr.com/workspace/api-keys`) to generate your **API Key** and **API Secret**.***\n",
    "\n",
    " ***4. Store the credentials securely, and then use them to initialise the SDK or API client with `api_key`, `api_secret`.*** \n",
    "\n",
    "\n",
    "### Use Labellerr SDK for uploading and perform annotation of your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to install required packages in a Jupyter notebook environment\n",
    "\n",
    "# !pip install git+https://github.com/Labellerr/SDKPython.git\n",
    "# !pip install ipyfilechooser\n",
    "# !git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab068b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports required for this notebook\n",
    "from labellerr.client import LabellerrClient\n",
    "from labellerr.core.datasets import create_dataset_from_local\n",
    "from labellerr.core.annotation_templates import create_template\n",
    "from labellerr.core.projects import create_project\n",
    "from labellerr.core.schemas import DatasetConfig, AnnotationQuestion, QuestionType, CreateTemplateParams, DatasetDataType, CreateProjectParams, RotationConfig\n",
    "from labellerr.core.projects import LabellerrProject\n",
    "from labellerr.core.exceptions import LabellerrError\n",
    "\n",
    "import uuid\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e37257",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"YOUR_API_KEY\")        # go to labellerr workspace to get your API key\n",
    "api_secret = input(\"YOUR_API_SECRET\")  # go to labellerr workspace to get your API secret\n",
    "client_id = input(\"YOUR_CLIENT_ID\")   # Contact labellerr support to get your client ID i.e. support@tensormatics.com\n",
    "\n",
    "client = LabellerrClient(api_key, api_secret, client_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53c2c3a",
   "metadata": {},
   "source": [
    "### ***STEP-1: Create a dataset on labellerr from your local folder***\n",
    "\n",
    "The SDK supports in creating dataset by uploading local files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321e551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bab602150d0462d8a724c7f48ac1984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileChooser(path='D:\\', filename='', title='Select a folder containing your dataset', show_hidden=False, selec‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a folder chooser starting from a directory (for example, your home directory)\n",
    "chooser = FileChooser('/')\n",
    "\n",
    "# Set the chooser to folder selection mode only\n",
    "chooser.title = 'Select a folder containing your dataset'\n",
    "chooser.show_only_dirs = True\n",
    "\n",
    "# Display the widget\n",
    "display(chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f93db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: D:\\Professional\\Projects\\Cell_Segmentation_using_YOLO\\frames_output\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = chooser.selected_path\n",
    "print(\"You selected:\", path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dataset type: video\n"
     ]
    }
   ],
   "source": [
    "my_dataset_type = input(\"Enter your dataset type (video or image): \").lower()\n",
    "print(\"Selected dataset type:\", my_dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset_from_local(\n",
    "    client=client,\n",
    "    dataset_config=DatasetConfig(dataset_name=\"My Dataset\", data_type=\"image\"),\n",
    "    folder_to_upload=path_to_dataset\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with ID: {dataset.dataset_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4926c02",
   "metadata": {},
   "source": [
    "### ***STEP-2: Create annotation project on labellerr of your created dataset***\n",
    "\n",
    "Create a annotation project of your uploaded dataset to start performing annotation on labellerr UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annotation guideline template for video annotation project (like classes to be annotated)\n",
    "\n",
    "template = create_template(\n",
    "    client=client,\n",
    "    params=CreateTemplateParams(\n",
    "        template_name=\"My Template\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        questions=[\n",
    "            AnnotationQuestion(\n",
    "                question_number=1,\n",
    "                question=\"Object\",\n",
    "                question_id=str(uuid.uuid4()),\n",
    "                question_type=QuestionType.polygon,\n",
    "                required=True,\n",
    "                color=\"#FF0000\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(f\"Annotation template created with ID: {template.annotation_template_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbe54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.status()        # wait until dataset is processed before creating project\n",
    "\n",
    "project = create_project(\n",
    "    client=client,\n",
    "    params=CreateProjectParams(\n",
    "        project_name=\"My Project\",\n",
    "        data_type=DatasetDataType.image,\n",
    "        rotations=RotationConfig(\n",
    "            annotation_rotation_count=1,\n",
    "            review_rotation_count=1,\n",
    "            client_review_rotation_count=1\n",
    "        )\n",
    "    ),\n",
    "    datasets=[dataset],\n",
    "    annotation_template=template\n",
    ")\n",
    "\n",
    "print(f\"‚úì Project created: {project.project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92373de6",
   "metadata": {},
   "source": [
    "Your project has been created now go to labellerr platform to perform annotation \n",
    "\n",
    "***click to go to labellerr.com***\n",
    "\n",
    "[![Labellerr](https://cdn.labellerr.com/1%20%20Documentation/1c9dc7ce-9a54-4111-8fd5-0363ba3e00e1.webp)](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks)\n",
    "Open the project you created (Projects ‚Üí select your project).\n",
    "\n",
    "Click Start Labeling to open the annotation interface. Use the configured labeling tools (bounding boxes, polygon, dot, classification, etc.) to annotate files.\n",
    "### ***STEP-3: Export your annotation in required format***\n",
    "\n",
    "Generate a temporary download URL to retrieve your exported JSON file:\n",
    "\n",
    "### Export Configuration Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `export_name` | string | Display name for the export |\n",
    "| `export_description` | string | Description of what this export contains |\n",
    "| `export_format` | string | Output format (e.g., `json`, `xml`, `coco`) |\n",
    "| `statuses` | list | Annotation statuses to include in export |\n",
    "\n",
    "### Common Annotation Statuses\n",
    "\n",
    "- **`review`**: Annotations pending review\n",
    "- **`r_assigned`**: Review assigned to a reviewer\n",
    "- **`client_review`**: Under client review\n",
    "- **`cr_assigned`**: Client review assigned\n",
    "- **`accepted`**: Annotations accepted and finalized\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeafaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_config = {\n",
    "    \"export_name\": \"Weekly Export\",\n",
    "    \"export_description\": \"Export of all accepted annotations\",\n",
    "    \"export_format\": \"coco_json\",\n",
    "    \"statuses\": ['review', 'r_assigned','client_review', 'cr_assigned','accepted']\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Get project instance\n",
    "    project = LabellerrProject(client=client, project_id=project.project_id)\n",
    "    \n",
    "    # Create export\n",
    "    result = project.create_local_export(export_config)\n",
    "    export_id = result[\"response\"]['report_id']\n",
    "    print(f\"Local export created successfully. Export ID: {export_id}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Local export creation failed: {str(e)}\")\n",
    "    \n",
    "    \n",
    "try:\n",
    "    download_url = client.fetch_download_url(\n",
    "        project_id=project.project_id,\n",
    "        uuid=str(uuid.uuid4()),\n",
    "        export_id=export_id\n",
    "    )\n",
    "    print(f\"Download URL: {download_url}\")\n",
    "except LabellerrError as e:\n",
    "    print(f\"Failed to fetch download URL: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54538a",
   "metadata": {},
   "source": [
    "Now you can download your annotations locally using given URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ca9a0",
   "metadata": {},
   "source": [
    "## **Convert COCO Annotations to YOLO Format**\n",
    "Convert custom dataset annotations from COCO JSON format to YOLO's required format (normalized bounding box coordinates in .txt files). This prepares the annotated data for fine-tuning the YOLO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.bbox_converter import coco_to_yolo_converter\n",
    "\n",
    "result = coco_to_yolo_converter(\n",
    "            json_path=r'./annotation.json',\n",
    "            images_dir=r'./dataset',\n",
    "            output_dir='yolo_format',\n",
    "            use_split=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff2452",
   "metadata": {},
   "source": [
    "### **Fine-tune YOLO Model**\n",
    "Train a custom YOLOv8x model on the prepared dataset. Runs for 200 epochs with batch size 20 and 640x640 image size. The model learns to detect traffic-specific objects based on your annotated training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccca1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train data=\"./yolo_format/dataset.yaml\" model=\"yolov8x.pt\" epochs=200 imgsz=640 batch=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d3f87",
   "metadata": {},
   "source": [
    "### **Test Fine-tuned Model**\n",
    "Run tracking inference on a test video using the fine-tuned model. Generates predictions with confidence threshold 0.25 and saves the annotated output video to verify model performance on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=track model=\"./runs/detect/train/weights/last.pt\" source=\"./assests/3.mp4\" conf=0.25 save=True show_labels=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f575776",
   "metadata": {},
   "source": [
    "## **Counting Cars using Drone View Camera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c7b37",
   "metadata": {},
   "source": [
    "### **Define Regions for Drone View**\n",
    "Create polygon regions for a drone video footage. These regions divide the aerial view into counting zones for tracking vehicle flow patterns from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d892766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polygon regions\n",
    "polygons2 = [\n",
    "            [(93, 322), (483, 260), (486, 483), (111, 522)],\n",
    "            [(112, 549), (482, 560), (507, 776), (123, 724)],\n",
    "            [(1443, 346), (1479, 564), (1870, 584), (1857, 430)],\n",
    "            [(1478, 598), (1414, 824), (1869, 784), (1868, 598)],\n",
    "            ]\n",
    "\n",
    "show_polygons_on_video_matplotlib('assests/3.mp4', polygons2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a4231a",
   "metadata": {},
   "source": [
    "### **Process Drone Video with Custom Model**\n",
    "Execute vehicle counting on drone footage using the fine-tuned custom YOLO model. Tracks cars in the specified regions and outputs results with region-specific vehicle counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d495864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using custom YOLO model\n",
    "counts2 = count_vehicles_in_regions(\n",
    "    polygons=polygons,\n",
    "    video_path= \"./assests/3.mp4\",\n",
    "    output_path= \"3_result_custom_2.mp4\",\n",
    "    model_path= \"./runs/detect/train/weights/last.pt\",\n",
    "    target_classes= {0: 'car'},  # Optional: specify which classes to track\n",
    "    confidence_threshold=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1f0fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
