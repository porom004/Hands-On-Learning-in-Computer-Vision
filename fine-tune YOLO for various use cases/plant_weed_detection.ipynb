{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb227f43",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Fine-Tune RT-DETR for Plant Weed Detection**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc8c42",
   "metadata": {},
   "source": [
    "1.  **Setup**: Imports libraries and clones the `yolo_finetune_utils` repository for helper functions.\n",
    "2.  **Data Preparation**:\n",
    "    *   Extracts random frames from input videos.\n",
    "    *   Converts annotations from COCO JSON to YOLO format.\n",
    "    *   Splits data into Train, Validation, and Test sets.\n",
    "3.  **Training**: Fine-tunes the **RT-DETR-L** model on the prepared custom dataset for 100 epochs.\n",
    "4.  **Visualization**: Runs inference on a new video and generates an output video on detected weeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb60a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import RTDETR\n",
    "import torch\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from pathlib import Path\n",
    "import random \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94620d",
   "metadata": {},
   "source": [
    "## Setup: Clone YOLO Fine-tuning Utilities for helper functions\n",
    "Clone the `yolo_finetune_utils` repository which contains helper functions for:\n",
    "- Frame extraction from videos\n",
    "- COCO to YOLO format conversion\n",
    "- Dataset preparation utilities\n",
    "> **Note**: Uncomment this cell only if you haven't cloned the repository yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fef5a4",
   "metadata": {},
   "source": [
    "## Data Preparation: Extract Random Frames from Videos\n",
    "Extract random frames from video files to create a dataset for annotation.\n",
    "**Parameters**:\n",
    "- `paths`: List of video directories\n",
    "- `total_images`: Number of frames to extract\n",
    "- `out_dir`: Output directory for extracted frames\n",
    "- `jpg_quality`: JPEG compression quality (100 = highest)\n",
    "- `seed`: Random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee08460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.frame_extractor import extract_random_frames\n",
    "\n",
    "extract_random_frames(\n",
    "        paths=[r\"videos\\manufacturing_video_data\"],\n",
    "        total_images=150,\n",
    "        out_dir=\"dataset_frames\",\n",
    "        jpg_quality=100,\n",
    "        seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945523c",
   "metadata": {},
   "source": [
    "## Data Preparation: Convert Annotations to YOLO Format\n",
    "Convert COCO-format annotations to YOLO format and split the dataset into train/val/test sets.\n",
    "**Configuration**:\n",
    "- **Train/Val/Test Split**: 80% / 10% / 10%\n",
    "- **Input**: COCO JSON annotations + image directory\n",
    "- **Output**: YOLO-format dataset in `model_dataset/`\n",
    "The converter automatically:\n",
    "- Creates train/val/test splits\n",
    "- Generates YOLO-format label files (.txt)\n",
    "- Organizes images and labels into proper directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde40174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.seg_converter import coco_to_yolo_converter\n",
    "\n",
    "ANNOTATION_JSON = \"annotations.json\"\n",
    "IMAGE_DIR = \"dataset_frames\"\n",
    "\n",
    "\n",
    "coco_to_yolo_converter(\n",
    "        json_path=ANNOTATION_JSON,\n",
    "        images_dir=IMAGE_DIR,\n",
    "        output_dir=\"model_dataset\",\n",
    "        use_split=True,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1,\n",
    "        shuffle=True,\n",
    "        verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa1332",
   "metadata": {},
   "source": [
    "## System Check: GPU Memory Status\n",
    "Clear GPU cache and display current memory usage to ensure sufficient resources for training.\n",
    "**Memory Metrics**:\n",
    "- **Allocated**: Currently used GPU memory\n",
    "- **Cached**: Reserved but not actively used\n",
    "- **Free**: Available GPU memory\n",
    "> Run this cell before training to free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b11a824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n",
      "Free: 6.87 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check GPU memory status\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
    "print(f\"Free: {torch.cuda.mem_get_info(0)[0]/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b75dc38",
   "metadata": {},
   "source": [
    "## Model Training: RT-DETR for Plant Weed Detection\n",
    "Train the RT-DETR-L (Large) model on the dataset.\n",
    "**Training Configuration**:\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `data` | `model_dataset/data.yaml` | Dataset configuration file |\n",
    "| `epochs` | 100 | Number of training epochs |\n",
    "| `imgsz` | 640 | Input image size (640x640) |\n",
    "| `batch` | 4 | Batch size |\n",
    "| `device` | 0 | GPU device ID (0 = first GPU) |\n",
    "| `workers` | 1 | Number of dataloader workers |\n",
    "**Model**: RT-DETR-L (Large variant)\n",
    "- Pre-trained weights: `rtdetr-l.pt`\n",
    "- Architecture: Real-Time Detection Transformer\n",
    "- Optimized for real-time object detection\n",
    "> **Training Time**: Approximately 30-60 minutes depending on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import RTDETR\n",
    "# Load a model\n",
    "model = RTDETR(\"rtdetr-l.pt\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=r\"model_dataset\\data.yaml\",    # Path to your dataset YAML file\n",
    "    epochs=100,                        # Number of training epochs\n",
    "    imgsz=640,                         # Image size\n",
    "    batch=4,                          # Batch size\n",
    "    device=0,                          # GPU device (0 for first GPU, 'cpu' for CPU)\n",
    "    workers=1                          # Number of dataloader workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276bfcd",
   "metadata": {},
   "source": [
    "## Visualize Model Inference\n",
    "\n",
    "**What it does:**\n",
    "This function takes a video, finds weeds using your trained AI model, and highlights them with **red transparent boxes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d25b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_video(video_path, output_path=\"output.mp4\", conf=0.5):\n",
    "    \"\"\"\n",
    "    Visualizes RT-DETR detections on a video with a red semi-transparent overlay.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the input video.\n",
    "        output_path (str): Path to save the output video.\n",
    "        model (ultralytics.RTDETR): Loaded RT-DETR model.\n",
    "        conf (float): Confidence threshold for detections.\n",
    "    \"\"\"\n",
    "    model = RTDETR(r\"runs\\detect\\train\\weights\\best.pt\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing {video_path}...\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Run inference\n",
    "        # RT-DETR model call returns a list of Results objects\n",
    "        results = model(frame, verbose=False, conf=conf)\n",
    "        \n",
    "        # Create overlay\n",
    "        overlay = frame.copy()\n",
    "        \n",
    "        for result in results:\n",
    "            # result.boxes.xyxy is a Tensor, convert to numpy\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box[:4])\n",
    "                    # Draw red filled rectangle (BGR: 0, 0, 255)\n",
    "                    cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 0, 255), -1)\n",
    "                \n",
    "        # Apply transparency\n",
    "        alpha = 0.5\n",
    "        cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "        \n",
    "        out.write(frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 50 == 0:\n",
    "            print(f\"Processed {frame_count}/{total_frames} frames\")\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Processed video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77734986",
   "metadata": {},
   "source": [
    "## Run Inference on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = r\"plant_weed_video\\sample2.mp4\"\n",
    "\n",
    "run_inference_video(video, conf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e29c9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üë®‚Äçüíª About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## üßë‚Äçüî¨ Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## üé¶ Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) ‚Äì Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) ‚Äì Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) ‚Äì Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) ‚Äì Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> üëâ **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
