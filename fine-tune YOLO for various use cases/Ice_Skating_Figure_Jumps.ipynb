{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880b2bd2",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **Ice Skating Figure Jump Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog/<BLOG_NAME>)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c290fb4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project implements an end-to-end computer vision and kinematics pipeline for automated figure skating jump analysis. By integrating skeletal pose estimation with deep learning and physics-based angular tracking, the system provides objective metrics for athletic performance, specifically focusing on jump classification and rotation precision.\n",
    "\n",
    "#### Key Features:\n",
    "* **Automated Jump Classification:** Utilizes an LSTM-based deep learning model to recognize specific jump types (Axel, Salchow, Toe Loop, etc.) with high accuracy.\n",
    "* **Kinematic Rotation Counting:** A physics engine that tracks 3D hip displacement to count total body revolutions in real-time.\n",
    "* **Skeletal Pose Estimation:** Leverages MediaPipe to track 33 key body landmarks for bio-mechanical analysis.\n",
    "* **Offline Analytics:** Supports frame-by-frame video processing to generate annotated broadcast-style overlays.\n",
    "\n",
    "#### Real-World Applications:\n",
    "* **Olympic Broadcast Overlays:** Real-time data visualization for viewers and commentators during international competitions.\n",
    "* **Professional Coaching:** Providing athletes with precise data on air time, jump height, and rotation completeness.\n",
    "* **Objective Judging:** Assisting technical panels with verifiable metrics to evaluate \"under-rotated\" or \"downgraded\" jumps.\n",
    "* **Injury Prevention:** Monitoring skeletal alignment and landing impact forces through bio-mechanical tracking.\n",
    "* **Automated Scouting:** Processing large datasets of training footage to identify and track athlete progress over time.\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "The system operates through a multi-stage pipeline designed to handle the high-velocity movements inherent in figure skating:\n",
    "\n",
    "1. **Pose Extraction:** High-fidelity 3D landmark detection using MediaPipe Pose.\n",
    "2. **Feature Normalization:** Landmark coordinates are centered and scaled relative to the hip center to ensure the model remains invariant to camera distance and skater height.\n",
    "3. **Sequential Classification:** A 150-frame temporal buffer is passed to a Long Short-Term Memory (LSTM) network, which analyzes the motion patterns over time to classify the jump.\n",
    "4. **Angular Displacement Logic:** The physics engine calculates the arctangent of the hip landmarks across the X and Z (depth) axes to accumulate total degrees of rotation while the skater is in the air."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880edd7",
   "metadata": {},
   "source": [
    "## Dataset Preparation & Taxonomy Mapping\n",
    "\n",
    "The following script is used to organize raw video data from multiple camera angles into a structured dataset. It categorizes specific figure skating jumps into a four-class taxonomy optimized for our LSTM classification model.\n",
    "\n",
    "### 4-Class Taxonomy Mapping:\n",
    "| Class Name | Jump Types Included |\n",
    "| :--- | :--- |\n",
    "| **Class_0_Axel** | Axel |\n",
    "| **Class_1_Edge** | Salchow, Loop |\n",
    "| **Class_2_Complex_Pick** | Lutz, Flip |\n",
    "| **Class_3_Simple_Pick** | Toe Loop |\n",
    "\n",
    "### Technical Implementation:\n",
    "The script traverses the `BASE_PATH` (unzipped camera folders), identifies `.mp4` files, and uses string splitting to extract the jump identity. It then performs a lookup against the `TAXONOMY` dictionary and copies the files to the `OUTPUT_PATH` while prepending the camera ID to prevent filename collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71b76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully organized into 4 classes!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "# Path to your unzipped folder from the screenshot\n",
    "BASE_PATH = \"D:\\\\Downloads\\\\skater_A\\\\skater_A\"\n",
    "# New root for organized training data\n",
    "OUTPUT_PATH = \"D:\\\\Desktop\\\\Desk\\\\Labellerr Github Projects\\\\Use_Case_Projects\\\\olympics_cv_project\\\\organized_dataset\"\n",
    "\n",
    "# Define your 4-class taxonomy\n",
    "TAXONOMY = {\n",
    "    \"Class_0_Axel\": [\"axel\"],\n",
    "    \"Class_1_Edge\": [\"salchow\", \"loop\"],\n",
    "    \"Class_2_Complex_Pick\": [\"lutz\", \"flip\"],\n",
    "    \"Class_3_Simple_Pick\": [\"toeloop\", \"toe_loop\"] # Catches both variations\n",
    "}\n",
    "\n",
    "def organize_files():\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "    \n",
    "    for cam_folder in os.listdir(BASE_PATH):\n",
    "        cam_path = os.path.join(BASE_PATH, cam_folder)\n",
    "        \n",
    "        if os.path.isdir(cam_path) and cam_folder.startswith(\"cam\"):\n",
    "            for file_name in os.listdir(cam_path):\n",
    "                if file_name.endswith(\".mp4\"):\n",
    "                    # Lowercase the name to match taxonomy keys\n",
    "                    jump_id = file_name.split(\"_\")[0].lower()\n",
    "                    \n",
    "                    for class_name, keywords in TAXONOMY.items():\n",
    "                        if jump_id in keywords:\n",
    "                            dest_dir = os.path.join(OUTPUT_PATH, class_name)\n",
    "                            os.makedirs(dest_dir, exist_ok=True)\n",
    "                            \n",
    "                            # Prefix with cam folder to avoid filename conflicts\n",
    "                            new_name = f\"{cam_folder}_{file_name}\"\n",
    "                            shutil.copy(os.path.join(cam_path, file_name), os.path.join(dest_dir, new_name))\n",
    "    print(\"Files successfully organized into 4 classes!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    organize_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824df4b",
   "metadata": {},
   "source": [
    "## Diverse Frame Extraction for Validation\n",
    "\n",
    "The following utility script is used to generate a representative image dataset from the raw video footage. These frames are essential for verifying pose estimation accuracy and creating a small-scale image dataset for YOLOv11-pose fine-tuning.\n",
    "\n",
    "### Technical Implementation:\n",
    "The script crawls the `organized_dataset` directory and selects 50 unique videos. For each video, it identifies the total frame count and seeks to a random temporal position using `cv2.CAP_PROP_POS_FRAMES` to capture a diverse snapshot of the skater's movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf807340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720 video clips.\n",
      "Successfully extracted 50 unique frames to img_dataset\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Your organized dataset path\n",
    "DATASET_PATH = \"organized_dataset\"\n",
    "OUTPUT_IMAGES = \"img_dataset\"\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_IMAGES, exist_ok=True)\n",
    "\n",
    "def extract_diverse_frames(total_needed=50):\n",
    "    all_videos = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(DATASET_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp4\"):\n",
    "                all_videos.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(all_videos)} video clips.\")\n",
    "    \n",
    "    \n",
    "    selected_vids = random.sample(all_videos, min(total_needed, len(all_videos)))\n",
    "    \n",
    "    count = 0\n",
    "    for video_path in selected_vids:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames > 0:\n",
    "            \n",
    "            frame_index = random.randint(0, total_frames - 1)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "            \n",
    "            success, frame = cap.read()\n",
    "            if success:\n",
    "            \n",
    "                vid_name = os.path.basename(video_path).split('.')[0]\n",
    "                img_name = f\"skater_{vid_name}_f{frame_index}.jpg\"\n",
    "                cv2.imwrite(os.path.join(OUTPUT_IMAGES, img_name), frame)\n",
    "                count += 1\n",
    "                \n",
    "        cap.release()\n",
    "        if count >= total_needed:\n",
    "            break\n",
    "\n",
    "    print(f\"Successfully extracted {count} unique frames to {OUTPUT_IMAGES}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_diverse_frames(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c4c7a",
   "metadata": {},
   "source": [
    "## Annotate your Custom dataset using Labellerr\n",
    "\n",
    " ***1. Visit the [Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) website and click **“Sign Up”**.*** \n",
    "\n",
    " ***2. After signing in, create your workspace by entering a unique name.***\n",
    "\n",
    " ***3. Navigate to your workspace’s API keys page (e.g., `https://<your-workspace>.labellerr.com/workspace/api-keys`) to generate your **API Key** and **API Secret**.***\n",
    "\n",
    " ***4. Store the credentials securely, and then use them to initialise the SDK or API client with `api_key`, `api_secret`.*** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52762d27",
   "metadata": {},
   "source": [
    "## Download Annotations from Labellerr\n",
    "\n",
    "After completing data labeling on the **Labellerr** platform, export the annotations in **COCO JSON format**.\n",
    "\n",
    "Download the COCO JSON file from the Labellerr website and upload it into this project workspace to use it for further dataset preparation and training.\n",
    "\n",
    "This COCO JSON file will be used in the next steps for:\n",
    "- Frame–annotation alignment\n",
    "- COCO → YOLO format conversion\n",
    "- Model training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffce1fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolo_finetune_utils'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Labellerr/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea054c7",
   "metadata": {},
   "source": [
    "# COCO to YOLO Format Conversion\n",
    "\n",
    "Converts COCO-style segmentation annotations to YOLO segmentation dataset format.  \n",
    "- Requires: `annotation.json` and images in `frames_output` directory.\n",
    "- Output: Generated YOLO dataset folder.\n",
    "- Parameters: allows train/val split, shuffling, and verbose mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e09f8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO dataset from export-#KfOtkMikXutSyD5AlN0q.json\n",
      "Found 50 images and 46 annotations\n",
      "Categories mapping:\n",
      "  COCO ID 0 (Skater) -> YOLO class 0\n",
      "Images with annotations: 46\n",
      "Dataset split:\n",
      "  train: 32 images\n",
      "  val: 9 images\n",
      "  test: 5 images\n",
      "\n",
      "Processing train split...\n",
      "\n",
      "Processing val split...\n",
      "\n",
      "Processing test split...\n",
      "\n",
      "Conversion completed:\n",
      "  Successfully processed: 46 images\n",
      "  Failed to find: 0 images\n",
      "  Total annotations converted: 46\n",
      "  Categories: 1\n",
      "\n",
      "YOLO dataset created at: yolo_img_dataset\n",
      "Dataset configuration: yolo_img_dataset\\dataset.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_path': 'yolo_img_dataset',\n",
       " 'yaml_path': 'yolo_img_dataset\\\\dataset.yaml',\n",
       " 'stats': {'total_images': 50,\n",
       "  'images_with_annotations': 46,\n",
       "  'successful_copies': 46,\n",
       "  'failed_copies': 0,\n",
       "  'total_annotations': 46,\n",
       "  'categories': 1,\n",
       "  'category_mapping': {0: 0},\n",
       "  'class_names': {0: 'Skater'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yolo_finetune_utils.coco_yolo_converter.bbox_converter import coco_to_yolo_converter\n",
    "\n",
    "coco_to_yolo_converter(\n",
    "    json_path=\"export-#KfOtkMikXutSyD5AlN0q.json\",\n",
    "    images_dir=\"img_dataset\",\n",
    "    output_dir=\"yolo_img_dataset\",\n",
    "    use_split=True,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaeb497",
   "metadata": {},
   "source": [
    "## YOLO11 Model Fine-Tuning\n",
    "\n",
    "The following code block handles the training phase for the skater detection model. This stage is critical for ensuring the AI can reliably locate the athlete before the LSTM analyzes their specific movements.\n",
    "\n",
    "### Training Parameters:\n",
    "| Parameter | Value | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **Model** | YOLO11 Nano | Optimized for low-latency performance. |\n",
    "| **Epochs** | 100 | Full passes through the training dataset. |\n",
    "| **Img Size** | 640 | Resolution for input image processing. |\n",
    "| **Batch** | 16 | Number of images processed before the model updates. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e19f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.9  Python-3.13.1 torch-2.10.0+cpu CPU (12th Gen Intel Core(TM) i5-1235U)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=yolo_img_dataset/dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=skater_detector_v1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=olympics_cv, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\olympics_cv\\skater_detector_v1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, 16, None, [64, 128, 256]] \n",
      "YOLO11n summary: 182 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 18.51.0 MB/s, size: 300.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\yolo_img_dataset\\train\\labels... 32 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 32/32 115.3it/s 0.3s.2s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\yolo_img_dataset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 29.43.4 MB/s, size: 287.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\yolo_img_dataset\\val\\labels... 9 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 9/9 192.2it/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\yolo_img_dataset\\val\\labels.cache\n",
      "Plotting labels to D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\olympics_cv\\skater_detector_v1\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mD:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\olympics_cv\\skater_detector_v1\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100         0G      1.327      3.377      1.125         35        640: 100% ━━━━━━━━━━━━ 2/2 11.2s/it 22.4s44.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.9s/it 1.9s\n",
      "                   all          9          9    0.00296      0.889      0.368      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100         0G      1.204      3.345      1.042         36        640: 100% ━━━━━━━━━━━━ 2/2 10.0s/it 20.1s34.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9    0.00333          1       0.48       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100         0G      1.085      3.204      1.055         36        640: 100% ━━━━━━━━━━━━ 2/2 9.4s/it 18.9s<30.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.7s/it 1.7s\n",
      "                   all          9          9    0.00333          1      0.442      0.322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100         0G     0.8487      2.849     0.9062         30        640: 100% ━━━━━━━━━━━━ 2/2 9.4s/it 18.8s<30.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9    0.00333          1       0.53      0.375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/100         0G      0.945      2.728     0.9355         31        640: 100% ━━━━━━━━━━━━ 2/2 9.3s/it 18.6s<32.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00333          1      0.681      0.414\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/100         0G     0.9978      2.389     0.9102         28        640: 100% ━━━━━━━━━━━━ 2/2 9.2s/it 18.5s<32.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9    0.00333          1       0.75      0.454\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/100         0G      0.955      1.976     0.9616         30        640: 100% ━━━━━━━━━━━━ 2/2 8.9s/it 17.8s<30.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00333          1      0.772      0.483\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/100         0G     0.8539      1.978     0.9483         22        640: 100% ━━━━━━━━━━━━ 2/2 8.2s/it 16.4s<27.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00333          1      0.712      0.467\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/100         0G     0.9579      1.623     0.9775         33        640: 100% ━━━━━━━━━━━━ 2/2 8.4s/it 16.8s<27.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00334          1      0.696      0.451\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/100         0G      1.008      1.758      1.002         36        640: 100% ━━━━━━━━━━━━ 2/2 8.2s/it 16.5s<27.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00335          1      0.699      0.438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/100         0G     0.9732      1.702     0.9754         27        640: 100% ━━━━━━━━━━━━ 2/2 8.1s/it 16.2s<26.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00301      0.889      0.679      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/100         0G     0.8393      1.468     0.9158         31        640: 100% ━━━━━━━━━━━━ 2/2 8.4s/it 16.7s<28.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00301      0.889      0.529      0.383\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/100         0G     0.7728      1.509      0.927         30        640: 100% ━━━━━━━━━━━━ 2/2 10.2s/it 20.5s35.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.7s/it 1.7s\n",
      "                   all          9          9    0.00303      0.889       0.49      0.292\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/100         0G     0.8595      1.466      0.904         27        640: 100% ━━━━━━━━━━━━ 2/2 9.3s/it 18.6s<29.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.0s/it 2.0s\n",
      "                   all          9          9    0.00298      0.889      0.357      0.254\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/100         0G      0.851      1.408     0.9173         44        640: 100% ━━━━━━━━━━━━ 2/2 11.2s/it 22.4s42.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.2s/it 2.2s\n",
      "                   all          9          9    0.00296      0.889      0.327      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/100         0G     0.8773      1.429     0.9607         36        640: 100% ━━━━━━━━━━━━ 2/2 10.1s/it 20.1s31.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00963      0.615      0.302      0.222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/100         0G     0.7495      1.343     0.8882         26        640: 100% ━━━━━━━━━━━━ 2/2 8.9s/it 17.8s<30.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9    0.00752      0.474      0.265      0.212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/100         0G     0.8564      1.356     0.9443         36        640: 100% ━━━━━━━━━━━━ 2/2 9.1s/it 18.1s<28.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9     0.0117      0.726      0.268      0.227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/100         0G     0.8931      1.293     0.9567         33        640: 100% ━━━━━━━━━━━━ 2/2 8.2s/it 16.5s<27.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00333          1      0.228      0.188\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/100         0G     0.7817      1.163     0.8877         38        640: 100% ━━━━━━━━━━━━ 2/2 8.0s/it 15.9s<27.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9    0.00333          1      0.248      0.177\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/100         0G     0.8308       1.36     0.9699         32        640: 100% ━━━━━━━━━━━━ 2/2 8.0s/it 16.0s<27.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9    0.00333          1      0.239      0.162\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     22/100         0G     0.9433      1.481      1.017         34        640: 100% ━━━━━━━━━━━━ 2/2 8.3s/it 16.6s<27.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9    0.00333          1       0.27      0.208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     23/100         0G     0.8041      1.363     0.9209         32        640: 100% ━━━━━━━━━━━━ 2/2 8.1s/it 16.2s<26.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00333          1      0.245      0.181\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     24/100         0G     0.7588      1.404     0.9175         27        640: 100% ━━━━━━━━━━━━ 2/2 8.4s/it 16.8s<27.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00333          1      0.243      0.149\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     25/100         0G     0.8208      1.287     0.9005         32        640: 100% ━━━━━━━━━━━━ 2/2 8.4s/it 16.9s<28.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.5s/it 1.5s\n",
      "                   all          9          9    0.00333          1      0.293      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     26/100         0G     0.7687      1.184     0.9595         29        640: 100% ━━━━━━━━━━━━ 2/2 8.9s/it 17.8s<30.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9    0.00333          1      0.456      0.299\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     27/100         0G     0.6884      1.168     0.9293         26        640: 100% ━━━━━━━━━━━━ 2/2 8.1s/it 16.2s<27.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.7s/it 1.7s\n",
      "                   all          9          9    0.00333          1      0.754      0.436\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     28/100         0G     0.7803      1.176     0.9849         35        640: 100% ━━━━━━━━━━━━ 2/2 8.0s/it 16.0s<26.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00333          1      0.754      0.436\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     29/100         0G     0.8037      1.263     0.9367         30        640: 100% ━━━━━━━━━━━━ 2/2 8.2s/it 16.5s<27.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9    0.00942      0.889      0.809      0.538\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     30/100         0G      0.823      1.238     0.9996         30        640: 100% ━━━━━━━━━━━━ 2/2 8.0s/it 16.0s<27.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9      0.757      0.349      0.825       0.63\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     31/100         0G     0.8046      1.172     0.9928         39        640: 100% ━━━━━━━━━━━━ 2/2 8.1s/it 16.2s<27.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9      0.757      0.349      0.825       0.63\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     32/100         0G     0.8023      1.245     0.9355         28        640: 100% ━━━━━━━━━━━━ 2/2 8.4s/it 16.8s<29.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9          1      0.316      0.886      0.625\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     33/100         0G     0.8255      1.161     0.9514         35        640: 100% ━━━━━━━━━━━━ 2/2 8.7s/it 17.4s<30.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9          1      0.554      0.886      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     34/100         0G     0.8034      1.146     0.9727         31        640: 100% ━━━━━━━━━━━━ 2/2 11.4s/it 22.9s39.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.7s/it 1.7s\n",
      "                   all          9          9          1      0.554      0.886      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     35/100         0G     0.8013      1.134     0.9553         40        640: 100% ━━━━━━━━━━━━ 2/2 11.6s/it 23.3s41.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.0s/it 2.0s\n",
      "                   all          9          9          1      0.547      0.886      0.592\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     36/100         0G      0.787      1.351     0.9552         18        640: 100% ━━━━━━━━━━━━ 2/2 13.5s/it 27.0s48.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.9s/it 2.9s\n",
      "                   all          9          9          1      0.487      0.886      0.574\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     37/100         0G     0.7865      1.146      0.967         32        640: 100% ━━━━━━━━━━━━ 2/2 14.7s/it 29.3s41.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.1s/it 2.1s\n",
      "                   all          9          9          1      0.487      0.886      0.574\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     38/100         0G     0.7332      1.332     0.9448         18        640: 100% ━━━━━━━━━━━━ 2/2 11.5s/it 23.0s38.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.9s/it 1.9s\n",
      "                   all          9          9          1      0.582      0.886      0.547\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     39/100         0G     0.9504      1.254      1.083         25        640: 100% ━━━━━━━━━━━━ 2/2 11.1s/it 22.3s39.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.8s/it 1.8s\n",
      "                   all          9          9          1       0.57      0.886      0.525\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     40/100         0G     0.8679      1.151     0.9354         33        640: 100% ━━━━━━━━━━━━ 2/2 12.1s/it 24.1s37.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.0s/it 2.0s\n",
      "                   all          9          9          1       0.57      0.886      0.525\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     41/100         0G      0.835      1.279     0.9888         27        640: 100% ━━━━━━━━━━━━ 2/2 11.8s/it 23.6s41.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.2s/it 2.2s\n",
      "                   all          9          9          1      0.549      0.886       0.61\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     42/100         0G        0.7      1.062     0.9117         41        640: 100% ━━━━━━━━━━━━ 2/2 12.4s/it 24.7s43.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.9s/it 1.9s\n",
      "                   all          9          9          1      0.715      0.886      0.631\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     43/100         0G     0.8225      1.126     0.9902         28        640: 100% ━━━━━━━━━━━━ 2/2 13.1s/it 26.3s47.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.7s/it 2.7s\n",
      "                   all          9          9          1      0.715      0.886      0.631\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     44/100         0G     0.7086      1.029     0.9202         34        640: 100% ━━━━━━━━━━━━ 2/2 14.2s/it 28.4s48.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.2s/it 3.2s\n",
      "                   all          9          9          1      0.846      0.886      0.616\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     45/100         0G     0.7515      1.016       1.07         32        640: 100% ━━━━━━━━━━━━ 2/2 14.7s/it 29.3s47.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.2s/it 2.2s\n",
      "                   all          9          9          1      0.846      0.886      0.616\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     46/100         0G      0.741      1.112     0.9743         28        640: 100% ━━━━━━━━━━━━ 2/2 12.0s/it 24.0s43.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.0s/it 2.0s\n",
      "                   all          9          9          1      0.872      0.886      0.649\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     47/100         0G     0.7974      1.228     0.9459         22        640: 100% ━━━━━━━━━━━━ 2/2 12.8s/it 25.5s41.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9          1      0.872      0.886      0.649\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     48/100         0G     0.7892      1.181     0.9357         30        640: 100% ━━━━━━━━━━━━ 2/2 8.5s/it 16.9s<27.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9          1       0.87      0.886      0.643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     49/100         0G     0.7377      1.082     0.9754         31        640: 100% ━━━━━━━━━━━━ 2/2 9.3s/it 18.7s<31.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9          1       0.87      0.886      0.643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     50/100         0G     0.7464      1.063     0.8766         26        640: 100% ━━━━━━━━━━━━ 2/2 11.0s/it 22.0s43.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.1s/it 2.1s\n",
      "                   all          9          9          1      0.871      0.891      0.601\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     51/100         0G     0.7736      1.237     0.8996         23        640: 100% ━━━━━━━━━━━━ 2/2 12.0s/it 24.0s47.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.9s/it 2.9s\n",
      "                   all          9          9          1      0.871      0.891      0.601\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     52/100         0G      0.786      1.067     0.9389         30        640: 100% ━━━━━━━━━━━━ 2/2 14.7s/it 29.5s43.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.9s/it 1.9s\n",
      "                   all          9          9          1      0.871      0.968      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     53/100         0G     0.6711       1.06     0.8891         32        640: 100% ━━━━━━━━━━━━ 2/2 9.9s/it 19.9s<32.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.2s/it 2.2s\n",
      "                   all          9          9          1      0.871      0.968      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     54/100         0G      0.837       1.16     0.9775         27        640: 100% ━━━━━━━━━━━━ 2/2 10.2s/it 20.3s36.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9          1      0.903      0.995        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     55/100         0G     0.7404     0.9817     0.9089         36        640: 100% ━━━━━━━━━━━━ 2/2 9.7s/it 19.4s<34.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.6s/it 1.6s\n",
      "                   all          9          9          1      0.903      0.995        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     56/100         0G     0.7259      1.018     0.9218         34        640: 100% ━━━━━━━━━━━━ 2/2 10.8s/it 21.5s38.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.5s/it 1.5s\n",
      "                   all          9          9      0.889      0.893      0.984      0.684\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     57/100         0G     0.6909     0.9942     0.9099         32        640: 100% ━━━━━━━━━━━━ 2/2 10.2s/it 20.5s37.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.6s/it 1.6s\n",
      "                   all          9          9      0.889      0.893      0.984      0.684\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     58/100         0G     0.7126     0.9512      0.883         27        640: 100% ━━━━━━━━━━━━ 2/2 9.7s/it 19.3s<31.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.6s/it 1.6s\n",
      "                   all          9          9          1      0.866      0.975      0.619\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     59/100         0G     0.6773     0.9676     0.8813         36        640: 100% ━━━━━━━━━━━━ 2/2 10.0s/it 20.0s28.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9          1      0.866      0.975      0.619\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     60/100         0G     0.7273      1.021     0.9201         23        640: 100% ━━━━━━━━━━━━ 2/2 8.2s/it 16.4s<28.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9       0.88      0.889       0.91      0.612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     61/100         0G     0.7368      1.015     0.9275         27        640: 100% ━━━━━━━━━━━━ 2/2 8.1s/it 16.2s<28.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9       0.88      0.889       0.91      0.612\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     62/100         0G     0.6588     0.8888     0.8891         41        640: 100% ━━━━━━━━━━━━ 2/2 9.8s/it 19.6s<38.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.8s/it 2.8s\n",
      "                   all          9          9      0.983      0.889      0.947      0.641\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     63/100         0G     0.6622     0.9169      0.893         29        640: 100% ━━━━━━━━━━━━ 2/2 11.9s/it 23.8s30.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.6s/it 2.6s\n",
      "                   all          9          9      0.983      0.889      0.947      0.641\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     64/100         0G     0.6783      0.949      0.906         26        640: 100% ━━━━━━━━━━━━ 2/2 12.5s/it 25.1s45.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.9s/it 2.9s\n",
      "                   all          9          9      0.869      0.889      0.931      0.715\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     65/100         0G     0.7573     0.9695     0.9625         28        640: 100% ━━━━━━━━━━━━ 2/2 16.9s/it 33.7s56.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.1s/it 3.1s\n",
      "                   all          9          9      0.869      0.889      0.931      0.715\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     66/100         0G      0.637     0.9667     0.9108         26        640: 100% ━━━━━━━━━━━━ 2/2 13.3s/it 26.7s36.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.8s/it 1.8s\n",
      "                   all          9          9      0.883      0.889      0.968      0.709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     67/100         0G     0.5782     0.8776     0.8746         36        640: 100% ━━━━━━━━━━━━ 2/2 11.4s/it 22.7s38.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.8s/it 1.8s\n",
      "                   all          9          9      0.883      0.889      0.968      0.709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     68/100         0G     0.6214     0.8962     0.8888         35        640: 100% ━━━━━━━━━━━━ 2/2 12.5s/it 25.1s45.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.9s/it 2.9s\n",
      "                   all          9          9      0.981      0.889      0.984      0.682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     69/100         0G     0.5941     0.8599     0.8847         32        640: 100% ━━━━━━━━━━━━ 2/2 10.8s/it 21.6s35.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.0s/it 2.0s\n",
      "                   all          9          9      0.981      0.889      0.984      0.682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     70/100         0G     0.6449     0.9186     0.9132         33        640: 100% ━━━━━━━━━━━━ 2/2 11.0s/it 22.1s37.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.7s/it 1.7s\n",
      "                   all          9          9          1       0.98      0.995      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     71/100         0G     0.6923     0.8561     0.8955         37        640: 100% ━━━━━━━━━━━━ 2/2 12.8s/it 25.5s50.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.8s/it 3.8s\n",
      "                   all          9          9          1       0.98      0.995      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     72/100         0G     0.6746     0.9578     0.9417         33        640: 100% ━━━━━━━━━━━━ 2/2 22.6s/it 45.3s1:10\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.4s/it 3.4s\n",
      "                   all          9          9          1      0.978      0.995      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     73/100         0G     0.6487     0.9601     0.9056         26        640: 100% ━━━━━━━━━━━━ 2/2 19.6s/it 39.3s1:05\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.3s/it 3.3s\n",
      "                   all          9          9          1      0.978      0.995      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     74/100         0G      0.609      0.941      0.901         33        640: 100% ━━━━━━━━━━━━ 2/2 19.9s/it 39.9s1:17\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.0s/it 2.0s\n",
      "                   all          9          9      0.992      0.889      0.984      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     75/100         0G     0.6901     0.8967     0.9042         27        640: 100% ━━━━━━━━━━━━ 2/2 10.3s/it 20.7s33.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.8s/it 1.8s\n",
      "                   all          9          9      0.992      0.889      0.984      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     76/100         0G      0.629     0.8586      0.885         30        640: 100% ━━━━━━━━━━━━ 2/2 10.2s/it 20.4s33.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.0s/it 2.0s\n",
      "                   all          9          9      0.991      0.889      0.975      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     77/100         0G     0.5897     0.8009     0.8575         28        640: 100% ━━━━━━━━━━━━ 2/2 10.5s/it 20.9s34.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.8s/it 1.8s\n",
      "                   all          9          9      0.991      0.889      0.975      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     78/100         0G     0.5918     0.8815     0.8841         28        640: 100% ━━━━━━━━━━━━ 2/2 16.4s/it 32.8s59.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.0s/it 3.0s\n",
      "                   all          9          9       0.99      0.889      0.968      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     79/100         0G      0.715      1.068     0.9394         35        640: 100% ━━━━━━━━━━━━ 2/2 13.0s/it 26.0s38.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.7s/it 1.7s\n",
      "                   all          9          9       0.99      0.889      0.968      0.717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     80/100         0G     0.6041     0.8603     0.8958         24        640: 100% ━━━━━━━━━━━━ 2/2 10.5s/it 21.0s34.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.7s/it 1.7s\n",
      "                   all          9          9      0.989      0.889      0.975       0.75\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     81/100         0G     0.5579     0.8304     0.8616         36        640: 100% ━━━━━━━━━━━━ 2/2 10.3s/it 20.5s33.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.7s/it 2.7s\n",
      "                   all          9          9      0.989      0.889      0.975       0.75\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     82/100         0G     0.5582     0.8059     0.8592         32        640: 100% ━━━━━━━━━━━━ 2/2 10.9s/it 21.9s36.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.1s/it 2.1s\n",
      "                   all          9          9       0.99      0.889      0.984      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     83/100         0G     0.5592     0.7908     0.8611         34        640: 100% ━━━━━━━━━━━━ 2/2 9.0s/it 17.9s<28.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9       0.99      0.889      0.984      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     84/100         0G     0.5609     0.8289     0.8942         28        640: 100% ━━━━━━━━━━━━ 2/2 8.4s/it 16.8s<27.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.1s/it 1.1s\n",
      "                   all          9          9          1      0.991      0.995      0.785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     85/100         0G     0.6234     0.8591     0.9144         36        640: 100% ━━━━━━━━━━━━ 2/2 8.7s/it 17.5s<30.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9          1      0.991      0.995      0.785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     86/100         0G        0.6     0.8046     0.8658         31        640: 100% ━━━━━━━━━━━━ 2/2 9.6s/it 19.2s<29.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9          1      0.997      0.995      0.781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     87/100         0G     0.5453     0.8188     0.9115         23        640: 100% ━━━━━━━━━━━━ 2/2 17.0s/it 34.0s1:00\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.1s/it 3.1s\n",
      "                   all          9          9          1      0.997      0.995      0.781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     88/100         0G      0.608      0.786     0.9326         38        640: 100% ━━━━━━━━━━━━ 2/2 16.3s/it 32.7s54.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.9s/it 2.9s\n",
      "                   all          9          9      0.997          1      0.995      0.766\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     89/100         0G     0.6376     0.8287     0.9602         29        640: 100% ━━━━━━━━━━━━ 2/2 16.2s/it 32.5s55.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.9s/it 2.9s\n",
      "                   all          9          9      0.997          1      0.995      0.766\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     90/100         0G     0.5953     0.8221     0.9197         29        640: 100% ━━━━━━━━━━━━ 2/2 16.3s/it 32.7s54.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.8s/it 2.8s\n",
      "                   all          9          9      0.994          1      0.995      0.772\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     91/100         0G     0.4751      1.069     0.8568         16        640: 100% ━━━━━━━━━━━━ 2/2 15.7s/it 31.5s51.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9      0.994          1      0.995      0.772\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     92/100         0G     0.4867      1.131      0.853         16        640: 100% ━━━━━━━━━━━━ 2/2 8.3s/it 16.6s<28.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9      0.994          1      0.995      0.762\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     93/100         0G      0.477     0.9932     0.8649         16        640: 100% ━━━━━━━━━━━━ 2/2 7.7s/it 15.4s<25.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9      0.994          1      0.995      0.762\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     94/100         0G     0.4919     0.9774     0.8214         15        640: 100% ━━━━━━━━━━━━ 2/2 7.7s/it 15.4s<26.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.1s/it 1.1s\n",
      "                   all          9          9      0.993          1      0.995      0.768\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     95/100         0G     0.4709      1.009     0.8045         15        640: 100% ━━━━━━━━━━━━ 2/2 7.5s/it 15.1s<25.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9      0.993          1      0.995      0.768\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     96/100         0G     0.5076      1.052     0.8284         16        640: 100% ━━━━━━━━━━━━ 2/2 7.5s/it 15.0s<25.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.4s/it 1.4s\n",
      "                   all          9          9      0.993          1      0.995      0.768\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     97/100         0G      0.487      1.124     0.8205         16        640: 100% ━━━━━━━━━━━━ 2/2 7.8s/it 15.6s<26.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9      0.993          1      0.995      0.768\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     98/100         0G     0.4415      1.055     0.8241         16        640: 100% ━━━━━━━━━━━━ 2/2 9.1s/it 18.2s<30.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.3s/it 1.3s\n",
      "                   all          9          9      0.993          1      0.995      0.786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     99/100         0G     0.4759      1.051     0.8208         16        640: 100% ━━━━━━━━━━━━ 2/2 8.4s/it 16.8s<25.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 1.2s/it 1.2s\n",
      "                   all          9          9      0.993          1      0.995      0.786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K    100/100         0G     0.4518      1.019     0.8223         16        640: 100% ━━━━━━━━━━━━ 2/2 7.7s/it 15.4s<26.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 3.0s/it 3.0s\n",
      "                   all          9          9      0.994          1      0.995      0.786\n",
      "\n",
      "100 epochs completed in 0.671 hours.\n",
      "Optimizer stripped from D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\olympics_cv\\skater_detector_v1\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\olympics_cv\\skater_detector_v1\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\olympics_cv\\skater_detector_v1\\weights\\best.pt...\n",
      "Ultralytics 8.4.9  Python-3.13.1 torch-2.10.0+cpu CPU (12th Gen Intel Core(TM) i5-1235U)\n",
      "YOLO11n summary (fused): 101 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 2.3s/it 2.3s\n",
      "                   all          9          9      0.994          1      0.995      0.786\n",
      "Speed: 2.1ms preprocess, 192.4ms inference, 0.0ms loss, 11.3ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\olympics_cv\\skater_detector_v1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Load the pre-trained YOLO11 Nano model (best for small datasets)\n",
    "model = YOLO(\"yolo11n.pt\") \n",
    "\n",
    "# 2. Train the model\n",
    "results = model.train(\n",
    "    data=\"yolo_img_dataset/dataset.yaml\",\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name=\"skater_detector_v1\",\n",
    "    project=\"olympics_cv\",\n",
    "    device=\"cpu\"  # Change this from 0 to 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3723cf",
   "metadata": {},
   "source": [
    "## Skater Tracking & Inference\n",
    "\n",
    "After training, the model is deployed to track athletes in raw video footage. This tracking step is the bridge between static object detection and temporal motion analysis.\n",
    "\n",
    "### Tracking Parameters:\n",
    "| Configuration | Value | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **Tracker** | ByteTrack | Optimized for high-velocity motion and occlusion handling. |\n",
    "| **Persist** | True | Maintains Skater ID consistency across the entire video clip. |\n",
    "| **Confidence** | 0.4 | Minimum probability required to consider a detection valid. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f01557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING \n",
      "Inference results will accumulate in RAM unless `stream=True` is passed, which can cause out-of-memory errors for large\n",
      "sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (frame 1/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 104.6ms\n",
      "video 1/1 (frame 2/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 78.8ms\n",
      "video 1/1 (frame 3/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 75.0ms\n",
      "video 1/1 (frame 4/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 74.4ms\n",
      "video 1/1 (frame 5/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 75.0ms\n",
      "video 1/1 (frame 6/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 68.8ms\n",
      "video 1/1 (frame 7/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 83.5ms\n",
      "video 1/1 (frame 8/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 78.8ms\n",
      "video 1/1 (frame 9/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 163.3ms\n",
      "video 1/1 (frame 10/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 89.6ms\n",
      "video 1/1 (frame 11/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 79.6ms\n",
      "video 1/1 (frame 12/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 75.6ms\n",
      "video 1/1 (frame 13/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 81.3ms\n",
      "video 1/1 (frame 14/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 94.3ms\n",
      "video 1/1 (frame 15/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 87.7ms\n",
      "video 1/1 (frame 16/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 79.5ms\n",
      "video 1/1 (frame 17/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 98.7ms\n",
      "video 1/1 (frame 18/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 85.9ms\n",
      "video 1/1 (frame 19/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 86.6ms\n",
      "video 1/1 (frame 20/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 84.3ms\n",
      "video 1/1 (frame 21/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 86.1ms\n",
      "video 1/1 (frame 22/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 100.2ms\n",
      "video 1/1 (frame 23/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 78.7ms\n",
      "video 1/1 (frame 24/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 64.6ms\n",
      "video 1/1 (frame 25/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 65.9ms\n",
      "video 1/1 (frame 26/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 63.3ms\n",
      "video 1/1 (frame 27/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 64.1ms\n",
      "video 1/1 (frame 28/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 60.5ms\n",
      "video 1/1 (frame 29/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 65.3ms\n",
      "video 1/1 (frame 30/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 69.7ms\n",
      "video 1/1 (frame 31/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 59.6ms\n",
      "video 1/1 (frame 32/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 68.7ms\n",
      "video 1/1 (frame 33/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 63.9ms\n",
      "video 1/1 (frame 34/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 76.0ms\n",
      "video 1/1 (frame 35/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 64.7ms\n",
      "video 1/1 (frame 36/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 59.9ms\n",
      "video 1/1 (frame 37/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.5ms\n",
      "video 1/1 (frame 38/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 61.2ms\n",
      "video 1/1 (frame 39/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 62.0ms\n",
      "video 1/1 (frame 40/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.0ms\n",
      "video 1/1 (frame 41/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 59.3ms\n",
      "video 1/1 (frame 42/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 70.9ms\n",
      "video 1/1 (frame 43/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 63.8ms\n",
      "video 1/1 (frame 44/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 66.7ms\n",
      "video 1/1 (frame 45/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 64.6ms\n",
      "video 1/1 (frame 46/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 59.8ms\n",
      "video 1/1 (frame 47/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.5ms\n",
      "video 1/1 (frame 48/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.0ms\n",
      "video 1/1 (frame 49/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 72.4ms\n",
      "video 1/1 (frame 50/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 69.2ms\n",
      "video 1/1 (frame 51/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 77.7ms\n",
      "video 1/1 (frame 52/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 75.3ms\n",
      "video 1/1 (frame 53/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 72.9ms\n",
      "video 1/1 (frame 54/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 68.3ms\n",
      "video 1/1 (frame 55/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 66.3ms\n",
      "video 1/1 (frame 56/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 72.1ms\n",
      "video 1/1 (frame 57/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 69.9ms\n",
      "video 1/1 (frame 58/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 68.3ms\n",
      "video 1/1 (frame 59/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 68.7ms\n",
      "video 1/1 (frame 60/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.3ms\n",
      "video 1/1 (frame 61/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 68.1ms\n",
      "video 1/1 (frame 62/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 69.1ms\n",
      "video 1/1 (frame 63/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 75.4ms\n",
      "video 1/1 (frame 64/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 71.2ms\n",
      "video 1/1 (frame 65/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 69.8ms\n",
      "video 1/1 (frame 66/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 68.4ms\n",
      "video 1/1 (frame 67/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 70.3ms\n",
      "video 1/1 (frame 68/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 68.3ms\n",
      "video 1/1 (frame 69/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 60.5ms\n",
      "video 1/1 (frame 70/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 72.2ms\n",
      "video 1/1 (frame 71/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 66.7ms\n",
      "video 1/1 (frame 72/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 82.2ms\n",
      "video 1/1 (frame 73/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.1ms\n",
      "video 1/1 (frame 74/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 66.0ms\n",
      "video 1/1 (frame 75/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.4ms\n",
      "video 1/1 (frame 76/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 74.3ms\n",
      "video 1/1 (frame 77/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 82.1ms\n",
      "video 1/1 (frame 78/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 107.7ms\n",
      "video 1/1 (frame 79/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 126.3ms\n",
      "video 1/1 (frame 80/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 110.1ms\n",
      "video 1/1 (frame 81/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 86.9ms\n",
      "video 1/1 (frame 82/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 73.0ms\n",
      "video 1/1 (frame 83/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 83.2ms\n",
      "video 1/1 (frame 84/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.3ms\n",
      "video 1/1 (frame 85/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 58.4ms\n",
      "video 1/1 (frame 86/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 65.1ms\n",
      "video 1/1 (frame 87/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 71.9ms\n",
      "video 1/1 (frame 88/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 67.8ms\n",
      "video 1/1 (frame 89/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 69.0ms\n",
      "video 1/1 (frame 90/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 (no detections), 66.2ms\n",
      "video 1/1 (frame 91/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 71.0ms\n",
      "video 1/1 (frame 92/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 70.5ms\n",
      "video 1/1 (frame 93/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 78.5ms\n",
      "video 1/1 (frame 94/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 75.0ms\n",
      "video 1/1 (frame 95/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 65.6ms\n",
      "video 1/1 (frame 96/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 68.5ms\n",
      "video 1/1 (frame 97/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 70.1ms\n",
      "video 1/1 (frame 98/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 61.6ms\n",
      "video 1/1 (frame 99/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 60.7ms\n",
      "video 1/1 (frame 100/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 68.7ms\n",
      "video 1/1 (frame 101/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 85.6ms\n",
      "video 1/1 (frame 102/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 121.0ms\n",
      "video 1/1 (frame 103/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 140.2ms\n",
      "video 1/1 (frame 104/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 124.4ms\n",
      "video 1/1 (frame 105/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.9ms\n",
      "video 1/1 (frame 106/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.7ms\n",
      "video 1/1 (frame 107/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 126.9ms\n",
      "video 1/1 (frame 108/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.4ms\n",
      "video 1/1 (frame 109/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 113.9ms\n",
      "video 1/1 (frame 110/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 105.0ms\n",
      "video 1/1 (frame 111/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.7ms\n",
      "video 1/1 (frame 112/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.1ms\n",
      "video 1/1 (frame 113/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 110.5ms\n",
      "video 1/1 (frame 114/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 209.9ms\n",
      "video 1/1 (frame 115/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 120.0ms\n",
      "video 1/1 (frame 116/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 104.2ms\n",
      "video 1/1 (frame 117/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 105.9ms\n",
      "video 1/1 (frame 118/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 101.1ms\n",
      "video 1/1 (frame 119/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 104.9ms\n",
      "video 1/1 (frame 120/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.1ms\n",
      "video 1/1 (frame 121/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 114.4ms\n",
      "video 1/1 (frame 122/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.8ms\n",
      "video 1/1 (frame 123/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.1ms\n",
      "video 1/1 (frame 124/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.1ms\n",
      "video 1/1 (frame 125/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 105.3ms\n",
      "video 1/1 (frame 126/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.3ms\n",
      "video 1/1 (frame 127/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 115.1ms\n",
      "video 1/1 (frame 128/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 101.1ms\n",
      "video 1/1 (frame 129/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 122.6ms\n",
      "video 1/1 (frame 130/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 120.3ms\n",
      "video 1/1 (frame 131/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 104.6ms\n",
      "video 1/1 (frame 132/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 108.5ms\n",
      "video 1/1 (frame 133/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 112.2ms\n",
      "video 1/1 (frame 134/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 144.0ms\n",
      "video 1/1 (frame 135/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 131.1ms\n",
      "video 1/1 (frame 136/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 125.3ms\n",
      "video 1/1 (frame 137/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.3ms\n",
      "video 1/1 (frame 138/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 104.2ms\n",
      "video 1/1 (frame 139/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 108.5ms\n",
      "video 1/1 (frame 140/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.9ms\n",
      "video 1/1 (frame 141/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 101.2ms\n",
      "video 1/1 (frame 142/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 101.7ms\n",
      "video 1/1 (frame 143/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.3ms\n",
      "video 1/1 (frame 144/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 119.7ms\n",
      "video 1/1 (frame 145/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 111.3ms\n",
      "video 1/1 (frame 146/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 108.1ms\n",
      "video 1/1 (frame 147/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 105.2ms\n",
      "video 1/1 (frame 148/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 96.3ms\n",
      "video 1/1 (frame 149/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 97.4ms\n",
      "video 1/1 (frame 150/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.4ms\n",
      "video 1/1 (frame 151/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.7ms\n",
      "video 1/1 (frame 152/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.2ms\n",
      "video 1/1 (frame 153/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 93.1ms\n",
      "video 1/1 (frame 154/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.6ms\n",
      "video 1/1 (frame 155/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.0ms\n",
      "video 1/1 (frame 156/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.2ms\n",
      "video 1/1 (frame 157/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 118.9ms\n",
      "video 1/1 (frame 158/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.6ms\n",
      "video 1/1 (frame 159/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.2ms\n",
      "video 1/1 (frame 160/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.6ms\n",
      "video 1/1 (frame 161/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.8ms\n",
      "video 1/1 (frame 162/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.6ms\n",
      "video 1/1 (frame 163/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.0ms\n",
      "video 1/1 (frame 164/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 109.7ms\n",
      "video 1/1 (frame 165/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.9ms\n",
      "video 1/1 (frame 166/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.2ms\n",
      "video 1/1 (frame 167/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 97.5ms\n",
      "video 1/1 (frame 168/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.6ms\n",
      "video 1/1 (frame 169/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.2ms\n",
      "video 1/1 (frame 170/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 101.1ms\n",
      "video 1/1 (frame 171/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.9ms\n",
      "video 1/1 (frame 172/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.5ms\n",
      "video 1/1 (frame 173/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 96.9ms\n",
      "video 1/1 (frame 174/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.4ms\n",
      "video 1/1 (frame 175/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 111.3ms\n",
      "video 1/1 (frame 176/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 143.5ms\n",
      "video 1/1 (frame 177/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 118.3ms\n",
      "video 1/1 (frame 178/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.1ms\n",
      "video 1/1 (frame 179/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 91.9ms\n",
      "video 1/1 (frame 180/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 87.6ms\n",
      "video 1/1 (frame 181/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 131.1ms\n",
      "video 1/1 (frame 182/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 110.2ms\n",
      "video 1/1 (frame 183/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 91.3ms\n",
      "video 1/1 (frame 184/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.4ms\n",
      "video 1/1 (frame 185/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.9ms\n",
      "video 1/1 (frame 186/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 93.1ms\n",
      "video 1/1 (frame 187/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.2ms\n",
      "video 1/1 (frame 188/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 101.0ms\n",
      "video 1/1 (frame 189/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 104.5ms\n",
      "video 1/1 (frame 190/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 96.6ms\n",
      "video 1/1 (frame 191/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 104.3ms\n",
      "video 1/1 (frame 192/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.9ms\n",
      "video 1/1 (frame 193/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 123.5ms\n",
      "video 1/1 (frame 194/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 150.9ms\n",
      "video 1/1 (frame 195/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 131.6ms\n",
      "video 1/1 (frame 196/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 142.2ms\n",
      "video 1/1 (frame 197/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 232.9ms\n",
      "video 1/1 (frame 198/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 175.7ms\n",
      "video 1/1 (frame 199/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 129.6ms\n",
      "video 1/1 (frame 200/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.7ms\n",
      "video 1/1 (frame 201/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 97.1ms\n",
      "video 1/1 (frame 202/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.1ms\n",
      "video 1/1 (frame 203/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 101.9ms\n",
      "video 1/1 (frame 204/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.9ms\n",
      "video 1/1 (frame 205/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 95.4ms\n",
      "video 1/1 (frame 206/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 97.2ms\n",
      "video 1/1 (frame 207/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.1ms\n",
      "video 1/1 (frame 208/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 95.3ms\n",
      "video 1/1 (frame 209/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 97.5ms\n",
      "video 1/1 (frame 210/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 105.9ms\n",
      "video 1/1 (frame 211/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 94.5ms\n",
      "video 1/1 (frame 212/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.3ms\n",
      "video 1/1 (frame 213/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.7ms\n",
      "video 1/1 (frame 214/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.0ms\n",
      "video 1/1 (frame 215/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 109.5ms\n",
      "video 1/1 (frame 216/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.7ms\n",
      "video 1/1 (frame 217/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 109.0ms\n",
      "video 1/1 (frame 218/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 161.5ms\n",
      "video 1/1 (frame 219/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 125.1ms\n",
      "video 1/1 (frame 220/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 127.0ms\n",
      "video 1/1 (frame 221/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 145.1ms\n",
      "video 1/1 (frame 222/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.8ms\n",
      "video 1/1 (frame 223/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 132.1ms\n",
      "video 1/1 (frame 224/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 156.4ms\n",
      "video 1/1 (frame 225/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 177.7ms\n",
      "video 1/1 (frame 226/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 118.7ms\n",
      "video 1/1 (frame 227/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 110.1ms\n",
      "video 1/1 (frame 228/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 104.4ms\n",
      "video 1/1 (frame 229/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.8ms\n",
      "video 1/1 (frame 230/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.8ms\n",
      "video 1/1 (frame 231/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 91.0ms\n",
      "video 1/1 (frame 232/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.8ms\n",
      "video 1/1 (frame 233/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.2ms\n",
      "video 1/1 (frame 234/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 112.1ms\n",
      "video 1/1 (frame 235/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.5ms\n",
      "video 1/1 (frame 236/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 97.5ms\n",
      "video 1/1 (frame 237/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 94.3ms\n",
      "video 1/1 (frame 238/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.7ms\n",
      "video 1/1 (frame 239/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 153.6ms\n",
      "video 1/1 (frame 240/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 135.1ms\n",
      "video 1/1 (frame 241/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 162.1ms\n",
      "video 1/1 (frame 242/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 157.1ms\n",
      "video 1/1 (frame 243/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 131.1ms\n",
      "video 1/1 (frame 244/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 110.7ms\n",
      "video 1/1 (frame 245/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.9ms\n",
      "video 1/1 (frame 246/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.5ms\n",
      "video 1/1 (frame 247/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 112.4ms\n",
      "video 1/1 (frame 248/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 111.5ms\n",
      "video 1/1 (frame 249/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 94.1ms\n",
      "video 1/1 (frame 250/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 95.8ms\n",
      "video 1/1 (frame 251/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.9ms\n",
      "video 1/1 (frame 252/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.6ms\n",
      "video 1/1 (frame 253/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 140.6ms\n",
      "video 1/1 (frame 254/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 174.2ms\n",
      "video 1/1 (frame 255/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 130.6ms\n",
      "video 1/1 (frame 256/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 119.9ms\n",
      "video 1/1 (frame 257/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 107.7ms\n",
      "video 1/1 (frame 258/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 103.5ms\n",
      "video 1/1 (frame 259/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.8ms\n",
      "video 1/1 (frame 260/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 167.2ms\n",
      "video 1/1 (frame 261/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 116.7ms\n",
      "video 1/1 (frame 262/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.2ms\n",
      "video 1/1 (frame 263/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 161.7ms\n",
      "video 1/1 (frame 264/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 116.0ms\n",
      "video 1/1 (frame 265/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 109.2ms\n",
      "video 1/1 (frame 266/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 87.9ms\n",
      "video 1/1 (frame 267/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 93.0ms\n",
      "video 1/1 (frame 268/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.8ms\n",
      "video 1/1 (frame 269/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 143.2ms\n",
      "video 1/1 (frame 270/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 161.6ms\n",
      "video 1/1 (frame 271/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 145.9ms\n",
      "video 1/1 (frame 272/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 137.2ms\n",
      "video 1/1 (frame 273/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 141.2ms\n",
      "video 1/1 (frame 274/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 130.0ms\n",
      "video 1/1 (frame 275/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 148.1ms\n",
      "video 1/1 (frame 276/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 160.6ms\n",
      "video 1/1 (frame 277/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 160.2ms\n",
      "video 1/1 (frame 278/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 140.5ms\n",
      "video 1/1 (frame 279/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 124.7ms\n",
      "video 1/1 (frame 280/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 111.9ms\n",
      "video 1/1 (frame 281/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 111.7ms\n",
      "video 1/1 (frame 282/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 111.0ms\n",
      "video 1/1 (frame 283/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 106.4ms\n",
      "video 1/1 (frame 284/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 96.4ms\n",
      "video 1/1 (frame 285/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 92.9ms\n",
      "video 1/1 (frame 286/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.8ms\n",
      "video 1/1 (frame 287/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 108.4ms\n",
      "video 1/1 (frame 288/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 111.2ms\n",
      "video 1/1 (frame 289/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 143.4ms\n",
      "video 1/1 (frame 290/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 99.7ms\n",
      "video 1/1 (frame 291/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 105.8ms\n",
      "video 1/1 (frame 292/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 104.0ms\n",
      "video 1/1 (frame 293/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.7ms\n",
      "video 1/1 (frame 294/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 92.8ms\n",
      "video 1/1 (frame 295/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 93.8ms\n",
      "video 1/1 (frame 296/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 98.1ms\n",
      "video 1/1 (frame 297/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 102.2ms\n",
      "video 1/1 (frame 298/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 100.5ms\n",
      "video 1/1 (frame 299/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 101.8ms\n",
      "video 1/1 (frame 300/300) d:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\organized_dataset\\Class_0_Axel\\cam_1_Axel_1.mp4: 384x640 1 Skater, 92.9ms\n",
      "Speed: 4.2ms preprocess, 100.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mD:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\track\u001b[0m\n",
      "Tracking complete! Video saved in: D:\\Desktop\\Desk\\Labellerr Github Projects\\Use_Case_Projects\\olympics_cv_project\\runs\\detect\\track\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Load your best model\n",
    "model = YOLO(\"runs/detect/olympics_cv/skater_detector_v1/weights/best.pt\")\n",
    "\n",
    "# 2. Path to your test video\n",
    "video_path = \"organized_dataset/Class_0_Axel/cam_1_Axel_1.mp4\"\n",
    "\n",
    "# 3. Run tracking\n",
    "# persist=True: maintains the same ID for the skater across frames\n",
    "# tracker=\"bytetrack.yaml\": Highly efficient for fast-moving sports\n",
    "results = model.track(\n",
    "    source=video_path, \n",
    "    persist=True, \n",
    "    tracker=\"bytetrack.yaml\", \n",
    "    save=True,\n",
    "    conf=0.4\n",
    ")\n",
    "\n",
    "print(f\"Tracking complete! Video saved in: {results[0].save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3c97d",
   "metadata": {},
   "source": [
    "## Skeleton Overlay & Video Inference\n",
    "\n",
    "The following script generates an annotated video showing the real-time skeletal tracking of the skater. This is used to verify that the pose estimation is accurate enough for the subsequent LSTM classification stage.\n",
    "\n",
    "### Inference Parameters:\n",
    "| Setting | Value | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **Padding (PAD)** | 0.20 | 20% buffer added to YOLO box for stable pose tracking. |\n",
    "| **Model Complexity** | 2 | Uses MediaPipe's \"Heavy\" model for maximum landmark precision. |\n",
    "| **Codec** | mp4v | Standard MP4 compression for the output file. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2558ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference video saved as: inference_check_axel.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "yolo_model = YOLO(\"runs/detect/olympics_cv/skater_detector_v1/weights/best.pt\")\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "input_path = \"organized_dataset/Class_0_Axel/cam_1_Axel_1.mp4\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "output_path = \"inference_check_axel.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "PAD = 0.20 \n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    \n",
    "    results = yolo_model.track(frame, persist=True, verbose=False)\n",
    "    \n",
    "    if results[0].boxes.id is not None:\n",
    "        for box in results[0].boxes.xyxy:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            bw, bh = x2 - x1, y2 - y1\n",
    "            \n",
    "           \n",
    "            pw, ph = int(bw * PAD), int(bh * PAD)\n",
    "            cx1, cy1 = max(0, x1 - pw), max(0, y1 - ph)\n",
    "            cx2, cy2 = min(w, x2 + pw), min(h, y2 + ph)\n",
    "            \n",
    "            crop = frame[cy1:cy2, cx1:cx2]\n",
    "            if crop.size == 0: continue\n",
    "            \n",
    "            \n",
    "            results_pose = pose.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            if results_pose.pose_landmarks:\n",
    "                \n",
    "                mp_drawing.draw_landmarks(\n",
    "                    crop, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "                \n",
    "                frame[cy1:cy2, cx1:cx2] = crop\n",
    "\n",
    "    \n",
    "    out.write(frame)\n",
    "    \n",
    "    cv2.imshow(\"Inference Check\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "out.release() \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Inference video saved as: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c1457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference video saved as: inference_check_edge.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Initialize YOLO and Legacy MediaPipe\n",
    "yolo_model = YOLO(\"runs/detect/olympics_cv/skater_detector_v1/weights/best.pt\")\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 2. Setup Video Input and Output\n",
    "input_path = \"organized_dataset/Class_1_Edge/cam_2_loop_1.mp4\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "# Get video properties for the writer\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Initialize VideoWriter (Saves as .mp4)\n",
    "output_path = \"inference_check_edge.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "PAD = 0.20 # 20% Padding\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    # YOLO Tracking\n",
    "    results = yolo_model.track(frame, persist=True, verbose=False)\n",
    "    \n",
    "    if results[0].boxes.id is not None:\n",
    "        for box in results[0].boxes.xyxy:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            bw, bh = x2 - x1, y2 - y1\n",
    "            \n",
    "            # Apply Padding\n",
    "            pw, ph = int(bw * PAD), int(bh * PAD)\n",
    "            cx1, cy1 = max(0, x1 - pw), max(0, y1 - ph)\n",
    "            cx2, cy2 = min(w, x2 + pw), min(h, y2 + ph)\n",
    "            \n",
    "            crop = frame[cy1:cy2, cx1:cx2]\n",
    "            if crop.size == 0: continue\n",
    "            \n",
    "            # MediaPipe Pose Estimation\n",
    "            results_pose = pose.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            if results_pose.pose_landmarks:\n",
    "                # Draw landmarks directly on the crop\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    crop, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "                # Overlay crop back onto the original frame\n",
    "                frame[cy1:cy2, cx1:cx2] = crop\n",
    "\n",
    "    # WRITE the annotated frame to your video file\n",
    "    out.write(frame)\n",
    "    \n",
    "    cv2.imshow(\"Inference Check\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "# 3. Clean up\n",
    "cap.release()\n",
    "out.release() # CRITICAL: If you don't release, the video will be corrupt\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Inference video saved as: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f70fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference video saved as: inference_check_complex_pick.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Initialize YOLO and Legacy MediaPipe\n",
    "yolo_model = YOLO(\"runs/detect/olympics_cv/skater_detector_v1/weights/best.pt\")\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 2. Setup Video Input and Output\n",
    "input_path = \"organized_dataset/Class_2_Complex_Pick/cam_3_Flip_1.mp4\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "# Get video properties for the writer\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Initialize VideoWriter (Saves as .mp4)\n",
    "output_path = \"inference_check_complex_pick.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "PAD = 0.20 # 20% Padding\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    # YOLO Tracking\n",
    "    results = yolo_model.track(frame, persist=True, verbose=False)\n",
    "    \n",
    "    if results[0].boxes.id is not None:\n",
    "        for box in results[0].boxes.xyxy:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            bw, bh = x2 - x1, y2 - y1\n",
    "            \n",
    "            # Apply Padding\n",
    "            pw, ph = int(bw * PAD), int(bh * PAD)\n",
    "            cx1, cy1 = max(0, x1 - pw), max(0, y1 - ph)\n",
    "            cx2, cy2 = min(w, x2 + pw), min(h, y2 + ph)\n",
    "            \n",
    "            crop = frame[cy1:cy2, cx1:cx2]\n",
    "            if crop.size == 0: continue\n",
    "            \n",
    "            # MediaPipe Pose Estimation\n",
    "            results_pose = pose.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            if results_pose.pose_landmarks:\n",
    "                # Draw landmarks directly on the crop\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    crop, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "                # Overlay crop back onto the original frame\n",
    "                frame[cy1:cy2, cx1:cx2] = crop\n",
    "\n",
    "    # WRITE the annotated frame to your video file\n",
    "    out.write(frame)\n",
    "    \n",
    "    cv2.imshow(\"Inference Check\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "# 3. Clean up\n",
    "cap.release()\n",
    "out.release() # CRITICAL: If you don't release, the video will be corrupt\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Inference video saved as: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a099ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference video saved as: inference_check_simple_pick.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Initialize YOLO and Legacy MediaPipe\n",
    "yolo_model = YOLO(\"runs/detect/olympics_cv/skater_detector_v1/weights/best.pt\")\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 2. Setup Video Input and Output\n",
    "input_path = \"organized_dataset/Class_3_Simple_Pick/cam_9_Toeloop_4.mp4\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "# Get video properties for the writer\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Initialize VideoWriter (Saves as .mp4)\n",
    "output_path = \"inference_check_simple_pick.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for .mp4\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "PAD = 0.20 # 20% Padding\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    # YOLO Tracking\n",
    "    results = yolo_model.track(frame, persist=True, verbose=False)\n",
    "    \n",
    "    if results[0].boxes.id is not None:\n",
    "        for box in results[0].boxes.xyxy:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            bw, bh = x2 - x1, y2 - y1\n",
    "            \n",
    "            # Apply Padding\n",
    "            pw, ph = int(bw * PAD), int(bh * PAD)\n",
    "            cx1, cy1 = max(0, x1 - pw), max(0, y1 - ph)\n",
    "            cx2, cy2 = min(w, x2 + pw), min(h, y2 + ph)\n",
    "            \n",
    "            crop = frame[cy1:cy2, cx1:cx2]\n",
    "            if crop.size == 0: continue\n",
    "            \n",
    "            # MediaPipe Pose Estimation\n",
    "            results_pose = pose.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            if results_pose.pose_landmarks:\n",
    "                # Draw landmarks directly on the crop\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    crop, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "                # Overlay crop back onto the original frame\n",
    "                frame[cy1:cy2, cx1:cx2] = crop\n",
    "\n",
    "    # WRITE the annotated frame to your video file\n",
    "    out.write(frame)\n",
    "    \n",
    "    cv2.imshow(\"Inference Check\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "# 3. Clean up\n",
    "cap.release()\n",
    "out.release() # CRITICAL: If you don't release, the video will be corrupt\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Inference video saved as: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7179b6",
   "metadata": {},
   "source": [
    "## Skeletal Feature Extraction Pipeline\n",
    "\n",
    "The following stage transforms raw video data into a series of 3D skeletal landmarks. This numerical representation is what allows the LSTM model to \"understand\" the motion of a jump without being distracted by background colors or rink lighting.\n",
    "\n",
    "### Pipeline Workflow:\n",
    "| Stage | Description | Technical Detail |\n",
    "| :--- | :--- | :--- |\n",
    "| **Tracking** | YOLOv11-Tracking | Maintains a consistent bounding box on the skater using GPU acceleration. |\n",
    "| **Cropping** | Dynamic ROI | Crops the frame around the skater with a 20% padding buffer to prevent limb clipping. |\n",
    "| **Pose Estimation** | MediaPipe Pose | Extracts 33 keypoints (x, y, z, visibility) from the localized crop. |\n",
    "| **Normalization** | Global Re-projection | Maps landmarks from the local crop back to the original video dimensions (0.0 to 1.0). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Setup GPU-accelerated Models\n",
    "DRIVE_PATH = \"/content/drive/MyDrive\"\n",
    "yolo_model = YOLO(f\"{DRIVE_PATH}/best (2).pt\").to('cuda')\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5)\n",
    "\n",
    "# 2. Paths\n",
    "DATASET_DIR = f\"{DRIVE_PATH}/organized_dataset\"\n",
    "OUTPUT_DIR = f\"{DRIVE_PATH}/processed_skeleton_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "PAD = 0.20 # The 20% padding for fast jumps\n",
    "\n",
    "def extract_skeleton(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    sequence = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # YOLO Tracking on GPU\n",
    "        results = yolo_model.track(frame, persist=True, verbose=False, device=0)\n",
    "\n",
    "        # Default: 33 points with 4 values (x,y,z,v) all set to 0\n",
    "        frame_landmarks = np.zeros((33, 4))\n",
    "\n",
    "        if results[0].boxes.id is not None:\n",
    "            # Detect first skater\n",
    "            box = results[0].boxes.xyxy[0].cpu().numpy()\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            bw, bh = x2 - x1, y2 - y1\n",
    "\n",
    "            # Padded Crop for MediaPipe\n",
    "            cx1, cy1 = max(0, x1 - int(bw*PAD)), max(0, y1 - int(bh*PAD))\n",
    "            cx2, cy2 = min(w, x2 + int(bw*PAD)), min(h, y2 + int(bh*PAD))\n",
    "            crop = frame[cy1:cy2, cx1:cx2]\n",
    "\n",
    "            if crop.size > 0:\n",
    "                res = pose.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "                if res.pose_landmarks:\n",
    "                    # Convert to re-projected full-frame normalized coordinates\n",
    "                    frame_landmarks = np.array([\n",
    "                        [(lm.x * (cx2-cx1) + cx1)/w, (lm.y * (cy2-cy1) + cy1)/h, lm.z, lm.visibility]\n",
    "                        for lm in res.pose_landmarks.landmark\n",
    "                    ])\n",
    "\n",
    "        sequence.append(frame_landmarks)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(sequence)\n",
    "\n",
    "# 3. Execution Loop\n",
    "print(\"Starting GPU Skeleton Extraction...\")\n",
    "# Change from your old list to the actual names found on your drive:\n",
    "for category in [\"Class_0_Axel\", \"Class_1_Edge\", \"Class_2_Complex_Pick\", \"Class_3_Simple_Pick\"]:\n",
    "    cat_path = os.path.join(DATASET_DIR, category)\n",
    "    out_cat_path = os.path.join(OUTPUT_DIR, category)\n",
    "    os.makedirs(out_cat_path, exist_ok=True)\n",
    "\n",
    "    for video in os.listdir(cat_path):\n",
    "        if video.endswith(\".mp4\"):\n",
    "            save_path = os.path.join(out_cat_path, video.replace(\".mp4\", \".npy\"))\n",
    "            if not os.path.exists(save_path):\n",
    "                data = extract_skeleton(os.path.join(cat_path, video))\n",
    "                np.save(save_path, data)\n",
    "                print(f\"Saved: {category}/{video} (Frames: {len(data)})\")\n",
    "\n",
    "print(\"All 720 clips processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739e6e0",
   "metadata": {},
   "source": [
    "## LSTM Training & Sequential Modeling\n",
    "\n",
    "This stage defines the neural network architecture and the data loading pipeline. The model is specifically designed to handle the variable-length nature of figure skating jumps by standardizing input sequences to a fixed temporal window.\n",
    "\n",
    "### Model Specifications:\n",
    "| Layer | Type | Configuration | Purpose |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Input** | Sequential | (150, 132) | 150 frames of 33 landmarks (x,y,z,v). |\n",
    "| **LSTM 1** | Bidirectional | 64 Units | Captures temporal dependencies in both directions. |\n",
    "| **Dropout** | Regularization | 0.5 | Prevents overfitting on small athletic datasets. |\n",
    "| **Output** | Dense | 4 Units (Softmax) | Classifies the jump into the 4-class taxonomy. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79e33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Update this to your local folder path\n",
    "DATA_PATH = \"D:/olympics_cv_project/processed_skeleton_data\" \n",
    "actions = np.array(['Class_0_Axel', 'Class_1_Edge', 'Class_2_Complex_Pick', 'Class_3_Simple_Pick'])\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "MAX_SEQUENCE_LENGTH = 150 # 5 Seconds @ 30fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02b3f8",
   "metadata": {},
   "source": [
    "## Skeletal Normalization & Preprocessing\n",
    "\n",
    "The following script implements spatial and scale normalization for the skeletal data. This step is essential for training a robust model that generalizes across different camera angles and athlete body types.\n",
    "\n",
    "### Normalization Workflow:\n",
    "| Technique | Description | Mathematical Goal |\n",
    "| :--- | :--- | :--- |\n",
    "| **Centering** | Sets origin at hip midpoint. | Eliminate positional bias (x, y, z translation). |\n",
    "| **Scaling** | Normalizes by torso height. | Eliminate camera distance and height bias. |\n",
    "| **Padding** | Concatenates zero-arrays. | Standardize input shape to 150 frames. |\n",
    "| **Flattening** | Reshapes (33, 4) to (132,) | Prepare features for LSTM input layers. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, normalizing, and preprocessing skeletal data...\n",
      "Data ready. X_train shape: (576, 150, 132)\n"
     ]
    }
   ],
   "source": [
    "def normalize_skeleton(frame_data):\n",
    "    \"\"\" Centers skater at hips and scales by body height. \"\"\"\n",
    "    \n",
    "    hip_center = (frame_data[23, :3] + frame_data[24, :3]) / 2\n",
    "    normalized = frame_data.copy()\n",
    "    normalized[:, :3] = frame_data[:, :3] - hip_center\n",
    "    \n",
    "    shoulder_center = (frame_data[11, :3] + frame_data[12, :3]) / 2\n",
    "    scale_dist = np.linalg.norm(shoulder_center - hip_center)\n",
    "    \n",
    "    if scale_dist > 0:\n",
    "        normalized[:, :3] = normalized[:, :3] / scale_dist\n",
    "        \n",
    "    return normalized\n",
    "\n",
    "print(\"Loading, normalizing, and preprocessing skeletal data...\")\n",
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    if not os.path.exists(action_path): continue\n",
    "    \n",
    "    for npy_file in os.listdir(action_path):\n",
    "        res = np.load(os.path.join(action_path, npy_file))\n",
    "        \n",
    "        \n",
    "        res_norm = np.array([normalize_skeleton(frame) for frame in res])\n",
    "        \n",
    "        if len(res_norm) < MAX_SEQUENCE_LENGTH:\n",
    "            padding = np.zeros((MAX_SEQUENCE_LENGTH - len(res_norm), 33, 4))\n",
    "            res_norm = np.concatenate([res_norm, padding], axis=0)\n",
    "        else:\n",
    "            res_norm = res_norm[:MAX_SEQUENCE_LENGTH]\n",
    "            \n",
    "        sequences.append(res_norm.flatten().reshape(MAX_SEQUENCE_LENGTH, -1))\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Data ready. X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31361a1b",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM Model Definition\n",
    "\n",
    "The following code defines the neural network architecture used to achieve high-accuracy jump classification. This model takes the normalized 150-frame skeletal sequences as input and outputs the predicted jump category.\n",
    "\n",
    "### Model Summary & Layer Stack:\n",
    "| Layer (type) | Output Shape | Param # | Description |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Bidirectional (LSTM)** | (None, 150, 128) | 100,864 | Processes temporal skeletal landmarks forward and backward. |\n",
    "| **Dropout (0.3)** | (None, 150, 128) | 0 | Regularization layer to prevent overfitting. |\n",
    "| **Bidirectional (LSTM)** | (None, 256) | 263,168 | Second temporal layer for high-level motion patterns. |\n",
    "| **Dense (ReLU)** | (None, 64) | 16,448 | Fully connected decision layer. |\n",
    "| **Dense (Softmax)** | (None, 4) | 260 | Final output for 4-class jump taxonomy. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e827880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m100,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m263,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">380,740</span> (1.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m380,740\u001b[0m (1.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">380,740</span> (1.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m380,740\u001b[0m (1.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, activation='tanh'), \n",
    "                        input_shape=(MAX_SEQUENCE_LENGTH, 132)))\n",
    "model.add(Dropout(0.3)) \n",
    "\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=False, activation='tanh')))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax')) \n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904a59b",
   "metadata": {},
   "source": [
    "## Model Training Execution\n",
    "\n",
    "The following code block initiates the training loop. This process transforms the raw skeletal data into a predictive AI model capable of classifying competitive skating jumps.\n",
    "\n",
    "### Training Workflow:\n",
    "| Metric | Value | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **Training Samples** | ~576 | 80% of the total dataset used for weight optimization. |\n",
    "| **Validation Samples** | ~144 | 20% of the dataset used to verify accuracy. |\n",
    "| **Epochs** | 100 | Duration of the training session. |\n",
    "| **Format** | .keras | Modern, single-file model serialization. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257fa50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on 576 samples...\n",
      "Epoch 1/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 469ms/step - categorical_accuracy: 0.3142 - loss: 1.3662 - val_categorical_accuracy: 0.2986 - val_loss: 1.3318\n",
      "Epoch 2/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 455ms/step - categorical_accuracy: 0.3854 - loss: 1.2960 - val_categorical_accuracy: 0.3333 - val_loss: 1.2701\n",
      "Epoch 3/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 465ms/step - categorical_accuracy: 0.4201 - loss: 1.2371 - val_categorical_accuracy: 0.4028 - val_loss: 1.2101\n",
      "Epoch 4/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 448ms/step - categorical_accuracy: 0.4688 - loss: 1.1829 - val_categorical_accuracy: 0.4306 - val_loss: 1.1551\n",
      "Epoch 5/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 395ms/step - categorical_accuracy: 0.4861 - loss: 1.1422 - val_categorical_accuracy: 0.4722 - val_loss: 1.0914\n",
      "Epoch 6/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 378ms/step - categorical_accuracy: 0.5538 - loss: 1.0770 - val_categorical_accuracy: 0.4792 - val_loss: 1.0398\n",
      "Epoch 7/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 499ms/step - categorical_accuracy: 0.5573 - loss: 1.0290 - val_categorical_accuracy: 0.5208 - val_loss: 0.9674\n",
      "Epoch 8/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 506ms/step - categorical_accuracy: 0.6146 - loss: 0.9544 - val_categorical_accuracy: 0.5347 - val_loss: 0.9059\n",
      "Epoch 9/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 380ms/step - categorical_accuracy: 0.6615 - loss: 0.8602 - val_categorical_accuracy: 0.5694 - val_loss: 0.8435\n",
      "Epoch 10/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 446ms/step - categorical_accuracy: 0.6979 - loss: 0.7817 - val_categorical_accuracy: 0.7014 - val_loss: 0.7294\n",
      "Epoch 11/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 444ms/step - categorical_accuracy: 0.7535 - loss: 0.6811 - val_categorical_accuracy: 0.7917 - val_loss: 0.5957\n",
      "Epoch 12/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 414ms/step - categorical_accuracy: 0.7951 - loss: 0.5829 - val_categorical_accuracy: 0.7847 - val_loss: 0.5588\n",
      "Epoch 13/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 284ms/step - categorical_accuracy: 0.8247 - loss: 0.5084 - val_categorical_accuracy: 0.7847 - val_loss: 0.5119\n",
      "Epoch 14/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 502ms/step - categorical_accuracy: 0.8906 - loss: 0.3959 - val_categorical_accuracy: 0.7986 - val_loss: 0.5215\n",
      "Epoch 15/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 463ms/step - categorical_accuracy: 0.8854 - loss: 0.3447 - val_categorical_accuracy: 0.9167 - val_loss: 0.3405\n",
      "Epoch 16/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 392ms/step - categorical_accuracy: 0.9288 - loss: 0.2557 - val_categorical_accuracy: 0.8889 - val_loss: 0.3509\n",
      "Epoch 17/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 372ms/step - categorical_accuracy: 0.9410 - loss: 0.2175 - val_categorical_accuracy: 0.8958 - val_loss: 0.3061\n",
      "Epoch 18/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 417ms/step - categorical_accuracy: 0.9670 - loss: 0.1583 - val_categorical_accuracy: 0.8819 - val_loss: 0.3366\n",
      "Epoch 19/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 444ms/step - categorical_accuracy: 0.9549 - loss: 0.1935 - val_categorical_accuracy: 0.9097 - val_loss: 0.2625\n",
      "Epoch 20/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 436ms/step - categorical_accuracy: 0.9688 - loss: 0.1411 - val_categorical_accuracy: 0.9028 - val_loss: 0.2600\n",
      "Epoch 21/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 384ms/step - categorical_accuracy: 0.9392 - loss: 0.1879 - val_categorical_accuracy: 0.9167 - val_loss: 0.3096\n",
      "Epoch 22/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 434ms/step - categorical_accuracy: 0.9774 - loss: 0.1021 - val_categorical_accuracy: 0.9097 - val_loss: 0.2814\n",
      "Epoch 23/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 459ms/step - categorical_accuracy: 0.9844 - loss: 0.0755 - val_categorical_accuracy: 0.9167 - val_loss: 0.2710\n",
      "Epoch 24/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 403ms/step - categorical_accuracy: 0.9792 - loss: 0.0827 - val_categorical_accuracy: 0.9236 - val_loss: 0.2295\n",
      "Epoch 25/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 373ms/step - categorical_accuracy: 0.9688 - loss: 0.0955 - val_categorical_accuracy: 0.9236 - val_loss: 0.2213\n",
      "Epoch 26/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 412ms/step - categorical_accuracy: 0.9722 - loss: 0.0962 - val_categorical_accuracy: 0.9236 - val_loss: 0.2817\n",
      "Epoch 27/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 476ms/step - categorical_accuracy: 0.9809 - loss: 0.0797 - val_categorical_accuracy: 0.9236 - val_loss: 0.2468\n",
      "Epoch 28/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 399ms/step - categorical_accuracy: 0.9826 - loss: 0.0661 - val_categorical_accuracy: 0.9306 - val_loss: 0.2459\n",
      "Epoch 29/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 369ms/step - categorical_accuracy: 0.9878 - loss: 0.0544 - val_categorical_accuracy: 0.9097 - val_loss: 0.2761\n",
      "Epoch 30/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 486ms/step - categorical_accuracy: 0.9896 - loss: 0.0445 - val_categorical_accuracy: 0.9306 - val_loss: 0.2182\n",
      "Epoch 31/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 537ms/step - categorical_accuracy: 0.9896 - loss: 0.0387 - val_categorical_accuracy: 0.9444 - val_loss: 0.2292\n",
      "Epoch 32/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 430ms/step - categorical_accuracy: 0.9913 - loss: 0.0368 - val_categorical_accuracy: 0.9444 - val_loss: 0.2041\n",
      "Epoch 33/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 380ms/step - categorical_accuracy: 0.9896 - loss: 0.0344 - val_categorical_accuracy: 0.9444 - val_loss: 0.2277\n",
      "Epoch 34/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 394ms/step - categorical_accuracy: 0.9896 - loss: 0.0407 - val_categorical_accuracy: 0.9167 - val_loss: 0.2961\n",
      "Epoch 35/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 389ms/step - categorical_accuracy: 0.9931 - loss: 0.0338 - val_categorical_accuracy: 0.9167 - val_loss: 0.3030\n",
      "Epoch 36/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 382ms/step - categorical_accuracy: 0.9931 - loss: 0.0316 - val_categorical_accuracy: 0.9236 - val_loss: 0.2635\n",
      "Epoch 37/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 386ms/step - categorical_accuracy: 0.9931 - loss: 0.0279 - val_categorical_accuracy: 0.9236 - val_loss: 0.2902\n",
      "Epoch 38/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 389ms/step - categorical_accuracy: 0.9965 - loss: 0.0239 - val_categorical_accuracy: 0.9236 - val_loss: 0.2932\n",
      "Epoch 39/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 394ms/step - categorical_accuracy: 0.9948 - loss: 0.0235 - val_categorical_accuracy: 0.9167 - val_loss: 0.2961\n",
      "Epoch 40/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 396ms/step - categorical_accuracy: 0.9965 - loss: 0.0220 - val_categorical_accuracy: 0.9167 - val_loss: 0.3281\n",
      "Epoch 41/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 387ms/step - categorical_accuracy: 0.9948 - loss: 0.0202 - val_categorical_accuracy: 0.9097 - val_loss: 0.3312\n",
      "Epoch 42/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 376ms/step - categorical_accuracy: 0.9948 - loss: 0.0209 - val_categorical_accuracy: 0.9167 - val_loss: 0.3343\n",
      "Epoch 43/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 393ms/step - categorical_accuracy: 0.9826 - loss: 0.0530 - val_categorical_accuracy: 0.9167 - val_loss: 0.3000\n",
      "Epoch 44/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 381ms/step - categorical_accuracy: 0.9861 - loss: 0.0507 - val_categorical_accuracy: 0.9236 - val_loss: 0.2782\n",
      "Epoch 45/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 368ms/step - categorical_accuracy: 0.9896 - loss: 0.0437 - val_categorical_accuracy: 0.9167 - val_loss: 0.3227\n",
      "Epoch 46/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 393ms/step - categorical_accuracy: 0.9948 - loss: 0.0291 - val_categorical_accuracy: 0.9236 - val_loss: 0.3155\n",
      "Epoch 47/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 402ms/step - categorical_accuracy: 0.9948 - loss: 0.0285 - val_categorical_accuracy: 0.9236 - val_loss: 0.3226\n",
      "Epoch 48/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 376ms/step - categorical_accuracy: 0.9983 - loss: 0.0219 - val_categorical_accuracy: 0.9167 - val_loss: 0.3388\n",
      "Epoch 49/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 379ms/step - categorical_accuracy: 0.9948 - loss: 0.0212 - val_categorical_accuracy: 0.9167 - val_loss: 0.3490\n",
      "Epoch 50/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 378ms/step - categorical_accuracy: 0.9913 - loss: 0.0238 - val_categorical_accuracy: 0.9167 - val_loss: 0.3665\n",
      "Epoch 51/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 404ms/step - categorical_accuracy: 0.9948 - loss: 0.0211 - val_categorical_accuracy: 0.9097 - val_loss: 0.4197\n",
      "Epoch 52/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 407ms/step - categorical_accuracy: 0.9965 - loss: 0.0176 - val_categorical_accuracy: 0.9306 - val_loss: 0.3003\n",
      "Epoch 53/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 435ms/step - categorical_accuracy: 0.9167 - loss: 0.3129 - val_categorical_accuracy: 0.8194 - val_loss: 0.5862\n",
      "Epoch 54/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 421ms/step - categorical_accuracy: 0.9410 - loss: 0.1849 - val_categorical_accuracy: 0.8958 - val_loss: 0.3708\n",
      "Epoch 55/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 369ms/step - categorical_accuracy: 0.9774 - loss: 0.0883 - val_categorical_accuracy: 0.8889 - val_loss: 0.3246\n",
      "Epoch 56/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 398ms/step - categorical_accuracy: 0.9844 - loss: 0.0563 - val_categorical_accuracy: 0.9097 - val_loss: 0.2969\n",
      "Epoch 57/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 370ms/step - categorical_accuracy: 0.9948 - loss: 0.0352 - val_categorical_accuracy: 0.9167 - val_loss: 0.2926\n",
      "Epoch 58/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 358ms/step - categorical_accuracy: 0.9948 - loss: 0.0327 - val_categorical_accuracy: 0.9306 - val_loss: 0.2835\n",
      "Epoch 59/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 354ms/step - categorical_accuracy: 0.9740 - loss: 0.0773 - val_categorical_accuracy: 0.9097 - val_loss: 0.2737\n",
      "Epoch 60/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 380ms/step - categorical_accuracy: 0.9809 - loss: 0.0628 - val_categorical_accuracy: 0.9028 - val_loss: 0.2777\n",
      "Epoch 61/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 391ms/step - categorical_accuracy: 0.9965 - loss: 0.0280 - val_categorical_accuracy: 0.9167 - val_loss: 0.2753\n",
      "Epoch 62/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 382ms/step - categorical_accuracy: 0.9965 - loss: 0.0235 - val_categorical_accuracy: 0.9306 - val_loss: 0.2854\n",
      "Epoch 63/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 427ms/step - categorical_accuracy: 0.9931 - loss: 0.0231 - val_categorical_accuracy: 0.9306 - val_loss: 0.3001\n",
      "Epoch 64/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 476ms/step - categorical_accuracy: 0.9931 - loss: 0.0221 - val_categorical_accuracy: 0.9097 - val_loss: 0.3023\n",
      "Epoch 65/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 507ms/step - categorical_accuracy: 0.9965 - loss: 0.0201 - val_categorical_accuracy: 0.9097 - val_loss: 0.3067\n",
      "Epoch 66/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 545ms/step - categorical_accuracy: 0.9931 - loss: 0.0182 - val_categorical_accuracy: 0.9167 - val_loss: 0.3287\n",
      "Epoch 67/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 395ms/step - categorical_accuracy: 0.9965 - loss: 0.0159 - val_categorical_accuracy: 0.9236 - val_loss: 0.3376\n",
      "Epoch 68/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 370ms/step - categorical_accuracy: 0.9948 - loss: 0.0196 - val_categorical_accuracy: 0.9167 - val_loss: 0.3104\n",
      "Epoch 69/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 420ms/step - categorical_accuracy: 0.9965 - loss: 0.0151 - val_categorical_accuracy: 0.9236 - val_loss: 0.3136\n",
      "Epoch 70/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 430ms/step - categorical_accuracy: 0.9965 - loss: 0.0143 - val_categorical_accuracy: 0.9236 - val_loss: 0.3223\n",
      "Epoch 71/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 489ms/step - categorical_accuracy: 0.9965 - loss: 0.0136 - val_categorical_accuracy: 0.9236 - val_loss: 0.3410\n",
      "Epoch 72/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 407ms/step - categorical_accuracy: 0.9965 - loss: 0.0126 - val_categorical_accuracy: 0.9236 - val_loss: 0.3391\n",
      "Epoch 73/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 450ms/step - categorical_accuracy: 0.9965 - loss: 0.0140 - val_categorical_accuracy: 0.9236 - val_loss: 0.3168\n",
      "Epoch 74/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 412ms/step - categorical_accuracy: 0.9965 - loss: 0.0117 - val_categorical_accuracy: 0.9375 - val_loss: 0.2807\n",
      "Epoch 75/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 416ms/step - categorical_accuracy: 0.9965 - loss: 0.0143 - val_categorical_accuracy: 0.9375 - val_loss: 0.2834\n",
      "Epoch 76/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 406ms/step - categorical_accuracy: 0.9983 - loss: 0.0118 - val_categorical_accuracy: 0.9375 - val_loss: 0.2740\n",
      "Epoch 77/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 375ms/step - categorical_accuracy: 0.9965 - loss: 0.0120 - val_categorical_accuracy: 0.9375 - val_loss: 0.2761\n",
      "Epoch 78/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 404ms/step - categorical_accuracy: 0.9983 - loss: 0.0118 - val_categorical_accuracy: 0.9375 - val_loss: 0.2788\n",
      "Epoch 79/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 372ms/step - categorical_accuracy: 0.9948 - loss: 0.0107 - val_categorical_accuracy: 0.9375 - val_loss: 0.2756\n",
      "Epoch 80/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 416ms/step - categorical_accuracy: 0.9931 - loss: 0.0180 - val_categorical_accuracy: 0.9167 - val_loss: 0.2938\n",
      "Epoch 81/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 372ms/step - categorical_accuracy: 0.9948 - loss: 0.0120 - val_categorical_accuracy: 0.9167 - val_loss: 0.2767\n",
      "Epoch 82/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 385ms/step - categorical_accuracy: 0.9965 - loss: 0.0113 - val_categorical_accuracy: 0.9375 - val_loss: 0.2705\n",
      "Epoch 83/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 376ms/step - categorical_accuracy: 0.9948 - loss: 0.0104 - val_categorical_accuracy: 0.9375 - val_loss: 0.2721\n",
      "Epoch 84/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 398ms/step - categorical_accuracy: 0.9948 - loss: 0.0099 - val_categorical_accuracy: 0.9375 - val_loss: 0.2741\n",
      "Epoch 85/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 404ms/step - categorical_accuracy: 0.9965 - loss: 0.0091 - val_categorical_accuracy: 0.9306 - val_loss: 0.2814\n",
      "Epoch 86/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 413ms/step - categorical_accuracy: 0.9965 - loss: 0.0090 - val_categorical_accuracy: 0.9306 - val_loss: 0.2841\n",
      "Epoch 87/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 393ms/step - categorical_accuracy: 0.9948 - loss: 0.0099 - val_categorical_accuracy: 0.9306 - val_loss: 0.2850\n",
      "Epoch 88/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 461ms/step - categorical_accuracy: 0.9948 - loss: 0.0109 - val_categorical_accuracy: 0.9306 - val_loss: 0.2624\n",
      "Epoch 89/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 418ms/step - categorical_accuracy: 0.9931 - loss: 0.0321 - val_categorical_accuracy: 0.9375 - val_loss: 0.2777\n",
      "Epoch 90/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 589ms/step - categorical_accuracy: 0.9861 - loss: 0.0348 - val_categorical_accuracy: 0.9375 - val_loss: 0.3115\n",
      "Epoch 91/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 406ms/step - categorical_accuracy: 0.9948 - loss: 0.0196 - val_categorical_accuracy: 0.9097 - val_loss: 0.3645\n",
      "Epoch 92/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 397ms/step - categorical_accuracy: 0.9948 - loss: 0.0164 - val_categorical_accuracy: 0.9236 - val_loss: 0.3473\n",
      "Epoch 93/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 451ms/step - categorical_accuracy: 0.9965 - loss: 0.0121 - val_categorical_accuracy: 0.9236 - val_loss: 0.3244\n",
      "Epoch 94/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 440ms/step - categorical_accuracy: 0.9931 - loss: 0.0172 - val_categorical_accuracy: 0.9306 - val_loss: 0.3091\n",
      "Epoch 95/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317ms/step - categorical_accuracy: 0.9965 - loss: 0.0114 - val_categorical_accuracy: 0.9306 - val_loss: 0.3118\n",
      "Epoch 96/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 347ms/step - categorical_accuracy: 0.9983 - loss: 0.0098 - val_categorical_accuracy: 0.9236 - val_loss: 0.3068\n",
      "Epoch 97/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 322ms/step - categorical_accuracy: 0.9948 - loss: 0.0098 - val_categorical_accuracy: 0.9236 - val_loss: 0.3090\n",
      "Epoch 98/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 317ms/step - categorical_accuracy: 0.9983 - loss: 0.0081 - val_categorical_accuracy: 0.9306 - val_loss: 0.3117\n",
      "Epoch 99/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 444ms/step - categorical_accuracy: 0.9965 - loss: 0.0095 - val_categorical_accuracy: 0.9306 - val_loss: 0.3072\n",
      "Epoch 100/100\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 422ms/step - categorical_accuracy: 0.9983 - loss: 0.0089 - val_categorical_accuracy: 0.9306 - val_loss: 0.3034\n",
      "Model saved as 'skating_jump_classifier.keras'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting training on {len(X_train)} samples...\")\n",
    "\n",
    "# Train the model (shuffle is True by default in Keras)\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the final model in the modern Keras format\n",
    "model.save('skating_jump_classifier.keras')\n",
    "print(\"Model saved as 'skating_jump_classifier.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e288d850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot Categorical Accuracy\n",
    "ax1.plot(history.history['categorical_accuracy'], label='Training Accuracy', color='blue')\n",
    "ax1.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy', color='orange')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot Loss\n",
    "ax2.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d5f81",
   "metadata": {},
   "source": [
    "## Real-Time Jump & Rotation Analysis\n",
    "\n",
    "The following script is the primary inference engine. It integrates the trained LSTM model with a custom physics layer to provide a comprehensive breakdown of skating performance in a single video output.\n",
    "\n",
    "### Pipeline Features:\n",
    "| Feature | Technical Implementation | Goal |\n",
    "| :--- | :--- | :--- |\n",
    "| **Jump Detection** | LSTM Sequence Buffer (150 frames) | Identify Axel, Edge, or Pick-based jumps. |\n",
    "| **Rotation Counting** | Angular Velocity Accumulation | Calculate total natural number revolutions (1, 2, 3...). |\n",
    "| **Air-Time Logic** | Hip-Height Thresholding | Automate the start/stop of rotation tracking. |\n",
    "| **Visualization** | OpenCV Rectangle & DUPLEX Fonts | Create a professional broadcast-style overlay. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7965852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Starting Analysis for Revolutions...\n",
      "\n",
      "✅ DONE! Check 'output_rotations.mp4' for the revolution count.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_PATH = \"skating_jump_classifier.keras\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model = load_model(MODEL_PATH)\n",
    "    print(f\"✅ Model loaded successfully from {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"❌ Error: {MODEL_PATH} not found. Please check the file path.\")\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "\n",
    "video_path = \"organized_dataset\\Class_0_Axel\\cam_2_Axel_3.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "out = cv2.VideoWriter('alex2_rotations.mp4', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "actions = ['Axel', 'Edge Jump', 'Complex Pick', 'Simple Pick']\n",
    "sequence_buffer = []\n",
    "is_in_air = False\n",
    "gliding_y, cal_frames = 0, 0\n",
    "current_label = \"Scanning...\"\n",
    "\n",
    "total_revolutions = 0\n",
    "accumulated_degrees = 0\n",
    "previous_angle = 0\n",
    "\n",
    "print(f\" Starting Analysis for Revolutions...\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    results = mp_pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        landmarks = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark])\n",
    "        \n",
    "        hip_center = (landmarks[23, :3] + landmarks[24, :3]) / 2\n",
    "        norm_landmarks = landmarks.copy()\n",
    "        norm_landmarks[:, :3] -= hip_center\n",
    "        sequence_buffer.append(norm_landmarks.flatten())\n",
    "        sequence_buffer = sequence_buffer[-150:]\n",
    "\n",
    "        left_hip, right_hip = landmarks[23], landmarks[24]\n",
    "        curr_y = (left_hip[1] + right_hip[1]) / 2\n",
    "        \n",
    "        current_angle = np.degrees(np.arctan2(right_hip[2] - left_hip[2], \n",
    "                                              right_hip[0] - left_hip[0]))\n",
    "\n",
    "        if cal_frames < 30:\n",
    "            gliding_y += curr_y / 30\n",
    "            cal_frames += 1\n",
    "            previous_angle = current_angle\n",
    "        else:\n",
    "            if curr_y < (gliding_y - 0.04) and not is_in_air:\n",
    "                is_in_air = True\n",
    "                accumulated_degrees = 0 \n",
    "                total_revolutions = 0\n",
    "            \n",
    "            elif curr_y >= (gliding_y - 0.02) and is_in_air:\n",
    "                is_in_air = False\n",
    "\n",
    "            if is_in_air:\n",
    "                delta = current_angle - previous_angle\n",
    "                \n",
    "                if delta > 180: delta -= 360\n",
    "                elif delta < -180: delta += 360\n",
    "                \n",
    "                accumulated_degrees += abs(delta)\n",
    "                total_revolutions = accumulated_degrees / 360\n",
    "            \n",
    "            previous_angle = current_angle\n",
    "\n",
    "        \n",
    "        if len(sequence_buffer) == 150:\n",
    "            input_data = np.expand_dims(sequence_buffer, axis=0).astype('float32')\n",
    "            preds = model(input_data, training=False)\n",
    "            current_label = actions[np.argmax(preds.numpy()[0])]\n",
    " \n",
    "\n",
    "        display_revs = int(total_revolutions) \n",
    "        \n",
    "        \n",
    "        box_size = 200\n",
    "        top_left = (20, 20)\n",
    "        bottom_right = (170 + box_size, box_size)\n",
    "        \n",
    "        cv2.rectangle(frame, top_left, bottom_right, (0, 0, 0), -1)\n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_DUPLEX \n",
    "        font_scale_main = 1.5\n",
    "        font_scale_sub = 0.9\n",
    "        thickness_standard = 1 \n",
    "\n",
    "        \n",
    "        cv2.putText(frame, f\"{current_label}\", (40, 80), \n",
    "                    font, font_scale_main, (0, 255, 0), thickness_standard)\n",
    "        \n",
    "       \n",
    "        cv2.putText(frame, f\"REVS: {display_revs}\", (40, 160), \n",
    "                    font, font_scale_main, (0, 255, 255), thickness_standard)\n",
    "        \n",
    "        \n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\n✅ DONE! Check 'output_rotations.mp4' for the revolution count.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f628b45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 👨‍💻 About Labellerr's Hands-On Learning in Computer Vision\n",
    "\n",
    "Thank you for exploring this **Labellerr Hands-On Computer Vision Cookbook**! We hope this notebook helped you learn, prototype, and accelerate your vision projects.  \n",
    "Labellerr provides ready-to-run Jupyter/Colab notebooks for the latest models and real-world use cases in computer vision, AI agents, and data annotation.\n",
    "\n",
    "---\n",
    "## 🧑‍🔬 Check Our Popular Youtube Videos\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use—environment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "---\n",
    "\n",
    "## 🎦 Popular Labellerr YouTube Videos\n",
    "\n",
    "Level up your skills and see video walkthroughs of these tools and notebooks on the  \n",
    "[Labellerr YouTube Channel](https://www.youtube.com/@Labellerr/videos):\n",
    "\n",
    "- [How I Fixed My Biggest Annotation Nightmare with Labellerr](https://www.youtube.com/watch?v=hlcFdiuz_HI) – Solving complex annotation for ML engineers.\n",
    "- [Explore Your Dataset with Labellerr's AI](https://www.youtube.com/watch?v=LdbRXYWVyN0) – Auto-tagging, object counting, image descriptions, and dataset exploration.\n",
    "- [Boost AI Image Annotation 10X with Labellerr's CLIP Mode](https://www.youtube.com/watch?v=pY_o4EvYMz8) – Refine annotations with precision using CLIP mode.\n",
    "- [Boost Data Annotation Accuracy and Efficiency with Active Learning](https://www.youtube.com/watch?v=lAYu-ewIhTE) – Speed up your annotation workflow using Active Learning.\n",
    "\n",
    "> 👉 **Subscribe** for Labellerr's deep learning, annotation, and AI tutorials, or watch videos directly alongside notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "## 🤝 Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
